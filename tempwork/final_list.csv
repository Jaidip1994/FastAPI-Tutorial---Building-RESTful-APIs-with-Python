urls,content,300DaysOfData
/feed/update/urn:li:activity:6864461297824612352/,"ğŸ”Š Hello everyone! Finally, I have completed my journey of #300DaysOfData! I am grateful for your                    support and appreciation along the way. I have learned a lot and I do strongly believe that you have                    also learned a lot so far.âœ¨ Here are the stories of my journey : ğŸ’¡ GitHub :                    https://lnkd.in/dTTVjw4BğŸ’¡ Blog : https://lnkd.in/dHccqWXkâœ¨ I want to continue my                    learning journey with the simple guidelines mentioned in the Snapshot below. I will be happy to hear                    your feedback and recommendations. Let's keep learning                    !!#machinelearningÂ #datascienceÂ #deeplearningÂ #300DaysOfDataÂ #learningÂ #githubÂ #linkedinfamÂ #66DaysOfData",True
/feed/update/urn:li:activity:6864461252442247168/,"ğŸ† Day 300 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Transformers Model, Data Loaders, Batch Size and Sequence Length, Language Model,                    Fine Tuning GPT2 Model, Callback, Learner, Perplexity and Cross Entropy Loss Function, Learner Rate                    Finder, Training and Generating Predictions and few more topics related to the same from here.                    ğŸ“— Notebook:ğŸ”° Transformers :Â https://lnkd.in/gqpVea3sğŸ“ˆ I have presented                    the implementation of Initializing DataLoaders, Fine Tuning GPT2Model and LR Finder using Fastai and                    PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine                    Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6864090914562953217/,"ğŸ† Day 299 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Transformers Model, GPT2 Pretrained Model and Tokenizer, Encodes and Decodes                    Methods, Preparing Dataset, Transform Method, Data Loaders and few more topics related to the same                    from here. ğŸ“— Notebook:ğŸ”° Transformers :Â https://lnkd.in/gqpVea3sğŸ“ˆ I have                    presented the implementation of Pretrained GPT2 Model and Tokenizer and Transformed DataLoaders                    using Fastai and PyTorch here in the snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’                    Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes : Finally, we are at final days of our journey.                    I will wrap up everything tomorrow with our achievements and new ways to start our next journey. I                    will be more than happy to hear your feedback and recommendations. One important point about our                    journey in past was about                    Production.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6863081298408312832/,"ğŸ† Day 298 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from PyImageSearch Blogs. Here, I have read about Convolutional Neural                    Networks, Adam Optimization Function, Compiling and Training Strided Net Model, Data Augmentation                    and Image Data Generator, Classification Report, Plotting Training Loss and Accuracy, Overfitting                    and Generalization and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”°                    Convolutional Layer:Â https://lnkd.in/dvwsgmEnğŸ“ˆ I have presented the implementation of                    Compiling and Training Model, Classification Report, Training Loss and Accuracy here in the                    snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ’»                    PyImageSearch :Â https://lnkd.in/dXTR8cMZğŸ“š Machine Learning Engineering for Production                    :Â https://lnkd.in/dpads35sğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #pythonÂ #PyImageSearch",True
/feed/update/urn:li:activity:6862237220384870400/,"ğŸ† Day 297 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from PyImageSearch Blogs. Here, I have read about CNN Architecture,                    Strided Net, Label Binarizer and One Hot Encoding, Image Data Generator and Data Augmentation,                    Loading and Resizing Images and few more topics related to the same from here. ğŸ“—                    Notebook:ğŸ”° Convolutional Layer:Â https://lnkd.in/dvwsgmEnğŸ“ˆ I have presented the                    implementation of Label Binarizer and Preparing Dataset here in the snapshot. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the topics from                    the Book mentioned below. Excited about the days ahead !!ğŸ’» PyImageSearch                    :Â https://lnkd.in/dXTR8cMZğŸ“š Machine Learning Engineering for Production                    :Â https://lnkd.in/dpads35sğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #pythonÂ #PyImageSearch",True
/feed/update/urn:li:activity:6861239719238361088/,"ğŸ† Day 296 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from PyImageSearch Blogs. Here, I have read about Convolutional Layers,                    Filters and Kernel Size, Strides, Padding, Input Data Format, Dilation Rate, Activation Function,                    Weights and Biases, Kernel and Bias Initializer and Regularizer, Generalization and Overfitting,                    Kernel and Bias Constraint, Caltech Dataset, Strided Net and few more topics related to the same                    from here. ğŸ“— Notebook:ğŸ”° Convolutional Layer:Â https://lnkd.in/dvwsgmEnğŸ“ˆ I                    have presented the implementation of Strided Net here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ’» PyImageSearch                    :Â https://lnkd.in/dXTR8cMZğŸ“š Machine Learning Engineering for Production                    :Â https://lnkd.in/dpads35sğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #pythonÂ #PyImageSearch",True
/feed/update/urn:li:activity:6860153555639115776/,"ğŸ† Day 295 ofÂ #300DaysOfData!âœ¨ Histogram Matching:Histogram MatchingÂ can be                    used as a normalization technique in an image processing pipeline as a form of color correction and                    color matching which allows to obtain a consistent, normalized representation of images even if                    lighting conditions change.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented from PyImageSearch Blogs. Here, I have read about Convolutional Neural                    Networks, Convolutional Matrix, Kernels, Spatial Dimensions, Padding, ROI of Image, Elementwise                    Multiplication and Addition, Rescaling Intensity, Laplacian Kernel, Detecting Blur and Smoothing and                    few more topics related to the same from here. ğŸ“— Notebook:ğŸ”°                    Convolution:Â https://lnkd.in/dUAA-T3wğŸ“ˆ I have presented the implementation of                    Convolution Method and Constructing Kernels here in the snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time learning the topics from the Book                    mentioned below. Excited about the days ahead !!ğŸ’» PyImageSearch                    :Â https://lnkd.in/dXTR8cMZğŸ“š Machine Learning Engineering for Production                    :Â https://lnkd.in/dpads35sğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #pythonÂ #PyImageSearch",True
/feed/update/urn:li:activity:6859437835355742208/,"ğŸ† Day 294 ofÂ #300DaysOfData!âœ¨ Histogram Matching: Histogram MatchingÂ can be                    used as a normalization technique in an image processing pipeline as a form of color correction and                    color matching which allows to obtain a consistent, normalized representation of images even if                    lighting conditions change.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented from PyImageSearch Blogs. Here, I have read about OpenCV, Color Detection, RGB                    Colorspace, Histogram Matching, Pixel Distribution, Cumulative Distribution, Resizing Image and few                    more topics related to the same from here. I have also read about Skewed Datasets, Performance                    Auditing, Data Centric AI Development and Data Augmentation from Machine Learning Engineering for                    Production Specialization of Coursera.ğŸ“— Notebook:ğŸ”°OpenCV                    :Â https://lnkd.in/d2VDFjBuğŸ“ˆ I have presented the implementation of OpenCV in Histogram                    Matching here in the snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ’» PyImageSearch :Â https://lnkd.in/dXTR8cMZğŸ“š Machine Learning Engineering                    for Production :Â https://lnkd.in/dpads35sğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #pythonÂ #PyImageSearch",True
/feed/update/urn:li:activity:6858643401059921920/,"ğŸ† Day 293 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from PyImageSearch Blogs. Here, I have read about OpenCV, Rotating Images,                    Image Preprocessing, Rotation Matrix and Center Coordinates, Image Parsing, Edge Detection and                    Contour Detection, Masking and Blurring Images and few more topics related to the same from here. I                    have also read about Baseline Model, Selecting and Training Model, Error Analysis and Prioritization                    from Machine Learning Engineering for Production Specialization of Coursera.ğŸ“—                    Notebook:ğŸ”° OpenCV: https://lnkd.in/d2GEb7fQğŸ“ˆ I have presented the implementation of                    OpenCV in Rotating Images and Getting ROI of Images here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ’» PyImageSearch                    :Â https://lnkd.in/dXTR8cMZğŸ“š Machine Learning Engineering for Production                    :Â https://lnkd.in/dpads35sğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #pythonÂ #PyImageSearch",True
/feed/update/urn:li:activity:6857932534915837952/,"ğŸ† Day 292 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from PyImageSearch Blogs. Here, I have read about OpenCV, Counting                    Objects, Converting Image to Grayscale, Edge Detection, Thresholding, Detecting and Drawing                    Contours, Erosions and Dilations, Masking and Bitwise Operations and few more topics related to the                    same from here. I have also read about Modeling Overview, Key Challenges and Low Average Error from                    Machine Learning Engineering for Production Specialization of Coursera.ğŸ“— Notebook:ğŸ”°                    OpenCV Notebook:Â https://lnkd.in/dmDhZAAzğŸ“ˆ I have presented the implementation of                    OpenCV in Converting Image to Grayscale, Edge Detection, Thresholding, Detecting and Drawing                    Contours, Erosions and Dilations here in the snapshot. I hope you will gain some insights and work                    on the same. I hope you will also spend some time learning the topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ’» PyImageSearch :Â https://lnkd.in/dXTR8cMZğŸ“š                    Machine Learning Engineering for Production :Â https://lnkd.in/dpads35sğŸ“’ Journey on Machine                    Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #pythonÂ #PyImageSearch",True
/feed/update/urn:li:activity:6857254111805497344/,"ğŸ† Day 291 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from PyImageSearch Blogs. Here, I have read about OpenCV, Loading and                    Displaying an Image, Accessing Pixels, Array Slicing and Cropping, Resizing Images, Rotating Image,                    Smoothing Image, Drawing on an Image and few more topics related to the same. I have also read about                    ML Project Lifecycle, Deployment Patterns and Pipeline Monitoring from Machine Learning Engineering                    for Production Specialization of Coursera. ğŸ“— Notebook:ğŸ”° OpenCV Notebook:                    https://lnkd.in/dmDhZAAzğŸ“ˆ I have presented the implementation of OpenCV in Resizing and                    Rotating and Image, Smoothing and Drawing on an Image here in the snapshot. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the topics from                    the Book mentioned below. Excited about the days ahead !!ğŸ’» PyImageSearch :                    https://lnkd.in/dXTR8cMZğŸ“š Machine Learning Engineering for Production :                    https://lnkd.in/dpads35sğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python                    #PyImageSearch",True
/feed/update/urn:li:activity:6856497002616905728/,"ğŸ† Day 290 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented about Object Detection and Fine Tuning, Image Segmentation, Tensors and Aspect                    Ratio, Arrays, Dataset and Data Loaders. I have also started the Machine Learning Engineering for                    Production Specialization from Coursera. Here, I have read about Steps of ML Project and Case Study,                    ML Project Lifecycle and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”°                    Object Detection: https://lnkd.in/gxXFhutpğŸ“ˆ I have presented the implementation of Dataset                    Class here in the snapshot. I hope you will gain some insights and work on the same. I hope you will                    also spend some time learning the topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹Notes:I will truly appreciate your feedbacks                    and recommendations                    here.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6855749798197239808/,"ğŸ† Day 289 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Approaching Almost Any Machine Learning Problem"". Here, I                    have read about Regression Metrics such as Mean Absolute and Average Error, Root Mean Squared Error,                    Squared Logarithmic Error, Mean Absolute Percentage Error, R-Squared and Coefficient of                    Determination, Cohen's Kappa Score, MCC Score and few more topics related to the same from here.                    ğŸ“— Notebook:ğŸ”° Evaluation Metrics:Â https://lnkd.in/d273Ru4nğŸ“ˆ I have                    presented the implementation of Mean Absolute and Average Error, Squared Logarithmic Error, Mean                    Absolute Percentage Error, R-Squared and MCC Score here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Approaching Almost Any Machine                    Learning ProblemğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“’                    AAML:Â https://lnkd.in/djFygNUbğŸ–‹ğŸ“‹Notes:I will truly appreciate your feedbacks and                    recommendations                    here.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #AAMLÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6855068409781075968/,"ğŸ† Day 288 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Approaching Almost Any Machine Learning Problem"". Here, I                    have read about Recall Metrics for Multiclass Classification, Weighted F1 Score, Confusion Matrix,                    Type I Error and Type II Error, AUC Curve, Multilabel Classification and Average Precision and few                    more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Evaluation                    Metrics:Â https://lnkd.in/d273Ru4nğŸ“ˆ I have presented the implementation of Weighted F1                    Score and Average Precision here in the snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Approaching Almost Any Machine Learning ProblemğŸ“’                    Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ“’ AAML: https://lnkd.in/djFygNUbğŸ–‹ğŸ“‹Notes:I will                    truly appreciate your feedbacks and recommendations                    here.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #AAMLÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6854379368140046336/,"ğŸ† Day 287 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Approaching Almost Any Machine Learning Problem"". Here, I                    have read about Multiclass Classification, Macro Averaged Precision, Micro Averaged Precision,                    Weighted Precision, Recall Metrics, Random Forest Regressor, Mean Squared Error, Root Mean Squared                    Error and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Evaluation                    Metrics:Â https://lnkd.in/d273Ru4nğŸ“ˆ I have presented the implementation of Micro                    Averaged Precision and Weighted Precision here in the snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time learning the topics from the Book                    mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and                    PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹Notes:I will truly appreciate your feedbacks                    and recommendations                    here.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #AAMLÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6853597743101841408/,"ğŸ† Day 286 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Approaching Almost Any Machine Learning Problem"". Here, I                    have read about True Positive Rate, Recall and Sensitivity, False Positive Rate and Specificity,                    Area Under ROC Curve, Prediction, Probability and Thresholds, Log Loss Function, Multiclass                    Classification and Macro Averaged Precision and few more topics related to the same from here.                    ğŸ“— Notebook:ğŸ”° Evaluation Metrics:Â https://lnkd.in/d273Ru4nğŸ“ˆ I have                    presented the implementation of True Negative Rate, False Positive Rate, Log Loss Function and Macro                    Averaged Precision here in the snapshot. I hope you will gain some insights and work on the same. I                    hope you will also spend some time learning the topics from the Book mentioned below. Excited about                    the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on                    Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹Notes: I will truly appreciate your feedbacks and                    recommendations here.                    #66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #AAMLÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6852877137121185792/,"ğŸ† Day 285 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Approaching Almost Any Machine Learning Problem"". Here, I                    have read about Evaluation Metrics and Accuracy Score, Training and Validation Set, Precision and                    Recall, True Positive and True Negative, False Positive and False Negative, Binary Classification                    and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Evaluation                    Metrics:Â https://lnkd.in/d273Ru4nğŸ“ˆ I have presented the implementation of True                    Negative, False Negative, False Positive and Accuracy Score here in the snapshot. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the topics                    from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders                    with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Note: I                    will truly appreciate any feedbacks and suggestion about my posts so that it would be more helpful                    for you and anyone out there.                    #66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #AAMLÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6852574995651039232/,"ğŸ† Day 284 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Approaching Almost Any Machine Learning Problem"". Here, I                    have read about Stratified KFold Cross Validation, Skewed Dataset and Classification, Data                    Distribution, Hold Out Cross Validation, Time Series Data, Regression and Sturge's Rule,                    Probabilities, Evaluation Metrics and Accuracy and few more topics related to the same from here.                    ğŸ“— Notebook:ğŸ”° Supervised and Unsupervised                    Learning:Â https://lnkd.in/d6tEjcCVğŸ“ˆ I have presented the implementation of                    Distribution of Labels and Stratified KFold here in the snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time learning the topics from the Book                    mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and                    PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #AAMLÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6852161943055151105/,"ğŸ† Day 283 ofÂ #300DaysOfData!ğŸ“‘ Cross Validation:Cross ValidationÂ is a step in                    the process of building a machine learning model which helps us to ensure that our models fit the                    data accurately and also ensures that we do not overfit.ğŸ¯ On my Journey of Machine Learning                    and Deep Learning, I have read and implemented from the book ""Approaching Almost Any Machine                    Learning Problem"". Here, I have read about Decision Trees and Classification, Features and                    Parameters, Accuracy and Model Predictions, Overfitting and Model Generalization, Training Loss and                    Validation Loss, Cross Validation and few more topics related to the same from here. ğŸ“—                    Notebook:ğŸ”° Supervised and Unsupervised Learning:Â https://lnkd.in/d6tEjcCVğŸ“ˆ I have                    presented the implementation of Decision Tree Classifier and Model Evaluation here in the snapshot.                    I hope you will gain some insights and work on the same. I hope you will also spend some time                    learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep                    Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #AAMLÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6851416430101975040/,"ğŸ† Day 282 ofÂ #300DaysOfData!ğŸ“‘ Cross Validation: Cross ValidationÂ is a step                    in the process of building a machine learning model which helps us to ensure that our models fit the                    data accurately and also ensures that we do not overfit.ğŸ¯ On my Journey of Machine Learning                    and Deep Learning, I have read and implemented from the book ""Approaching Almost Any Machine                    Learning Problem"". Here, I have read about Supervised and Unsupervised Learning, Features, Samples                    and Targets, Classification and Regression, Clustering, T-Distributed Stochastic Neighbour                    Embedding, 2D Arrays, Cross Validation, Overfitting and few more topics related to the same from                    here. ğŸ“— Notebook:ğŸ”° Supervised and Unsupervised Learning:                    https://lnkd.in/d6tEjcCVğŸ“ˆ I have presented the implementation of TSNE Decomposition and                    Preparing Dataset here in the snapshot. I hope you will gain some insights and work on the same. I                    hope you will also spend some time learning the topics from the Book mentioned below. Excited about                    the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on                    Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #AAML                    #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6850773434188759040/,"ğŸ† Day 281 ofÂ #300DaysOfData!ğŸ“‘ Sensitivity & Specificity: - Sensitivity = True                    Positive / (True Positive + False Negative). It is also known as aÂ Type II Error. -                    Specificity = True Negative / (False Positive + True Negative). It is also known as aÂ Type I                    Error.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read and implemented                    from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read about                    Sensitivity and Specificity, Positive Predictive Value and Negative Predictive Value, Confusion                    Matrix and Model Interpretation, Type I & II Error, Accuracy and Prevalence and few more topics                    related to the same from here. ğŸ“— Notebook:ğŸ”° Chest X-Rays                    Classification:Â https://lnkd.in/gQsmTMQpğŸ“ˆ I have presented the implementation of                    Confusion Matrix, Sensitivity and Specificity, Accuracy using Fastai and PyTorch here in the                    snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š                    Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6850042384693133312/,"ğŸ† Day 280 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Binary Classification, Initializing Data Block and Data Loaders, Image Block and                    Category Block, Batch Transformations, Training Pretrained Model, Learning Rate Finder, Tensors and                    Probabilities, Model Interpretation and few more topics related to the same from here. ğŸ“—                    Notebook:ğŸ”° Chest X-Rays Classification:Â https://lnkd.in/gQsmTMQpğŸ“ˆ I have                    presented the implementation of Initializing Data Block and Data Loaders, Training Pretrained Model                    and Interpretation using Fastai and PyTorch here in the snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time learning the topics from the Book                    mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and                    PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6849725750061465600/,"ğŸ† Day 279 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have                    read Binary Classification, Chest X-Rays, DICOM or Digital Imaging and Communications in Medicine,                    Plotting the DICOM Data, Random Splitter Function, Medical Imaging, Pixel Data and few more topics                    related to the same from here.ğŸ“— Notebook:ğŸ”° Chest X-Rays Classification:                    https://lnkd.in/gQsmTMQpğŸ“ˆ I have presented the implementation of Getting DICOM Files and                    Inspection using Fastai and PyTorch here in the snapshot. I hope you will gain some insights and                    work on the same. I hope you will also spend some time learning the topics from the Book mentioned                    below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and                    PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6848921648570093568/,"ğŸ† Day 278 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Data, Convolutional Neural Net Model, Loss Function, Stochastic Gradient Descent and                    Optimization Function, Learner, Callbacks, Parameters, Training and Epochs and few more topics                    related to the same from here. ğŸ“— Notebook:ğŸ”° Fastai Learner                    :Â https://lnkd.in/dwfwK3vPğŸ“ˆ I have presented the implementation of Learner and                    Callbacks using Fastai and PyTorch here in the snapshot. I hope you will gain some insights and work                    on the same. I hope you will also spend some time learning the topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’                    Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6848227591493582848/,"ğŸ† Day 277 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Loss Function, Negative Log Likelihood Function, Log Softmax Function, Log of Sum of                    Exponentials, Stochastic Gradient Descent Optimizer Function, Data Loaders, Training and Validation                    Sets and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Fastai Learner                    :Â https://lnkd.in/dwfwK3vPğŸ“ˆ I have presented the implementation of Negative Log                    Likelihood Function, Cross Entropy Loss Function, SGD Optimizer and Data Loaders using Fastai and                    PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine                    Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6847736839265124352/,"ğŸ† Day 276 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Convolutional Neural Networks, Linear Model, Testing Module, Sequential Module,                    Parameters, Adaptive Pooling Layer and Mean, Stride, Hook Function, Pipeline and few more topics                    related to the same from here. ğŸ“— Notebook:ğŸ”° Fastai Learner                    :Â https://lnkd.in/dwfwK3vPğŸ“ˆ I have presented the implementation of Testing Module,                    Sequential Module and Convolutional Neural Network using Fastai and PyTorch here in the snapshot. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning                    for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6847519011756892160/,ğŸ”Š Hello Everyone! You can read or download the article now.,False
/feed/update/urn:li:activity:6847392193829728256/,"ğŸ† Day 275 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Module and Parameter, Forward Propagation Function, Convolutional Layer, Training                    Attributes, Kaiming Normalization and Xavier Normalization Initializer, Transformation Function,                    Weights and Biases, Linear Model, Tensors and few more topics related to the same from here.                    ğŸ“— Notebook:ğŸ”° Fastai Learner :Â https://lnkd.in/dwfwK3vPğŸ“ˆ I have presented                    the implementation of Defining Module : Convolutional Layer and Linear Model using Fastai and                    PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine                    Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6846786623439437824/,"ğŸ† Day 274 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have                    read about Creating Collation Function, Parallel Preprocessing, Decoding Images, Data Loader Class,                    Normalization and Image Statistics, Permuting Axis Order, Precision and few more topics related to                    the same from here. ğŸ“— Notebook:ğŸ”° Fastai Learner                    :Â https://lnkd.in/dwfwK3vPğŸ“ˆ I have presented the implementation of Initializing Data                    Loader and Normalization using Fastai and PyTorch here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai                    and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6846407169567424512/,"ğŸ† Day 273 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Fastai Learner from Scratch, Dependent and Independent Variable, Vocabulary, Dataset                    and Indexing and few more topics related to the same. I have also read about Convolutional Neural                    Networks, Perturbations and Loss Functions. ğŸ“— Notebook:ğŸ”° Fastai Learner                    :Â https://lnkd.in/dwfwK3vPğŸ“ˆ I have presented the implementation of Preparing Training                    and Validation Dataset using Fastai and PyTorch here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai                    and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6845911167337041920/,"ğŸ† Day 272 ofÂ #300DaysOfData!ğŸ“‘ Class Activation Map:The Class Activation Map uses                    the output of the last convolutional layer which is just before the average pooling layer together                    with predictions to give a heatmap visualization of model decision.ğŸ¯ On my Journey of                    Machine Learning and Deep Learning, I have read and implemented from the book ""Deep Learning for                    Coders with Fastai and PyTorch"". Here, I have read about Hook Class and Context Manager, Gradient                    Class Activation Map, Heatmap Visualization, Activations and Weights, Gradients and Back                    Propagation, Model Interpretation and few more topics related to the same from here. ğŸ“—                    Notebook:ğŸ”° CNN Interpretation :Â https://lnkd.in/dGUNHMZtğŸ“ˆ I have presented the                    implementation of Defining Hook Function, Activations, Gradients and Heatmap Visualization using                    Fastai and PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I                    hope you will also spend some time learning the topics from the Book mentioned below. Excited about                    the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on                    Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6845674667177480192/,"ğŸ† Day 271 ofÂ #300DaysOfData!ğŸ“‘ Class Activation Map: The Class Activation Map uses                    the output of the last convolutional layer which is just before the average pooling layer together                    with predictions to give a heatmap visualization of model decision.ğŸ¯ On my Journey of                    Machine Learning and Deep Learning, I have read and implemented from the book ""Deep Learning for                    Coders with Fastai and PyTorch"". Here, I have read about CNN Interpretation, Class Activation Map,                    Hooks, Heatmap Visualization, Activations and Convolutional Layer, Dot Product, Feature Map, Data                    Loaders and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”° CNN                    Interpretation : https://lnkd.in/dGUNHMZtğŸ“ˆ I have presented the implementation of Defining                    Hook Function and Decoding Images using Fastai and PyTorch here in the snapshot. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the topics                    from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders                    with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6844826211164684288/,"ğŸ† Day 270 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Defining Base Class and Sub Classes, Linear Layer, RELU Activation Function and Non                    Linearities, Mean Squared Error Function, Super Class Initializer, Kaiming Initialization,                    Elementwise Arithmetic and Broadcasting and few more topics related to the same from here.                    ğŸ“— Notebook:ğŸ”° Neural Network Foundations :Â https://lnkd.in/dUGqrcgXğŸ“ˆ I                    have presented the implementation of Defining Linear Layer and Linear Model using Fastai and PyTorch                    here in the snapshot. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and                    Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6844578456181710848/,"ğŸ† Day 269 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Initializing Model Architecture, Callable Function, Forward and Backward Propagation                    Function, Linear Function, Mean Squared Error Loss Function, RELU Activation Function, Back                    Propagation Function and Gradients, Squeeze Function and few more topics related to the same from                    here. I have also read about Perturbations and Neural Networks, Vanishing Gradients and                    Convolutional Neural Networks. ğŸ“— Notebook:ğŸ”° Neural Network Foundations                    :Â https://lnkd.in/dUGqrcgXğŸ“ˆ I have presented the implementation of Defining Model                    Architecture, Layer Function and RELU using Fastai and PyTorch here in the snapshot. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the topics                    from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders                    with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6844188239264288768/,"ğŸ† Day 268 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Gradients of Matrix Multiplication, Symbolic Computation, Forward and Backward                    Propagation Function, Model Parameters, Weights and Biases, Refactoring the Model, Callable Module                    and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Neural Network                    Foundations :Â https://lnkd.in/dUGqrcgXğŸ“ˆ I have presented the implementation of RELU                    Module, Linear Module and Mean Squared Error Module using Fastai and PyTorch here in the snapshot. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning                    for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6843807765618561024/,"ğŸ† Day 267 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Kaiming Initialization, Forward Pass, Mean Squared Error Loss Function, Gradients                    and Backward Pass, Linear Layers and RELU Activation Function, Chain Rule, Backpropagation and few                    more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Neural Network Foundations                    :Â https://lnkd.in/dUGqrcgXğŸ“ˆ I have presented the implementation of Kaiming                    Initialization, MSE Loss Function and Gradients using Fastai and PyTorch here in the snapshot. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning                    for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6843529918010138624/,"ğŸ† Day 266 ofÂ #300DaysOfData!ğŸ“‘ Forward and Backward Passes :Computing all the                    gradients of a given loss with respect to its parameters is known asÂ Backward Pass. Similarly                    computing the output of the model on a given input based on the matrix products is known                    asÂ Forward Pass.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read and                    implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read                    about Mean and Standard Deviation, Matrix Multiplications, Xavier Initialization, RELU Activation,                    Kaiming Initialization, Weights and Activations and few more topics related to the same from here.                    ğŸ“— Notebook:ğŸ”° Neural Network Foundations :Â https://lnkd.in/dUGqrcgXğŸ“ˆ I                    have presented the implementation of Xavier Initialization, RELU Activation, and Matrix                    Multiplications using Fastai and PyTorch here in the snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time learning the topics from the Book                    mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and                    PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes :Einstein Summation is a compact                    representation for combining products and                    sums.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6843166811098226688/,"ğŸ† Day 265 ofÂ #300DaysOfData!ğŸ“‘ Forward and Backward Passes : Computing all the                    gradients of a given loss with respect to its parameters is known asÂ Backward Pass. Similarly                    computing the output of the model on a given input based on the matrix products is known                    asÂ Forward Pass.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read and                    implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read                    about Broadcasting with Scalar, Broadcasting Vector and Matrix, Unsqueeze Method, Einstein                    Summation, Matrix Multiplication, The Forward and Backward Passes, Defining and Initializing Layer,                    Activation Function, Linear Layer, Weights and Biases and few more topics related to the same from                    here. ğŸ“— Notebook:ğŸ”° Neural Network Foundations                    :Â https://lnkd.in/dUGqrcgXğŸ“ˆ I have presented the implementation of Einstein Summation                    and Defining and Initializing Linear Layer using Fastai and PyTorch here in the snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for                    Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes :                    Einstein Summation is a compact representation for combining products and                    sums.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6842428801310302209/,"ğŸ† Day 264 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Neural Networks, Building a Neural Network from Scratch, Modeling a Neuron,                    Nonlinear Activation Functions, Hidden Size, Fully Connected Layer and Dense Layer, Linear Layer,                    Matrix Multiplication from Scratch, Elementwise Arithmetic and few more topics related to the same                    from here. ğŸ“— Notebook:ğŸ”° Neural Network Foundations :                    https://lnkd.in/dUGqrcgXğŸ“ˆ I have presented the implementation of Matrix Multiplication from                    Scratch and Elementwise Arithmetic using Fastai and PyTorch here in the snapshot. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the topics                    from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders                    with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6841668435206926336/,"ğŸ† Day 263 ofÂ #300DaysOfData!ğŸ“‘ Adam Optimizer :Adam mixes the idea of SGD with                    momentum and RMSProp together where it uses the moving average of the gradients as a direction and                    divides by the square root of the moving average of the gradients squared to give an adaptive                    learning rate to each parameter. It takes the unbiased moving average.ğŸ¯ On my Journey of                    Machine Learning and Deep Learning, I have read and implemented from the book ""Deep Learning for                    Coders with Fastai and PyTorch"". Here, I have read about Creating Callbacks, Loss Functions, Model                    Resetter Callbacks, RNN Regularization, Callback Ordering and Exceptions, Stochastic Gradient                    Descent and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Training                    Process :Â https://lnkd.in/dfv565VağŸ“ˆ I have presented the implementation of Model                    Resetter Callback and RNN Regularization Callback using Fastai and PyTorch here in the snapshot. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning                    for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6840983171719290881/,"ğŸ† Day 262 ofÂ #300DaysOfData!ğŸ“‘ Adam Optimizer : Adam mixes the idea of SGD with                    momentum and RMSProp together where it uses the moving average of the gradients as a direction and                    divides by the square root of the moving average of the gradients squared to give an adaptive                    learning rate to each parameter. It takes the unbiased moving average.ğŸ¯ On my Journey of                    Machine Learning and Deep Learning, I have read and implemented from the book ""Deep Learning for                    Coders with Fastai and PyTorch"". Here, I have read about RMSProp Optimizer, SGD, Adam Optimizer,                    Unbiased Moving Average of Gradients, Momentum Parameter, Decoupled Weight Decay, L1 and L2                    Regularization, Callbacks and few more topics related to the same from here. ğŸ“—                    Notebook:ğŸ”° Training Process :Â https://lnkd.in/dfv565VağŸ“ˆ I have presented the                    implementation of RMS Prop and Adam Optimizer using Fastai and PyTorch here in the snapshot. I hope                    you will gain some insights and work on the same. I hope you will also spend some time learning the                    topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for                    Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6840635729991741440/,"ğŸ† Day 261 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Stochastic Gradient Descent and Optimization Function, Momentum, Exponentially                    Weighted Moving Average, Gradient Averages, Callbacks, RMS Prop, Adaptive Learning Rate, Divergence                    and Epsilon and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Training                    Process :Â https://lnkd.in/dfv565VağŸ“ˆ I have presented the implementation of Momentum                    and RMS Prop using Fastai and PyTorch here in the snapshot. I hope you will gain some insights and                    work on the same. I hope you will also spend some time learning the topics from the Book mentioned                    below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and                    PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #python",True
/feed/update/urn:li:activity:6840255821583020032/,"ğŸ”Š Hello everyone! Our research paper is published at Genomics journal of Elsevier publication. I am                    so grateful to Chanin Nantasenamat and Hao Li for giving me the opportunity to work with them.                    ğŸ“‘ The paper is titled ""Toward insights on antimicrobial selectivity of host defense                    peptides via machine learning model interpretation"". I will really appreciate your support and                    feedback. Have a great time! ğŸ“’ Elsevier : https://lnkd.in/dK9PCrgxğŸ“’ PubMed :                    https://lnkd.in/dy-WwyTn#researchpaper #machinelearning #bioinformatics #linkedincommunity                    #Antimicrobial #qsar #datascience #hostdefense #peptides #genomics #elsevier",False
/feed/update/urn:li:activity:6839889935437205504/,"ğŸ† Day 260 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Training Process, Stochastic Gradient Descent, Optimization Function, Learning Rate                    Finder, Momentum, Optimizer Callbacks, Zeroing Gradients, Partial Function and few more topics                    related to the same from here. ğŸ“— Notebook:ğŸ”° Training Process                    :Â https://lnkd.in/dfv565VağŸ“ˆ I have presented the implementation of Functions for                    Optimizer and SGD here in the snapshot. I hope you will gain some insights and work on the same. I                    hope you will also spend some time learning the topics from the Book mentioned below. Excited about                    the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on                    Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6839530194689130496/,"ğŸ† Day 259 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Stochastic Gradient Descent, Loss Function, Updating Weights, Optimization Function,                    Creating Data Block and Data Loaders, ResNet Model and Learner, Training Process and few more topics                    related to the same from here. ğŸ“— Notebook:ğŸ”° Training Process :                    https://lnkd.in/dfv565VağŸ“ˆ I have presented the implementation of Preparing Dataset and                    Baseline Model using Fastai and PyTorch here in the snapshot. I hope you will gain some insights and                    work on the same. I hope you will also spend some time learning the topics from the Book mentioned                    below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and                    PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes :The head of a neural net is the part                    that is specialized for a particular task. For aÂ Convolutional Neural Network, it's generally                    the part after the adaptive average pooling layer. The body of the neural net contains everything                    except the head of a network and also includes the stem of the                    network.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunityÂ #linkedinupdateÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6838820937928847360/,"ğŸ† Day 258 ofÂ #300DaysOfData!ğŸ“‘ Splitter Function : A splitter is a function that                    tells the fastai library how to split the model into parameter groups which are used to train only                    the head of the model during transfer learning. TheÂ paramsÂ is just a function that returns                    all parameters of a given module.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Body and Head of Networks, Batch Normalization Layer, Unet Learner and Architecture,                    Generative Vision Models, Nearest Neighbor Interpolation, Transposed Convolutions, Siamese Network,                    Loss Function and Splitter Function and few more topics related to the same from here. ğŸ“—                    Notebook:ğŸ”° Architecture Details :Â https://lnkd.in/d6KvVP-PğŸ“ˆ I have presented the                    implementation of Siamese Network Model, Loss Function and Splitter Function using Fastai and                    PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine                    Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes :The head of a neural net is the part that is                    specialized for a particular task. For aÂ Convolutional Neural Network, it's generally the part                    after the adaptive average pooling layer. The body of the neural net contains everything except the                    head of a network and also includes the stem of the                    network.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #linkedincommunity                    #linkedinupdate #computervision #python #ai",True
/feed/update/urn:li:activity:6838089099497230336/,"ğŸ† Day 257 ofÂ #300DaysOfData!ğŸ“‘ Bottleneck Layers : Bottleneck LayersÂ use                    three convolutions: Two 1X1 at the begining and the end and one 3X3. The 1X1 convolutions are much                    faster which facilitates to use higher number of filters in and out. The 1X1 convolutions diminish                    and then restore the number of channels so calledÂ Bottleneck. The overall impact is to                    facilitate the use of more filters in the same amount of time.ğŸ¯ On my Journey of Machine                    Learning and Deep Learning, I have read and implemented from the book ""Deep Learning for Coders with                    Fastai and PyTorch"". Here, I have read about Stem of the Network, Residual Network Architecture,                    Bottleneck Layers, Convolutional Neural Networks, Progressive Resizing and few more topics related                    to the same from here. ğŸ“— Notebook:ğŸ”° Residual Networks                    :Â https://lnkd.in/deAMgbYbğŸ“ˆ I have presented the implementation of Training Deeper                    Networks and Bottleneck Layers using Fastai and PyTorch here in the snapshot. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the topics from                    the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with                    Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes                    :StemÂ is defined as the first few layers ofÂ CNN. It has different structure than the                    main body of                    CNN.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #linkedinupdateÂ Â #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6837730519233372160/,"ğŸ† Day 256 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have                    read about Residual Networks, RELU Activation Function, Skip Connections, Training Deeper Models,                    Loss Landscape of NN, Stem of the Network, Convolutional Layers, Max Pooling Layer and few more                    topics related to the same from here. ğŸ“— Notebook:ğŸ”° Residual Networks                    :Â https://lnkd.in/deAMgbYbğŸ“ˆ I have presented the implementation of Training Deeper                    Models and Stem of Network using Fastai and PyTorch here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai                    and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes :StemÂ is defined as the first few                    layers ofÂ CNN. It has different structure than the main body of                    CNN.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday                    #linkedinupdate Â #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6837369625471135744/,"ğŸ† Day 255 ofÂ #300DaysOfData!ğŸ“‘ Fully Convolutional Networks :The idea                    inÂ Fully Convolutional NetworksÂ is to take the average of activations across a                    convolutional grid. AÂ Fully Convolutional NetworksÂ has a number of convolutional layers,                    some of which will be stride 2 convolutions at the end of which is an adaptive average pooling                    layer, a flatten layer to remove the unit axis and finally a linear layer.ğŸ¯ On my Journey                    of Machine Learning and Deep Learning, I have read and implemented from the book ""Deep Learning for                    Coders with Fastai and PyTorch"". Here, I have read about Fully Convolutional Neural Networks,                    Building ResNet, Skip Connections, Identity Mapping, SGD, Batch Normalization Layer, Trainable                    Parameters, True Identity Path, Convolutional Neural Networks, Average Pooling Layer and few more                    topics related to the same from here. ğŸ“— Notebook:ğŸ”° Residual Networks                    :Â https://lnkd.in/deAMgbYbğŸ“ˆ I have presented the implementation of ResNet Architecture                    and Skip Connections using Fastai and PyTorch here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai                    and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes :Identity MappingÂ is the process of                    returning the input without changing it at all which is performed by the identity                    function.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6836996197438263296/,"ğŸ† Day 254 ofÂ #300DaysOfData!ğŸ“‘ Fully Convolutional Networks : The idea                    inÂ Fully Convolutional NetworksÂ is to take the average of activations across a                    convolutional grid. AÂ Fully Convolutional NetworksÂ has a number of convolutional layers,                    some of which will be stride 2 convolutions at the end of which is an adaptive average pooling                    layer, a flatten layer to remove the unit axis and finally a linear layer.ğŸ¯ On my Journey                    of Machine Learning and Deep Learning, I have read and implemented from the book ""Deep Learning for                    Coders with Fastai and PyTorch"". Here, I have read about Residual Networks or ResNets, Convolutional                    Neural Networks, Strides and Padding, Fully Convolutional Networks, Adaptive Average Pooling Layer,                    Flatten Layer, Activations and Matrix Multiplications and few more topics related to the same from                    here. ğŸ“— Notebook:ğŸ”° Residual Networks :Â https://lnkd.in/deAMgbYbğŸ“ˆ I have                    presented the implementation of Preparing Data and Fully Convolutional Networks using Fastai and                    PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine                    Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes :Larger batches have gradients that are more                    accurate since they are calculated from more data. But larger batch size means fewer batches per                    epoch which means fewer opportunities for the model to update                    weights.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6836625619451695105/,ğŸ”Š Hello everyone! I have been documenting my journey on Machine Learning and Deep Learning for                    about a 10 months now. My journey might help you out incase you are confused to get a right path.                    âœ¨ The repository just hit 200 â­ today. I really appreciate your support. Let's keep learning                    !!ğŸ“’ GitHub : https://lnkd.in/d-aDKvq#machinelearning #datascience #deeplearning                    #300DaysOfData #learning #github #linkedinfam #naturallanguageprocessing #gans #fastai,True
/feed/update/urn:li:activity:6836264138738561024/,"ğŸ† Day 253 ofÂ #300DaysOfData!ğŸ“‘ One Cycle Training : 1 Cycle TrainingÂ is a                    combination of warmup and annealing.Â WarmupÂ is the one where learning rate grows from the                    minimum value to the maximum value andÂ AnnealingÂ is the one where it decreases back to the                    minimum value.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read and                    implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read                    about Activation Stats Callbacks, Increasing Batch Size, Activations, 1 Cycle Training, Warmup and                    Annealing, Super Convergence, Learning Rate and Momentum, Colorful Dimension and Histograms and few                    more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Convolutional Neural Network                    :Â https://lnkd.in/dmdtzCzHğŸ“ˆ I have presented the implementation of Increasing Batch                    Size, 1 Cycle Training and Inspecting Momentum and Activations using Fastai and PyTorch here in the                    snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š                    Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes                    :Larger batches have gradients that are more accurate since they are calculated from more data.                    But larger batch size means fewer batches per epoch which means fewer opportunities for the model to                    update                    weights.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6835847378533154816/,"ğŸ† Day 252 ofÂ #300DaysOfData!ğŸ“‘ Channels and Features                    :ChannelsÂ andÂ FeaturesÂ are largely used interchangeably and refer to the size of                    the second axis of a weight matrix which is the number of activations per grid cell after a                    convolution.Â ChannelsÂ refer to the input data i.e colors or activations inside the                    network. Using a stride 2 convolution often increases the number ofÂ FeaturesÂ at the same                    time because the number of activations in the activation map decrease by the factor of 4.ğŸ¯                    On my Journey of Machine Learning and Deep Learning, I have read and implemented from the book ""Deep                    Learning for Coders with Fastai and PyTorch"". Here, I have read about Improving Training Stability                    of Convolutional Neural Networks, Batch Size and Splitting the Dataset, Simple Baseline Network,                    Activations and Kernel Size, Activation Stat Callbacks, Learning Rate, Creating a Learner and                    Training and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Convolutional                    Neural Network :Â https://lnkd.in/dmdtzCzHğŸ“ˆ I have presented the implementation of                    Convolutional Neural Network and Training the Learner using Fastai and PyTorch here in the snapshot.                    I hope you will gain some insights and work on the same. I hope you will also spend some time                    learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep                    Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes                    :Activation StatsÂ callback records mean, standard deviation and histogram of activations of                    every trainable                    layer.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6835413501641539584/,"ğŸ† Day 251 ofÂ #300DaysOfData!ğŸ“‘ Channels and Features :                    ChannelsÂ andÂ FeaturesÂ are largely used interchangeably and refer to the size of                    the second axis of a weight matrix which is the number of activations per grid cell after a                    convolution.Â ChannelsÂ refer to the input data i.e colors or activations inside the                    network. Using a stride 2 convolution often increases the number ofÂ FeaturesÂ at the same                    time because the number of activations in the activation map decrease by the factor of 4.ğŸ¯                    On my Journey of Machine Learning and Deep Learning, I have read and implemented from the book ""Deep                    Learning for Coders with Fastai and PyTorch"". Here, I have read about Convolutional Neural Network,                    Refactoring, Channels and Features, Understanding Convolution Arithmetic, Biases, Receptive Fields,                    Convolution over RGB Image, Stochastic Gradient Descent and few more topics related to the same from                    here. ğŸ“— Notebook:ğŸ”° Convolutional Neural Network                    :Â https://lnkd.in/dmdtzCzHğŸ“ˆ I have presented the implementation of Convolutional                    Neural Network and Training the Learner using Fastai and PyTorch here in the snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for                    Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes                    :TheÂ Receptive FieldÂ is the area of an image that is involved in the calculation of a                    layer. In the deeper layers of network, we have semantically rich features, corresponding to                    largerÂ Receptive                    Fields.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6834668243244875776/,"ğŸ† Day 250 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Convolutions with PyTorch, Rank Tensors, Creating Data Block and Data Loaders,                    Channel of Images, Unsqueeze Method and Unit Axis, Strides and Padding, Understanding the                    Convolutions Equations, Matrix Multiplication, Shared Weights and few more topics related to the                    same from here. ğŸ“— Notebook:ğŸ”° Convolutional Neural Network                    :Â https://lnkd.in/dmdtzCzHğŸ“ˆ I have presented the implementation of Convolutions and                    DataLoaders using Fastai and PyTorch here in the snapshot. I hope you will gain some insights and                    work on the same. I hope you will also spend some time learning the topics from the Book mentioned                    below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and                    PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes :AÂ channelÂ is a single basic                    color in an image. For a regular full color images, there are three channels : red, green and blue.                    Kernels passed to convolutions need to be rank 4                    tensors.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6834311574727991296/,"ğŸ† Day 249 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Convolutional Neural Networks, The Magic of Convolutions, Feature Engineering,                    Kernel and Matrix, Mapping a Convolutional Kernel, Nested List Comprehensions, Matrix                    Multiplications and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”°                    Convolutional Neural Network :Â https://lnkd.in/dmdtzCzHğŸ“ˆ I have presented the                    implementation of Feature Engineering and Mapping a Convolutional Kernel using Fastai and PyTorch                    here in the snapshot. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and                    Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹                    Notes :Feature EngineeringÂ is the process of creating a new transformations of the input                    data in order to make it easier to                    model.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6833945979440500737/,"ğŸ† Day 248 ofÂ #300DaysOfData!ğŸ“‘ Activation Regularization : Activation                    RegularizationÂ is a process of adding the small penalty to the final activations produced by                    the LSTM to make it as small as possible. It is a regularization method very similar to weight                    decay.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read and implemented                    from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read about Activation                    Regularization and Temporal Activation Regularization, Language Model using Long Short Term Memory,                    Weight Decay, Training a Weight Tied Regularized LSTM, Weight Tying and Input Embeddings, Text                    Learner, Cross Entropy Loss Function and few more topics related to the same from here. ğŸ“—                    Notebook:ğŸ”° Language Model from Scratch :Â https://lnkd.in/gVsiHSHQğŸ“ˆ I have                    presented the implementation Language Model using Regularized Long Short Term Memory and Regularized                    Dropout and Activation Regularization using Fastai and PyTorch here in the snapshot. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the topics                    from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders                    with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes                    :DropoutÂ is a regularization technique which randomly changes some activations to zero at a                    training                    time.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6833582298059812864/,"ğŸ† Day 247 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Training Language Model using LSTM, Embedding Layer, Linear Layer, Overfitting and                    Regularization of LSTM, Dropout Regularization, Training or Inference, Bernoulli Method and few more                    topics related to the same from here. ğŸ“— Notebook:ğŸ”° Language Model from Scratch                    :Â https://lnkd.in/gVsiHSHQğŸ“ˆ I have presented the implementation Language Model using                    Long Short Term Memory and Dropout using Fastai and PyTorch here in the snapshot. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the topics                    from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders                    with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes                    :DropoutÂ is a regularization technique which randomly changes some activations to zero at a                    training                    time.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6833218577445732352/,"ğŸ† Day 246 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Exploding and Disappearing Activations, Matrix Multiplication, Architecture of Long                    Short Term Memory and RNN, Sigmoid and Tanh Function, Hidden State and Cell State, Forget Gate,                    Input Gate, Cell Gate and Output Gate, Chunk Method and few more topics related to the same from                    here. ğŸ“— Notebook:ğŸ”° Language Model from Scratch                    :Â https://lnkd.in/gVsiHSHQğŸ“ˆ I have presented the implementation Long Short Term Memory                    using Fastai and PyTorch here in the snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’                    Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes :TanhÂ is just aÂ SigmoidÂ function                    rescaled to the range of -1 and                    1.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6832855846569943040/,"ğŸ† Day 245 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Creating More Signal or Sequence, Cross Entropy Loss Function and Flatten Method,                    Multilayer Recurrent Neural Networks and Activations, Unrolled Representation, Stack and few more                    topics related to the same from here. ğŸ“— Notebook:ğŸ”° Language Model from Scratch                    :Â https://lnkd.in/gVsiHSHQğŸ“ˆ I have presented the implementation Creating more Signal                    and Multilayer Recurrent Neural Network using Fastai and PyTorch here in the snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for                    Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes                    :The single layerÂ Recurrent Neural NetworkÂ performed better thanÂ Multilayer                    Recurrent Neural NetworkÂ because a deeper model leads to exploding and vanishing                    activations.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6832493001026105344/,"ğŸ† Day 244 ofÂ #300DaysOfData!ğŸ“‘ Backpropagation Through Time :Backpropagation                    through TimeÂ is a process of treating a neural network with effectively one layer per time step                    as one big model and calculating gradients on it in the usual way. TheÂ BPTTÂ technique is                    used to avoid running out of memory and time which detaches the history of computation steps in the                    hidden state every few time steps.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Backpropagation Through Time, LMDataLoader Object and Arranging the Dataset,                    Creating Data Loaders, Callbacks and Reset Method, Creating More Signal and few more topics related                    to the same from here. ğŸ“— Notebook:ğŸ”° Language Model from Scratch                    :Â https://lnkd.in/gVsiHSHQğŸ“ˆ I have presented the implementation of Arranging Dataset,                    Creating Data Loaders, Callbacks and Reset Method using Fastai and PyTorch here in the snapshot. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning                    for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes                    :Hidden StateÂ is defined as the activations that are updated at each step of a recurrent                    neural                    network.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6832133072352227328/,"ğŸ† Day 243 ofÂ #300DaysOfData!ğŸ“‘ Backpropagation Through Time : Backpropagation                    through TimeÂ is a process of treating a neural network with effectively one layer per time step                    as one big model and calculating gradients on it in the usual way. TheÂ BPTTÂ technique is                    used to avoid running out of memory and time which detaches the history of computation steps in the                    hidden state every few time steps.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Recurrent Neural Networks, Hidden State of NN, Improving the RNN, Maintaining the                    State of RNN, Unrolled Representation, Backpropagation and Derivatives, Detach Method, Stateful RNN,                    Backpropagation Through Time and few more topics related to the same from here. ğŸ“—                    Notebook:ğŸ”° Language Model from Scratch :Â https://lnkd.in/gVsiHSHQğŸ“ˆ I have                    presented the implementation of Recurrent Neural Networks and Language Model using Fastai and                    PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine                    Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes :Hidden StateÂ is defined as the                    activations that are updated at each step of a recurrent neural                    network.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6831788480637497344/,"ğŸ† Day 242 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Language Model from Scratch using PyTorch, Sequence Tensors, Creating Data Loaders                    and Batchsize, Neural Network Architecture and Linear Layers, Words Embeddings and Activations,                    Weight Matrix, Creating Learner and Training and few more topics related to the same from here.                    ğŸ“— Notebook:ğŸ”° Language Model from Scratch :Â https://lnkd.in/gVsiHSHQğŸ“ˆ I                    have presented the implementation of Creating Data Loaders, Language Model from Scratch and Training                    using Fastai and PyTorch here in the snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’                    Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes : I will create neural network architecture                    that takes three words as input and returns the predictions of the probability of each possible next                    word in the vocab. I will use three standard linear layers. The first linear layer will use only the                    first words embedding as activations. The second layer will use the second words embedding plus the                    first layers output activations and the third layer will use the third words embedding plus the                    second layers output activations. The key effect is that every word is interpreted in the                    information context of any words preceding it. Each of these three layers will use the same weight                    matrix.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6831463849833721857/,"ğŸ† Day 241 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Language Model from Scratch, Data Concatenation and Tokenization, Vocabulary and                    Numericalization, Neural Networks, Independent Variables and Dependent Variable, Sequence of Tensors                    and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Language Model from                    Scratch : https://lnkd.in/gVsiHSHQğŸ“ˆ I have presented the implementation of Preparing                    Sequence of Tensors for Language Model using Fastai and PyTorch here in the snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for                    Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6831100108260577280/,"ğŸ† Day 240 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Siamese Transform Object, Random Splitting, Transformed Collections and Datasets                    Class, Data Loaders, ToTensor and IntToFloatTensor Methods, Data and Batch Normalization and few                    more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Data Munging                    :Â https://lnkd.in/dnGva577ğŸ“ˆ I have presented the implementation of Siamese Transform                    Object and Data Augmentation using Fastai and PyTorch here in the snapshot. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the topics from                    the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with                    Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹                    Notes:ToTensor method converts images to tensors. IntToFloatTensor methodÂ converts the                    tensor of images containing the integers from 0 to 255 to a tensor of floats and divide by 255 to                    make values between 0 and                    1.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeverydayÂ #computervisionÂ #pythonÂ #ai",True
/feed/update/urn:li:activity:6830711091400601600/,"ğŸ† Day 239 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Applying Mid Level Data API for Siamese Pair and Computer Vision, Data Loaders,                    Transforms and Resizing Images, Data Augmentation, Subclasses, Transformed Collections and few more                    topics related to the same from here. ğŸ“— Notebook:ğŸ”° Data Munging                    :Â https://lnkd.in/dnGva577ğŸ“ˆ I have presented the implementation of Siamese Image                    Object and Data Augmentation using Fastai and PyTorch here in the snapshot. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the topics from                    the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with                    Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹                    Notes:Datasets class will apply two or more pipelines in parallel to the same raw object and                    build a tuple with the result. It will automatically do the setup and index into                    aÂ Datasets.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday                    #computervision #python #ai",True
/feed/update/urn:li:activity:6830377548917047296/,"ğŸ† Day 238 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Datasets Class, Transformed Collections, Pipelines, Categorize Method, Data Loaders                    and Data Block, Text Block, Partial Function, Category Block and few more topics related to the same                    from here. ğŸ“— Notebook:ğŸ”° Data Munging :Â https://lnkd.in/dnGva577ğŸ“ˆ I have                    presented the implementation of Datasets Class, Transformed Collections and Data Loaders using                    Fastai and PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I                    hope you will also spend some time learning the topics from the Book mentioned below. Excited about                    the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on                    Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes:Datasets class will apply two or more pipelines                    in parallel to the same raw object and build a tuple with the result. It will automatically do the                    setup and index into                    aÂ Datasets.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6830004103175798784/,"ğŸ† Day 237 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Data Munging, Decorator, Pipeline Method, Transformed Collections, Training and                    Validation Set, Data Loaders Object, Categorize Method, Transformations and few more topics related                    to the same from here. ğŸ“— Notebook:ğŸ”° Data Munging                    :Â https://lnkd.in/dnGva577ğŸ“ˆ I have presented the implementation of Pipeline Class and                    Transformed Collections using Fastai and PyTorch here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai                    and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ–‹ğŸ“‹ Notes: PipelineÂ class helps to compose                    severalÂ TransformsÂ together.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6829292887335821313/,"ğŸ† Day 236 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Data Munging with Fastai, Tokenization and Numericalization, Creating Data Loaders                    and Data Block, Mid Level API, Transforms, Decode Method, Data Augmentation, Cropping and Padding                    and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Data Munging :                    https://lnkd.in/dnGva577ğŸ“ˆ I have presented the implementation of Training Text Classifier                    Model using Discriminative Learning Rates and Gradual Unfreezing using Fastai and PyTorch here in                    the snapshot. I hope you will gain some insights and work on the same. I hope you will also spend                    some time learning the topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and                    Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6828942841591558145/,"ğŸ† Day 235 ofÂ #300DaysOfData!ğŸ“‘ Encoder : EncoderÂ is defined as the model                    which doesn't contain task specific final layers. The termÂ EncoderÂ means much the same                    thing as body when applied to visionÂ CNNÂ butÂ EncoderÂ tends to be more used for                    NLP and generative models.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have                    read about Encoder Model, Text Generation and Classification, Creating the Classifier Data Loaders,                    Embeddings, Data Augmentation, Fine Tuning the Classifier, Discriminative Learning Rates and Gradual                    Unfreezing, Disinformation and Language Models and few more topics related to the same from here.                    ğŸ“— Notebook:ğŸ”° Natural Language Processing :Â https://lnkd.in/dHhUzizğŸ“ˆ I                    have presented the implementation of Training Text Classifier Model using Discriminative Learning                    Rates and Gradual Unfreezing using Fastai and PyTorch here in the snapshot. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the topics from                    the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with                    Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :I                    will train the Text ClassifierÂ with discriminative learning rates and gradual unfreezing. In                    computer vision unfreezing the model at once is common approach but forÂ NLP                    ClassifierÂ unfreezing a few layers at a time will make a real                    difference.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6828580440027267072/,"ğŸ† Day 234 ofÂ #300DaysOfData!ğŸ“‘ Tokenization :Subword TokenizationÂ splits                    words into smaller parts based on the most commonly occurring sub strings. Word                    TokenizationÂ splits a sentence on spaces as well as applying language specific rules to try to                    separate parts of meaning even when there are no spaces. Subword TokenizationÂ provides a way to                    easily scale between character tokenization i.e. using a small subword vocab and word tokenization                    i.e using a large subword vocab and handles every human language without needing language specific                    algorithms to be developed.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have                    read about Numericalization with Fastai, Embedding Matrices, Creating Batches for Language Model,                    Tokenization, Training a Text Classifier, Language Model using DataBlock, Data Loaders, Fine Tuning                    Language Model and Transfer Learning and few more topics related to the same from here. ğŸ“—                    Notebook:ğŸ”° Natural Language Processing :Â https://lnkd.in/dHhUzizğŸ“ˆ I have                    presented the implementation of Creating Data Loaders and Data Block for Language Model using Fastai                    and PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I hope                    you will also spend some time learning the topics from the Book mentioned below. Excited about the                    days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine                    Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :NumericalizationÂ is the process of                    mapping tokens to integers. It involves making a list of all possible levels of that categorical                    variable or the vocab and replacing each level with its index in the                    vocab.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6828204194089041920/,"ğŸ† Day 233 ofÂ #300DaysOfData!ğŸ“‘ Tokenization : Subword TokenizationÂ splits                    words into smaller parts based on the most commonly occurring sub strings. Word                    TokenizationÂ splits a sentence on spaces as well as applying language specific rules to try to                    separate parts of meaning even when there are no spaces. Subword TokenizationÂ provides a way to                    easily scale between character tokenization i.e. using a small subword vocab and word tokenization                    i.e using a large subword vocab and handles every human language without needing language specific                    algorithms to be developed.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have                    read about Word Tokenization, Subword Tokenization, Setup Method, Vocabulary, Numericalization with                    Fastai, Embedding Matrices and few more topics related to the same from here. ğŸ“—                    Notebook:ğŸ”° Natural Language Processing :Â https://lnkd.in/dHhUzizğŸ“ˆ I have                    presented the implementation of Subword Tokenization and Numericalization using Fastai and PyTorch                    here in the snapshot. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and                    Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹                    Notes :NumericalizationÂ is the process of mapping tokens to integers. It involves making a                    list of all possible levels of that categorical variable or the vocab and replacing each level with                    its index in the                    vocab.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6827861935418052608/,"ğŸ† Day 232 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about NLP and Language Model, Self Supervised Learning, Text Preprocessing, Tokenization,                    Numericalization and Embedding Matrix, Subword and Characters, Tokens and few more topics related to                    the same from here. ğŸ“— Notebook:ğŸ”° Natural Language Processing                    :Â https://lnkd.in/dHhUzizğŸ“ˆ I have presented the implementation of Loading the Data and                    Word Tokenization using Fastai and PyTorch here in the snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time learning the topics from the Book                    mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and                    PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :TokenÂ is a element of a list                    created by theÂ TokenizationÂ process which could be a word, a part of a word or subword or                    a single                    character.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6827481348110880768/,"ğŸ† Day 231 ofÂ #300DaysOfData!ğŸ“‘ Random Forest :ğŸ”° Random Forest Model just averages                    the predictions of a number of trees and therefore it can never predict values outside the range of                    the training data.ğŸ”° Random ForestsÂ are not able to extrapolate outside the types of data                    i.e out of domain data.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read                    and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read                    about Tabular Modeling and Neural Networks, Continuous and Categorical Features, Embedding Matrix,                    Mean Squared Error and Regression, Tabular Learner and Learning Rate, Ensembling, Bagging and                    Boosting, Combining Embeddings and few more topics related to the same from here. ğŸ“—                    Notebook:ğŸ”° Tabular Modeling :Â https://lnkd.in/drqXM5UğŸ“ˆ I have presented the                    implementation of Tabular Modeling and Neural Networks and Ensembling using Fastai and PyTorch here                    in the snapshot. I hope you will gain some insights and work on the same. I hope you will also spend                    some time learning the topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and                    Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹                    Notes :EnsemblingÂ is the generalization technique in which the average of the predictions                    of several models are                    used.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6827132163356405760/,"ğŸ† Day 230 ofÂ #300DaysOfData!ğŸ“‹ğŸ–‹ Notes :ğŸ”° Random Forest Model just averages the                    predictions of a number of trees and therefore it can never predict values outside the range of the                    training data.ğŸ”° Random ForestsÂ are not able to extrapolate outside the types of data i.e                    out of domain data.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read and                    implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read                    about The Extrapolation Problem and Random Forest, Finding Out of Domain Data, Root Mean Squared                    Error and Feature Importance, Histograms and few more topics related to the same from here.                    ğŸ“— Notebook:ğŸ”° Tabular Modeling :Â https://lnkd.in/drqXM5UğŸ“ˆ I have                    presented the implementation of Finding Out of Domain Data and RMSE using Fastai and PyTorch here in                    the snapshot. I hope you will gain some insights and work on the same. I hope you will also spend                    some time learning the topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and                    Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6826753785235312640/,"ğŸ† Day 229 ofÂ #300DaysOfData!ğŸ“‹ğŸ–‹ Notes :ğŸ”° Random Forest Model just averages the                    predictions of a number of trees and therefore it can never predict values outside the range of the                    training data.ğŸ”° Random ForestsÂ are not able to extrapolate outside the types of data i.e                    out of domain data.ğŸ”° HereÂ predictionÂ is simply the prediction that theÂ Random                    ForestÂ makes. HereÂ biasÂ is the prediction based on taking the mean of the dependent                    variable. SimilarlyÂ contributionsÂ tells us the total change in prediction due to each of                    the independent variables.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have                    read about Tree Interpreter, Redundant Features, Waterfall Charts or Plots, Random Forest,                    Prediction, Bias and Contributions, The Extrapolation Problem, Unsqueeze Method, Out of Domain Data                    and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Tabular Modeling                    :Â https://lnkd.in/drqXM5UğŸ“ˆ I have presented the implementation of Tree Interpreter,                    Waterfall Plots, Extrapolation Problem using Fastai and PyTorch here in the snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for                    Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjs#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6826394779455111168/,"ğŸ† Day 228 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Removing Redundant Features, Determining Similarity, OOB Score, Partial Dependence                    Plots, Data Leakage, Root Mean Squared Error and few more topics related to the same from here.                    ğŸ“— Notebook:ğŸ”° Tabular Modeling :Â https://lnkd.in/drqXM5UğŸ“ˆ I have                    presented the implementation of Removing Redundant Features and Partial Dependence Plots using                    Fastai and PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I                    hope you will also spend some time learning the topics from the Book mentioned below. Excited about                    the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on                    Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :Standard DeviationÂ of predictions across                    the trees presents the relative confidence of predictions. The model is more consistent when                    theÂ Standard DeviationÂ is                    lower.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6826029489785462784/,"ğŸ† Day 227 ofÂ #300DaysOfData!ğŸ’¡ Random Forest :AÂ Random ForestÂ is a model                    that averages the predictions of a large number of decision trees which are generated by randomly                    varying various parameters that specify what data is used to train the tree and other tree                    parameters.Â BaggingÂ is a particular approach to ensembling or combining the results of                    multiple models together.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read                    and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read                    about Random Forest, Feature Importance, Removing Low Importance Variables, Removing Redundant                    Features, Determining Similarity of Features, Rank Correlation, OOB Score and few more topics                    related to the same from here. ğŸ“— Notebook:ğŸ”° Tabular Modeling                    :Â https://lnkd.in/drqXM5UğŸ“ˆ I have presented the implementation of Random Forest and                    Feature Importance using Fastai and PyTorch here in the snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time learning the topics from the Book                    mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and                    PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :TheÂ Out of Bag                    ErrorÂ orÂ OOBÂ error is a way of measuring prediction error in the training dataset by                    including in the calculation of a rows error trees only where that row was not included in the                    training.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6825677936973094914/,"ğŸ† Day 226 ofÂ #300DaysOfData!ğŸ’¡ Random Forest : AÂ Random ForestÂ is a                    model that averages the predictions of a large number of decision trees which are generated by                    randomly varying various parameters that specify what data is used to train the tree and other tree                    parameters.Â BaggingÂ is a particular approach to ensembling or combining the results of                    multiple models together.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read                    and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read                    about Categorical Variables, Random Forests and Bagging Predictors, Ensembling, Optimal Parameters,                    Out of Bag Error, Tree Variance for Prediction Confidence and Standard Deviation, Model                    Interpretation and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”° Tabular                    Modeling :Â https://lnkd.in/drqXM5UğŸ“ˆ I have presented the implementation of Creating                    Random Forest and Model Interpretation using Fastai and PyTorch here in the snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for                    Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :TheÂ Out of Bag ErrorÂ orÂ OOBÂ error is a way of measuring prediction error in                    the training dataset by including in the calculation of a rows error trees only where that row was                    not included in the                    training.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6825446028653813760/,"ğŸ† Day 225 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Tabular Modeling, Creating the Decision Tree, Leaf Nodes, Root Mean Squared Error,                    DTreeviz Library, Stopping Criterion, Overfitting and few more topics related to the same from here.                    ğŸ“— Notebook:ğŸ”° Tabular Modeling :Â https://lnkd.in/drqXM5UğŸ“ˆ I have                    presented the implementation of Creating Decision Tree and Leaf Nodes using Fastai and PyTorch here                    in the snapshot. I hope you will gain some insights and work on the same. I hope you will also spend                    some time learning the topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and                    Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹                    Notes :max_card parameter in cont_cat_split function dictates what the maximum cardinality is                    number of categories or different values per variable. So anything with more than one unique value                    in this case will be a continuous                    variable.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligenceÂ #learningeveryday",True
/feed/update/urn:li:activity:6824908442608648192/,"ğŸ† Day 224 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Tabular Modeling, Categorical Embeddings, Continuous and Categorical Variables,                    Recommendation System, The Tabular Dataset, Ordinal Columns, Decision Trees, Handling Dates, Tabular                    Pandas and Tabular Proc Object and few more topics related to the same from here. ğŸ“—                    Notebook:ğŸ”° Tabular Modeling :Â https://lnkd.in/drqXM5UğŸ“ˆ I have presented the                    implementation of Handling Dates, Tabular Pandas and Tabular Proc using Fastai and PyTorch here in                    the snapshot. I hope you will gain some insights and work on the same. I hope you will also spend                    some time learning the topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and                    Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹                    Notes :max_card parameter in cont_cat_split function dictates what the maximum cardinality is                    number of categories or different values per variable. So anything with more than one unique value                    in this case will be a continuous                    variable.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligence                    #learningeveryday",True
/feed/update/urn:li:activity:6824189261978333184/,"ğŸ† Day 223 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Deep Learning and Collaborative Filtering, Embedding Matrices, Linear Function, RELU                    and Nonlinear Functions, Sigmoid Range, Forward Propagation Function, Tabular Model and Embedding                    Neural Networks and few more topics related to the same from here. ğŸ“— Notebook:ğŸ”°                    Collaborative Filtering :Â https://lnkd.in/dUus_GwğŸ“ˆ I have presented the implementation                    Deep Learning for Collaborative Filtering and Neural Networks using Fastai and PyTorch here in the                    snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š                    Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :In                    PythonÂ kwargsÂ in a parameter list means ""put any additional keyword arguments into a dict                    called kwargs."" AndÂ kwargsÂ in an argument list means ""insert all key and value pairs in                    the kwargs dict as named arguments                    here.""#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascienceÂ #artificialintelligence",True
/feed/update/urn:li:activity:6823813925008617472/,"ğŸ† Day 222 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Interpreting Embedding and Biases, Principal Component Analysis or PCA, Collab                    Learner, Embedding Distance and Cosine Similarity, Bootstrapping a Collaborative Filtering Model,                    Probabilistic Matrix Factorization or Dot Product Model and few more topics related to the same from                    here. ğŸ“— Notebook:ğŸ”° Collaborative Filtering :Â https://lnkd.in/dUus_GwğŸ“ˆ I                    have presented the implementation Interpreting Biases, Collab Learner Model and Embedding Distance                    using Fastai and PyTorch here in the snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’                    Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :Weight DecayÂ consists of adding sum of                    the squared weights to the loss function. The idea is that the larger the coefficients are, the                    sharper the canyons will be in the loss                    function.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascience                    #artificialintelligence",True
/feed/update/urn:li:activity:6823463602767319040/,"ğŸ† Day 221 ofÂ #300DaysOfData!ğŸ“‘ Embedding :The special layer that indexes into a                    vector using an integer but has its derivative calculated in such a way that it is identical to what                    it would have been if it had done a matrix multiplication with a one hot encoded vector is                    calledÂ Embedding. Multiplying by a one hot encoded matrix using the computational shortcut that                    it can be implemented by simply indexing directly. The thing that multiply the one hot encoded                    matrix is called theÂ Embedding Matrix.ğŸ¯ On my Journey of Machine Learning and Deep                    Learning, I have read and implemented from the book ""Deep Learning for Coders with Fastai and                    PyTorch"". Here, I have read about Collaborative Filtering, Weight Decay or L2 Regularization,                    Overfitting, Creating Embeddings and Weight Matrices, Parameter Module and few more topics related                    to the same from here. ğŸ“— Notebooks:ğŸ”° Collaborative Filtering                    :Â https://lnkd.in/dUus_GwğŸ“ˆ I have presented the implementation of Biases and Weight                    Decay and Matrices using Fastai and PyTorch here in the snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time learning the topics from the Book                    mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and                    PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :Weight DecayÂ consists of adding                    sum of the squared weights to the loss function. The idea is that the larger the coefficients are,                    the sharper the canyons will be in the loss                    function.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascience",True
/feed/update/urn:li:activity:6823093519725424640/,"ğŸ† Day 220 ofÂ #300DaysOfData!ğŸ“‘ Embedding : The special layer that indexes into a                    vector using an integer but has its derivative calculated in such a way that it is identical to what                    it would have been if it had done a matrix multiplication with a one hot encoded vector is                    calledÂ Embedding. Multiplying by a one hot encoded matrix using the computational shortcut that                    it can be implemented by simply indexing directly. The thing that multiply the one hot encoded                    matrix is called theÂ Embedding Matrix.ğŸ¯ On my Journey of Machine Learning and Deep                    Learning, I have read and implemented from the book ""Deep Learning for Coders with Fastai and                    PyTorch"". Here, I have read about Creating DataLoaders, Embedding Matrix, Collaborative Filtering,                    Object Oriented Programming with Python, Inheritance, Module and Forward Propagation Function,                    Batches and Learner, Sigmoid Range and few more topics related to the same from here. ğŸ“—                    Notebooks:ğŸ”° Collaborative Filtering :Â https://lnkd.in/dUus_GwğŸ“ˆ I have presented                    the implementation Embedding, Dot Product Class and Sigmoid Range using Fastai and PyTorch here in                    the snapshot. I hope you will gain some insights and work on the same. I hope you will also spend                    some time learning the topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and                    Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹                    Notes :The mathematical operation of multiplying the elements of two vectors together and then                    summing up the result is calledÂ Dot                    Product.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascience",True
/feed/update/urn:li:activity:6822755055507120128/,"ğŸ† Day 219 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Collaborative Filtering, Learning the Latent Factors, Loss Function and Stochastic                    Gradient Descent, Creating DataLoaders, Batches, Dot Product and Matrix Multiplication and few more                    topics related to the same from here. ğŸ“— Notebooks:ğŸ”° Collaborative Filtering                    :Â https://lnkd.in/dUus_GwğŸ“ˆ I have presented the implementation of Initializing Dataset                    and Creating DataLoaders using Fastai and PyTorch here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai                    and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :The mathematical operation of                    multiplying the elements of two vectors together and then summing up the result is calledÂ Dot                    Product.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascience",True
/feed/update/urn:li:activity:6822366747266424832/,"ğŸ† Day 218 ofÂ #300DaysOfData!ğŸ“‘ Label Smoothing : Label SmoothingÂ is a process                    which replaces all the labels i.e 1s with a number a bit less than 1 and 0s with a number a bit more                    than 0 for training. It will make training more robust even if there is mislabeled data which                    results to be a model that generalizes better at inference.ğŸ¯ On my Journey of Machine                    Learning and Deep Learning, I have read and implemented from the book ""Deep Learning for Coders with                    Fastai and PyTorch"". Here, I have read about Progressive Resizing, Test Time Augmentation, Mixup                    Augmentation, Linear Combinations, Callbacks, Label Smoothing and Cross Entropy Loss Function and                    few more topics related to the same from here. ğŸ“— Notebooks:ğŸ”° Imagenette Classification                    :Â https://lnkd.in/dKyZKhgğŸ“ˆ I have presented the implementation of Progressive                    Resizing, Test Time Augmentation, Mixup Augmentation and Label Smoothing using Fastai and PyTorch                    here in the snapshot. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and                    Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹                    Notes :During inference or validation, creating multiple versions of each image using data                    augmentation and then taking the average or maximum of the predictions for each augmented version of                    the image is calledÂ Test Time                    Augmentation.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascience",True
/feed/update/urn:li:activity:6821999839807590400/,"ğŸ† Day 217 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Imagenette Classification, DataBlock and DataLoaders, Data Normalization and                    Normalize Function, Progressive Resizing and Data Augmentation, Transfer Learning, Mean and Standard                    Deviation and few more topics related to the same from here. ğŸ“— Notebooks:ğŸ”° Imagenette                    Classification : https://lnkd.in/dKyZKhgğŸ“ˆ I have presented the implementation of                    Initializing DataBlock and DataLoaders, Normalization and Progressive Resizing using Fastai and                    PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine                    Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :Progressive Resizing is the process of                    gradually using larger and larger images as training                    progresses.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascience",True
/feed/update/urn:li:activity:6821287265621315584/,"ğŸ† Day 216 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Image Regression and Localization, Assembling the Dataset, Initializing DataBlock                    and DataLoaders, Points and Data Augmentation, Training the Model, Sigmoid Range, MSE Loss Function,                    Transfer Learning and few more topics related to the same from here. ğŸ“— Notebooks:ğŸ”°                    Multilabel Classification :Â https://lnkd.in/dfPshDyğŸ”° Image                    Regression:Â https://lnkd.in/dZgi8P6ğŸ“ˆ I have presented the implementation of                    Initializing DataBlock and DataLoaders and Training Image Regression using Fastai and PyTorch here                    in the snapshot. I hope you will gain some insights and work on the same. I hope you will also spend                    some time learning the topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and                    Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹                    Notes :ğŸ”° nn.CrossEntropyLossÂ for single label classification.ğŸ”°                    nn.BCEWithLogitsLossÂ for multi label classification.ğŸ”° nn.MSELossÂ for                    regression.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascience",True
/feed/update/urn:li:activity:6820960792792334336/,"ğŸ† Day 215 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Multilabel Classification and Threshold, Sigmoid Activation, Overfitting, Image                    Regression, Validation Loss and Metrics, Partial Function and few more topics related to the same                    from here. ğŸ“— Notebooks: ğŸ”° Multilabel Classification : https://lnkd.in/dfPshDyğŸ”°                    Image Regression: https://lnkd.in/dZgi8P6ğŸ“ˆ I have presented the implementation of Training                    the Convolutions with Accuracy and Threshold using Fastai and PyTorch here in the snapshot. I hope                    you will gain some insights and work on the same. I hope you will also spend some time learning the                    topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for                    Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :F.binary_cross_entropyÂ and its module equivalent nn.BCELoss calculate cross entropy on a                    one hot encoded target but don't include the initial sigmoid. Normally,                    F.binary_cross_entropy_with_logits orÂ nn.BCEWithLogitsLossÂ do both sigmoid and binary                    cross entropy in a single function. Similarly for single label                    dataset,Â F.nll_lossÂ orÂ nn.NLLossÂ for the version without initial softmax                    andÂ F.cross_entropyÂ orÂ nn.CrossEntropyLossÂ for the version with initial                    softmax.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascience",True
/feed/update/urn:li:activity:6820551987944288256/,"ğŸ† Day 214 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Binary Cross Entropy Loss Function, DataLoaders and Learner, Getting Model                    Activations, Sigmoid and Softmax Functions, One Hot Encoding, Getting Accuracy, Partial Function and                    few more topics related to the same from here. ğŸ“˜ In case you want to see my Notebook with                    proper documentation is here:Â https://lnkd.in/dfPshDyğŸ“ˆ I have presented the                    implementation of Cross Entropy Loss Functions and Accuracy using Fastai and PyTorch here in the                    snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š                    Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :F.binary_cross_entropyÂ and its module equivalent nn.BCELoss calculate cross entropy on a                    one hot encoded target but don't include the initial sigmoid. Normally,                    F.binary_cross_entropy_with_logits orÂ nn.BCEWithLogitsLossÂ do both sigmoid and binary                    cross entropy in a single function. Similarly for single label                    dataset,Â F.nll_lossÂ orÂ nn.NLLossÂ for the version without initial softmax                    andÂ F.cross_entropyÂ orÂ nn.CrossEntropyLossÂ for the version with initial                    softmax.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascience",True
/feed/update/urn:li:activity:6819864678735339520/,"ğŸ† Day 213 ofÂ #300DaysOfData!ğŸ”° Multilabel Classification:Multilabel Classification                    refers to the problem of identifying the categories of objects in images that may not contain                    exactly one type of object.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have                    read about Lambda Functions, Transformation Blocks such as Image Block and Multi Category Block, One                    Hot Encoding, Data Splitting, DataLoaders, Datasets and DataBlock, Resizing and Cropping and few                    more topics related to the same from here. ğŸ“˜ In case you want to see my Notebook with                    proper documentation is here:Â https://lnkd.in/dfPshDyğŸ“ˆ I have presented the                    implementation of Creating DataBlock and DataLoaders using Fastai and PyTorch here in the snapshot.                    I hope you will gain some insights and work on the same. I hope you will also spend some time                    learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep                    Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :Using a vector of 0s with a 1 in each location that is represented in the data to encode a list                    of integers is calledÂ OneHot                    Encoding.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastaiÂ #datascience",True
/feed/update/urn:li:activity:6819530833196539905/,"ğŸ† Day 212 ofÂ #300DaysOfData!ğŸ”° Multilabel Classification: Multilabel                    Classification refers to the problem of identifying the categories of objects in images that may not                    contain exactly one type of object. ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    I have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here,                    I have read about Questionnaire of Image Classification, Multilabel Classification and Regression,                    Pascal Dataset, Pandas and DataFrames, Constructing DataBlock, Datasets and DataLoaders, Lambda                    Functions and few more topics related to the same from here. ğŸ“˜ In case you want to see my                    Notebook with proper documentation is here: https://lnkd.in/dfPshDyâœ… I have presented the                    implementation of Creating DataBlock and DataLoaders using Fastai and PyTorch here in the snapshot.                    I hope you will gain some insights and work on the same. I hope you will also spend some time                    learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep                    Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :Dataset is a collection that returns a tuple of independent and dependent variable for a single                    item.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastai #datascience",True
/feed/update/urn:li:activity:6819150681950195712/,"ğŸ† Day 211 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Unfreezing and Transfer Learning, Freezing Trained Layers, Discriminative Learning                    Rates, Selecting the Number of Epochs, Deeper Architectures and few more topics related to the same                    from here. ğŸ“˜ In case you want to see my Notebook with proper documentation is                    here:Â https://lnkd.in/d22XqYcâœ… I have presented the implementation of Unfreezing and                    Transfer Learning and Discriminative Learning Rates using Fastai and PyTorch here in the snapshot. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning                    for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :When we first take theÂ SoftmaxÂ and then theÂ Log LikelihoodÂ of that, that                    combination is calledÂ Cross Entropy                    Loss.#66DaysOfDataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6818782201560215552/,"ğŸ† Day 210 ofÂ #300DaysOfData!ğŸ”° Exponential Function:Exponential FunctionÂ is                    defined as e**x where e is a special number approximately equal to 2.718. It is the inverse of                    natural logarithm function.Â Exponential FunctionÂ is always positive and increases very                    rapidly.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read and implemented                    from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read about                    Logarithmic Function, Negative Log Likelihood, Cross Entropy Loss Function, Softmax Function, Model                    Interpretation, Confusion Matrix, Improving the Model, The Learning Rate Finder, Logarithmic Scale                    and few more topics related to the same from here. ğŸ“˜ In case you want to see my Notebook                    with proper documentation is here:Â https://lnkd.in/d22XqYcâœ… I have presented the                    implementation of Cross Entropy Loss, Confusion Matrix and Learning Rate Finder using Fastai and                    PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine                    Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :When we first take theÂ SoftmaxÂ and                    then theÂ Log LikelihoodÂ of that, that combination is calledÂ Cross Entropy                    Loss.#66DaysOfData #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6818493141155364864/,"ğŸ† Day 209 ofÂ #300DaysOfData!ğŸ”° Exponential Function: Exponential FunctionÂ is                    defined as e**x where e is a special number approximately equal to 2.718. It is the inverse of                    natural logarithm function.Â Exponential FunctionÂ is always positive and increases very                    rapidly.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read and implemented                    from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read about Cross                    Entropy Loss Function, Viewing Activations and Labels, Softmax Activation Function, Sigmoid                    Function, Exponential Function, Negative Log Likelihood, Binary Classification and few more topics                    related to the same from here. ğŸ“˜ In case you want to see my Notebook with proper                    documentation is here:Â https://lnkd.in/d22XqYcâœ… I have presented the implementation of                    Softmax Function and Negative Log Likelihood using Fastai and PyTorch here in the snapshot. I hope                    you will gain some insights and work on the same. I hope you will also spend some time learning the                    topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for                    Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :SoftmaxÂ is the multicategory equivalent                    ofÂ Sigmoid.#66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6818017506712657920/,"ğŸ† Day 208 ofÂ #300DaysOfData!ğŸ“‹ğŸ–‹ Notes :I have usedÂ ResizeÂ as an item                    transform with a large size andÂ RandomResizedCropÂ as a batch transform with a smaller                    size.Â RandomResizedCropÂ will be added if min scale parameter is passed in aug transforms                    function as was done inÂ DataBlockÂ call below.ğŸ¯ On my Journey of Machine Learning                    and Deep Learning, I have read and implemented from the book ""Deep Learning for Coders with Fastai                    and PyTorch"". Here, I have read about Image Classification, Localization, Regular Expressions, Data                    Block and Data Loaders, Regex Labeller, Data Augmentation, Presizing, Checking and Debugging Data                    Block, Item and Batch Transformations and few more topics related to the same from here. ğŸ“˜                    In case you want to see my Notebook with proper documentation is here:                    https://lnkd.in/d22XqYcâœ… I have presented the implementation of Creating and Debugging Data                    Block and Data Loaders using Fastai and PyTorch here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai                    and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai                    :Â https://lnkd.in/dfDtfjs#66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6817663749403500544/,"ğŸ† Day 207 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Adding a Nonlinearity, Simple Linear Classifiers, Basic Neural Networks, Weight and                    Bias Tensors, Rectified Linear Unit or RELU Activation Function, Universal Approximation Theorem,                    Sequential Module and few more topics related to the same from here. ğŸ“˜ In case you want to                    see my Notebook with proper documentation is here:Â https://lnkd.in/dSuV5Agâœ… I have                    presented the implementation of Creating Simple Neural Networks using Fastai and PyTorch here in the                    snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š                    Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :Reading Questionnaire is compulsory.                    #66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6817283671880417280/,"ğŸ† Day 206 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about Creating an Optimizer, Linear Module, Weights and Biases, Model Parameters,                    Optimization and Zeroing Gradients, SGD Class, Data Loaders and Learner Class of Fastai and few more                    topics related to the same from here. ğŸ“˜ In case you want to see my Notebook with proper                    documentation is here:Â https://lnkd.in/dSuV5Agâœ… I have presented the implementation of                    Creating Optimizer and Learner Class using Fastai and PyTorch here in the snapshot. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the topics                    from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders                    with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :Adding nonlinearity to linear models.                    #66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6816925021127106560/,"ğŸ† Day 205 ofÂ #300DaysOfData!âš¡ SGD and Minibatches :The process to change or update                    the weights based on the gradients in order to consider some of the details involved in the next                    phase of the learning process is called anÂ Optimization Step. The calculation of average loss                    for a few data items at a time is called aÂ Minibatch. The number of data items in                    theÂ MinibatchÂ is calledÂ Batchsize. A largerÂ BatchsizeÂ means more accurate                    and stable estimate of the dataset gradients from the loss function whereas a                    singleÂ BatchsizeÂ result in an imprecise and unstable gradient.ğŸ¯ On my Journey of                    Machine Learning and Deep Learning, I have read and implemented from the book ""Deep Learning for                    Coders with Fastai and PyTorch"". Here, I have read about Calculating Gradients and Back Propagation,                    Weights, Bias and Parameters, Zeroing Gradients, Training Loop and Learning Rate, Accuracy and                    Evaluation, Creating an Optimizer and few more topics related to the same from here. ğŸ“˜ In                    case you want to see my Notebook with proper documentation is                    here:Â https://lnkd.in/dSuV5Agâœ… I have presented the implementation of Calculating                    Gradients, Accuracy and Training using Fastai and PyTorch here in the snapshot. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the topics from                    the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with                    Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :I                    am starting to explore Optimizer.                    #66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6816575821294137344/,"ğŸ† Day 204 ofÂ #300DaysOfData!âš¡ SGD and Minibatches : The process to change or                    update the weights based on the gradients in order to consider some of the details involved in the                    next phase of the learning process is called anÂ Optimization Step. The calculation of average                    loss for a few data items at a time is called aÂ Minibatch. The number of data items in                    theÂ MinibatchÂ is calledÂ Batchsize. A largerÂ BatchsizeÂ means more accurate                    and stable estimate of the dataset gradients from the loss function whereas a                    singleÂ BatchsizeÂ result in an imprecise and unstable gradient.ğŸ¯ On my Journey of                    Machine Learning and Deep Learning, I have read and implemented from the book ""Deep Learning for                    Coders with Fastai and PyTorch"". Here, I have read about Stochastic Gradient Descent and                    Minibatches, Optimization Step, Batch Size, DataLoader and Dataset, Initializing Parameters, Weights                    and Bias, Backpropagation and Gradients, Loss Function and few more topics related to the same from                    here. ğŸ“˜ In case you want to see my Notebook with proper documentation is                    here:Â https://lnkd.in/dSuV5Agâœ… I have presented the implementation of DataLoader and                    Gradients using Fastai and PyTorch here in the snapshot. I hope you will gain some insights and work                    on the same. I hope you will also spend some time learning the topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’                    Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :A collection that contains the tuples of                    independent and dependent variables is known inÂ PyTorchÂ as                    aÂ Dataset.#66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6816209229150478336/,"ğŸ† Day 203 ofÂ #300DaysOfData!âœ¨ Accuracy and Loss Function : The key difference                    between metric such as accuracy and loss function is that the loss is to drive automated learning                    and the metric is to drive human understanding. The loss must be a function with meaningful                    derivative and metrics focuses on performance of the model. ğŸ¯ On my Journey of Machine                    Learning and Deep Learning, I have read and implemented from the book ""Deep Learning for Coders with                    Fastai and PyTorch"". Here, I have read about Matrix Multiplication, Activation Function, Loss                    Function, Gradients and Slope, Sigmoid Function, Accuracy Metrics and Understanding and few more                    topics related to the same from here. ğŸ“˜ In case you want to see my Notebook with proper                    documentation is here:Â https://lnkd.in/dSuV5Agâœ… I have presented the implementation of                    Loss Function and Sigmoid using Fastai and PyTorch here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai                    and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’                    Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :The key difference between metric such                    as accuracy and loss function is that the loss is to drive automated learning.                    #66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6815852974154231808/,"ğŸ† Day 202 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about The MNIST Loss Function, Matrices and Vectors, Independent Variables, Weights and                    Biases, Parameters, Matrix Multiplication and Dataset Class, Gradient Descent Process and Learning                    Rate, Activation Function and few more topics related to the same from here. ğŸ“˜ In case you                    want to see my Notebook with proper documentation is here:Â https://lnkd.in/dSuV5Agâœ… I                    have presented the implementation of The Dataset Class and Matrix Multiplication using Fastai and                    PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine                    Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :InÂ Neural NetworksÂ the                    equationÂ y=w*x+b, w is called theÂ weightsÂ and the b is called theÂ bias. Together                    the weights and bias make up the                    Parameters.#66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6815478689879343104/,"ğŸ† Day 201 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, I                    have read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I                    have read about The Gradient Descent Process, Initializing Parameters, Calculating Predictions and                    Inspecting, Calculating Loss and MSE, Calculating Gradients and Backpropagation, Stepping the                    Weights and Updating Parameters, Repeating the Process & Stopping the Process and few more                    topics related to the same from here. ğŸ“˜ In case you want to see my Notebook with proper                    documentation is here:Â https://lnkd.in/dSuV5Agâœ… I have presented the implementation of                    The Gradient Descent Process using Fastai and PyTorch here in the snapshot. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the topics from                    the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with                    Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :The Gradient Descent Process. These topics are foundations for Fastai and Deep Learning.                    #66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6815126529903742976/,"ğŸ† Day 200 ofÂ #300DaysOfData!ğŸ“‘ L1 and L2 Norm:Taking the mean of absolute value of                    differences is calledÂ Mean Absolute DifferenceÂ orÂ L1 Norm. Taking the mean of square                    of differences and then taking the square root is calledÂ Root Mean Squared                    ErrorÂ orÂ L2 Norm.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have                    read about Computing Metrics using Broadcasting, Mean Absolute Error, Stochastic Gradient Descent,                    Initializing Parameters, Loss Function, Calculating Gradients, Backpropagation and Derivatives,                    Learning Rate Optimization and few more topics related to the same from here. ğŸ“˜ In case you                    want to see my Notebook with proper documentation is here:Â https://lnkd.in/dSuV5Agâœ… I                    have presented the simple implementation of Stochastic Gradient Descent using Fastai here in the                    snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š                    Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :Stochastic Gradient Descent is interesting. These topics are foundations for Fastai and                    PyTorch.#66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6814030295638016000/,"ğŸ† Day 199 ofÂ #300DaysOfData!ğŸ“‘ L1 and L2 Norm: Taking the mean of absolute value                    of differences is calledÂ Mean Absolute DifferenceÂ orÂ L1 Norm. Taking the mean of                    square of differences and then taking the square root is calledÂ Root Mean Squared                    ErrorÂ orÂ L2 Norm.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have                    read and implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have                    read about Rank of Tensors, Mean Absolute Difference or L1 Norm and Root Mean Squared Error or L2                    Norm, Numpy Arrays and PyTorch Tensors, Computing Metrics using Broadcasting and few more topics                    related to the same from here. ğŸ“˜ In case you want to see my Notebook with proper                    documentation is here:Â https://lnkd.in/dSuV5Agâœ… I have presented the simple                    implementation of Arrays and Tensors, L1 and L2 Norm using Fastai here in the snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for                    Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :These topics are foundations for Fastai and PyTorch.                    #66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6813665705263677440/,"ğŸ† Day 198 ofÂ #300DaysOfData!ğŸ“‘ Data Ethics : Ethics refers to well founded                    standards of right and wrong that prescribe what humans should do. It is the study and development                    of ones ethical standards. Recourse Process, Feedback Loops, Bias are key examples for Data Ethics.                    ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read and implemented from the                    book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read about Data Ethics, Bugs                    and Recourse, Feedback Loops, Bias, Integrating ML with Product Design, Training a Digit Classifier,                    Pixels and Computer Vision, Tenacity and Deep Learning, Pixel Similarity, List Comprehensions and                    few more topics related to the same from here. ğŸ“˜ In case you want to see my Notebook with                    proper documentation is here: https://lnkd.in/dSuV5Agâœ… I have presented the simple                    implementation of Pixels and Computer Vision using Fastai here in the snapshot. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the topics from                    the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with                    Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :I                    am still reading the Data Ethics. I want to continue with next topic along with it.                    #66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6812983546844712960/,"ğŸ† Day 197 ofÂ #300DaysOfData!ğŸ“‘ Data Augmentation:Data AugmentationÂ refers to                    creating random variations of the input data such that they appear different but do not change the                    meaning of the data.Â RandomResizedCropÂ is a specific example ofÂ Data                    Augmentation.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read and                    implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read                    about Training Pretrained Model, Data Augmentation and Transformations, Classification                    Interpretation and Confusion Matrix, Cleaning Dataset, Inference Model and Parameters, Notebooks and                    Widgets and few more topics related to the same from here. ğŸ“˜ In case you want to see my                    Notebook with proper documentation is here:Â https://lnkd.in/dZRiSnpâœ… I have presented                    the implementation of Classification Interpretation, Cleaning Dataset, Inference Model and                    Parameters using Fastai here in the snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’                    Journey on Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai                    :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :Voila is not supported in Colab which is                    mentioned in the Fastai. I will skip this topic though I have prepared all the dependencies for                    deployment. #66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6812621947017424896/,"ğŸ† Day 196 ofÂ #300DaysOfData!ğŸ“‘ Data Augmentation: Data AugmentationÂ refers to                    creating random variations of the input data such that they appear different but do not change the                    meaning of the data.Â RandomResizedCropÂ is a specific example ofÂ Data                    Augmentation.ğŸ¯ On my Journey of Machine Learning and Deep Learning, I have read and                    implemented from the book ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read                    about Data Loaders, Image Block, Resizing, Squishing and Stretching Images, Padding Images, Data                    Augmentation, Image Transformations, Training the Model and Error Rate, Random Resizing and Cropping                    and few more topics related to the same from here. ğŸ“˜ In case you want to see my Notebook                    with proper documentation is here:Â https://lnkd.in/dZRiSnpâœ… I have presented the                    implementation of Data Loaders, Data Augmentation and Training the Model using Fastai here in the                    snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š                    Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :I                    spend time on reading documentation of Fastai.                    #66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6811939688807456768/,"ğŸ† Day 195 ofÂ #300DaysOfData!ğŸ“‘ The Drivetrain Approach :It can be stated as start                    with considering your objective then think about what actions you can take to meet that objective                    and what data you have or can acquire that can help and then build a model that you can use to                    determine the best actions to take to get the best results in terms of your objective.ğŸ¯ On                    my Journey of Machine Learning and Deep Learning, Today I have read and implemented from the book                    ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have Fastai Dependencies and Functions,                    Biased Dataset, Data to Data Loaders, Data Block API, Dependent and Independent Variables, Random                    Splitting, Image Transformations and few more topics related to the same from here. ğŸ“˜ In                    case you want to see my Notebook with proper documentation is                    here:Â https://lnkd.in/dZRiSnpâœ… I have presented the implementation of Gathering Data                    and Initializing Data Loaders using Duck Duck Go and Fastai here in the snapshot. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the topics                    from the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders                    with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :The function for extracting images mentioned in my Notebook updated again. Let's keep                    learning!!#66daysofdataÂ #machinelearningÂ #deeplearningÂ #fastai",True
/feed/update/urn:li:activity:6811543406301483008/,"ğŸ† Day 194 ofÂ #300DaysOfData!ğŸ“‘ The Drivetrain Approach : It can be stated as start                    with considering your objective then think about what actions you can take to meet that objective                    and what data you have or can acquire that can help and then build a model that you can use to                    determine the best actions to take to get the best results in terms of your objective.ğŸ¯ On                    my Journey of Machine Learning and Deep Learning, Today I have read and implemented from the book                    ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read about The Practice of Deep                    Learning, The State of DL, Computer Vision, Text and NLP, Combining Text and Images, Tabular Data                    and Recommendation Systems, The Drivetrain Approach, Gathering Data and Duck Duck Go, Questionnaire                    and few more topics related to the same from here. ğŸ“˜ In case you want to see my Notebook                    with proper documentation is here: https://lnkd.in/dZRiSnpâœ… I have presented the                    implementation of Gathering Data for Object Detection using Duck Duck Go and Fastai here in the                    snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š                    Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :The function for extracting images mentioned here is taken from Fastai. I have just removed the                    decode function which is updated and it might update in Fastai very soon.                    #66daysofdataÂ #machinelearningÂ #deeplearning #fastai",True
/feed/update/urn:li:activity:6810808597090471936/,"ğŸ† Day 193 ofÂ #300DaysOfData!â˜‘ Transfer Learning :Transfer Learning is defined as                    the process of using pretrained model for a task different from what it was originally trained for.                    Fine Tuning is a transfer learning technique that updates the parameters of pretrained model by                    training for additional epochs using a different task from that used for pretraining.ğŸ¯ On                    my Journey of Machine Learning and Deep Learning, Today I have read and implemented from the book                    ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read about Tabular Data and                    Classification, Tabular Data Loaders, Categorical and Continuous Data, Recommendation System and                    Collaborative Filtering, Datasets for Models, Validation Sets and Test Sets, Judgement in Test Sets                    and few more topics related to the same from here. ğŸ“— In case you want to see my Notebook                    with proper documentation is here:Â https://lnkd.in/dJZxQJJâœ… I have presented the                    implementation of Tabular Classification and Recommendation System Model using Fastai here in the                    snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š                    Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes                    :Questionnaire part is interesting. I need to complete all the questions.                    #66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6810451189109407744/,"ğŸ† Day 192 ofÂ #300DaysOfData!â˜‘ Transfer Learning : Transfer Learning is defined as                    the process of using pretrained model for a task different from what it was originally trained for.                    Fine Tuning is a transfer learning technique that updates the parameters of pretrained model by                    training for additional epochs using a different task from that used for pretraining. ğŸ¯ On                    my Journey of Machine Learning and Deep Learning, Today I have read and implemented from the book                    ""Deep Learning for Coders with Fastai and PyTorch"". Here, I have read about Machine Learning and                    Weight Assignment, Neural Networks and Stochastic Gradient Descent, Limitations Inherent to ML,                    Image Recognition, Classification and Regression, Overfitting and Validation Set, Transfer Learning,                    Semantic Segmentation, Sentiment Classification, Data Loaders and few more topics related to the                    same from here. ğŸ“— In case you want to see my Notebook with proper documentation is                    here:Â https://lnkd.in/dJZxQJJâœ… I have presented the implementation of Semantic                    Segmentation and Sentiment Classification using Fastai here in the snapshot. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the topics from                    the Book mentioned below. Excited about the days ahead !!ğŸ“š Deep Learning for Coders with                    Fastai and PyTorchğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :Â https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Notes :                    Please spend some time writing the code. The Model provides start of art results with just few                    lines of code. Let's keep learning !!#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6810178776534671361/,"ğŸ† Day 191 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have started reading and implementing from the book ""Deep Learning for Coders with Fastai and                    PyTorch"". Here, I have read about Deep Learning in Practice, Areas of Deep Learning, A Brief History                    of Neural Networks, Fastai and Jupyter Notebooks, Cat and Dog Classification, Image Loaders,                    Pretrained Models, RESNET and CNNs, Error Rate and few more topics related to the same from here.                    ğŸ“— In case you want to see my Notebook with proper documentation is here:                    https://lnkd.in/dJZxQJJâœ… I have presented the implementation of Cat and Dog Classification                    using Fastai here in the snapshot. I hope you will gain some insights and work on the same. I hope                    you will also spend some time learning the topics from the Book mentioned below. Excited about the                    days ahead !!ğŸ“š Deep Learning for Coders with Fastai and PyTorchğŸ“’ Journey on Machine                    Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Fastai :                    https://lnkd.in/dfDtfjsğŸ“‹ğŸ–‹ Note : If you are reading this, I really appreciate the                    support and appreciation of you guys. We have completed one of the best books out there in Machine                    Learning and Deep Learning recently. If you are around here since the beginning I strongly believe                    that you have also learned a lot. Now, I am starting this amazing book. I will dedicate enough time                    for this one. Let's keep learning !!#66daysofdata #machinelearning #deeplearning",True
/feed/update/urn:li:activity:6809348483267481601/,"ğŸ† Day 190 ofÂ #300DaysOfData!âœ¨ Generative Adversarial Networks :Generative                    Adversarial Networks consist of two deep networks Generator and Discriminator. The Generator                    generates the image as much closer to the true image as possible to fool Discriminator by maximizing                    the cross entropy loss. The Discriminator tries to distinguish the generated images from the true                    images by minimizing the cross entropy loss.ğŸ¯ On my Journey of Machine Learning and Deep                    Learning, Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have                    learned about Deep Convolutional Generative Adversarial Networks, The Generator and The                    Discriminator Blocks, Cross Entropy Loss Function, Adam Optimization Function and few more topics                    related to the same from here. ğŸ“” In case you want to see the Notebook with proper                    documentation is here:Â https://lnkd.in/dYH-vchâœ… I have presented the implementation of                    Training Generator and Discriminator Networks using PyTorch here in the snapshots. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the topics                    from the Book mentioned below. Excited about the days ahead !!ğŸ“š Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ GAN                    :Â https://lnkd.in/dnv3enP#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6809040156960555008/,"ğŸ† Day 189 ofÂ #300DaysOfData!ğŸ† Generative Adversarial Networks :Generative                    Adversarial Networks consist of two deep networks Generator and Discriminator. The Generator                    generates the image as much closer to the true image as possible to fool Discriminator by maximizing                    the cross entropy loss. The Discriminator tries to distinguish the generated images from the true                    images by minimizing the cross entropy loss.ğŸ¯ On my Journey of Machine Learning and Deep                    Learning, Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have                    learned about Deep Convolutional Generative Adversarial Networks, The Generator and The                    Discriminator Networks, Leaky RELU Activation Function and Dying RELU Problem, Batch Normalization,                    Convolutional Layer, Stride and Padding and few more topics related to the same from here.                    ğŸ“” In case you want to see the Notebook with proper documentation is                    here:Â https://lnkd.in/dYH-vchâœ… I have presented the implementation of The Discriminator                    Block and The Generator Block using PyTorch here in the snapshots. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ GAN                    :Â https://lnkd.in/dnv3enP#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6808601616724316160/,"ğŸ† Day 188 ofÂ #300DaysOfData!ğŸ† Generative Adversarial Networks :Generative                    Adversarial Networks consist of two deep networks Generator and Discriminator. The Generator                    generates the image as much closer to the true image as possible to fool Discriminator by maximizing                    the cross entropy loss. The Discriminator tries to distinguish the generated images from the true                    images by minimizing the cross entropy loss.ğŸ¯ On my Journey of Machine Learning and Deep                    Learning, Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have                    learned about Deep Convolutional Generative Adversarial Networks, The Pokemon Dataset, Resizing and                    Normalization, DataLoader, The Generator Block Module, Transposed Convolution Layer, Batch                    Normalization Layer, RELU Activation Function and few more topics related to the same from here. I                    have also read about Inter Quartile Range, Mean Absolute Deviation, Box Plots, Density Plots,                    Frequency Tables and few more topics related to the same. ğŸ“” In case you want to see the                    Notebook with proper documentation is here: https://lnkd.in/dYH-vchâœ… I have presented the                    implementation of The Generator Block and Pokemon Dataset using PyTorch here in the snapshots. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the Topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Dive into Deep                    Leaving :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ GAN                    :Â https://lnkd.in/dnv3enP#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6808227959514767360/,"ğŸ† Day 187 ofÂ #300DaysOfData!ğŸ’¡ Generative Adversarial Networks :Generative                    Adversarial Networks consist of two deep networks Generator and Discriminator. The Generator                    generates the image as much closer to the true image as possible to fool Discriminator by maximizing                    the cross entropy loss. The Discriminator tries to distinguish the generated images from the true                    images by minimizing the cross entropy loss.ğŸ¯ On my Journey of Machine Learning and Deep                    Learning, Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have                    learned about Generator and Discriminator Networks, Binary Cross Entropy Loss Function, Adam                    Optimizer and Normalized Tensors, Gaussian Distribution, Real and Generated Data and few more topics                    related to the same from here. âœ… I have presented a simple implementation of Updating                    Generator and Training Function using PyTorch here in the snapshots. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the Topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Natural Language Inference                    :Â https://lnkd.in/dc4FQtd#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6807867618112884736/,"ğŸ† Day 186 ofÂ #300DaysOfData!ğŸ’¡ Generative Adversarial Networks : Generative                    Adversarial Networks consist of two deep networks Generator and Discriminator. The Generator                    generates the image as much closer to the true image as possible to fool Discriminator by maximizing                    the cross entropy loss. The Discriminator tries to distinguish the generated images from the true                    images by minimizing the cross entropy loss.ğŸ¯ On my Journey of Machine Learning and Deep                    Learning, Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have                    learned about Generative Adversarial Networks, Generator and Discriminator Networks, Updating                    Discriminator and few more topics related to the same from here. I have also read about Recommender                    Systems, Collaborative Filtering, Explicit and Implicit Feedbacks, Recommendation Tasks and few more                    topics related to the same. âœ… I have presented a simple implementation of Generator and                    Discriminator Networks and Optimization using PyTorch here in the snapshot. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the Topics from                    the Book mentioned below. Excited about the days ahead !!ğŸ“š Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Natural Language Inference                    :Â https://lnkd.in/dc4FQtd#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6807154227383681024/,"ğŸ† Day 185 of #300DaysOfData!ğŸ’¡ BERT Model Notes :BERT requires minimal architecture                    changes for sequence level and token level NLP applications such as Single Text Classification, Text                    Pair Classification or Regression and Text Tagging.ğŸ¯ On my Journey of Machine Learning and                    Deep Learning, Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I                    have learned about Loading Pretrained BERT Model and Parameters, The Dataset for Fine Tuning BERT                    Model, Premise, Hypothesis and Input Sequence, Tokenization and Vocabulary, Truncating and Padding                    Tokens, Natural Language Inference and few more topics related to the same from here. ğŸ“” In                    case you want to see the Notebook with proper documentation is                    here:Â https://lnkd.in/d6ST9EAâœ… I have presented the implementation of The Dataset for                    Fine Tuning BERT Model and Generating Training and Test Examples using PyTorch here in the snapshot.                    I hope you will gain some insights and work on the same. I hope you will also spend some time                    learning the Topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Dive                    into Deep Leaving :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep                    Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Natural Language Inference                    :Â https://lnkd.in/dc4FQtd#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6806798383311802368/,"ğŸ† Day 184 ofÂ #300DaysOfData!ğŸ† BERT Model Notes : BERT requires minimal                    architecture changes for sequence level and token level NLP applications such as Single Text                    Classification, Text Pair Classification or Regression and Text Tagging.ğŸ¯ On my Journey of                    Machine Learning and Deep Learning, Today I have read and implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Fine Tuning BERT for Sequence Level and Token Level                    Applications, Single Text Classification, Text Pair Classification or Regression, Text Tagging,                    Question Answering, Natural Language Inference and Pretrained BERT Model, Loading Pretrained BERT                    Model and Parameters, Semantic Textual Similarity, POS Tagging and few more topics related to the                    same from here. ğŸ“” In case you want to see the Notebook with proper documentation is                    here:Â https://lnkd.in/d6ST9EAâœ… I have presented the implementation of Loading                    Pretrained BERT Model and Parameters using PyTorch here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the Topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Natural Language Inference                    :Â https://lnkd.in/dc4FQtd#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6806429113939709952/,"ğŸ† Day 183 ofÂ #300DaysOfData!ğŸ’» Comparing and Aggregating Class :Comparing Class                    compares a word in one sequence with the other sequence that is softly aligned with the word.                    Aggregating Class aggregates the two sets of comparison vectors to infer the logical relationship.                    It feeds the concatenation of both summarization results into MLP function to obtain the                    classification result of the logical relationship.ğŸ¯ On my Journey of Machine Learning and                    Deep Learning, Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I                    have learned about Decomposable Attention Model, Embedding Layer and Linear Layer, Training and                    Evaluating the Attention Model, Natural Language Inference, Entailment, Contradiction and Neutral,                    Pretrained Glove Embedding, SNLI Dataset, Adam Optimizer and Cross Entropy Loss Function, Premises                    and Hypotheses and few more topics related to the same from here.ğŸ“” In case you want to see                    the Notebook with proper documentation is here:Â https://lnkd.in/d5wmqeqâœ… I have                    presented the implementation of Training and Evaluating Attention Model using PyTorch here in the                    snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š                    Dive into Deep Leaving :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep                    Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Sentiment Analysis with Neural Networks                    :Â https://lnkd.in/ddPCySmğŸ“” Natural Language Inference                    :Â https://lnkd.in/dc4FQtd#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6806053676709359616/,"ğŸ† Day 182 ofÂ #300DaysOfData!ğŸ’» Comparing and Aggregating Class : Comparing Class                    compares a word in one sequence with the other sequence that is softly aligned with the word.                    Aggregating Class aggregates the two sets of comparison vectors to infer the logical relationship.                    It feeds the concatenation of both summarization results into MLP function to obtain the                    classification result of the logical relationship.ğŸ¯ On my Journey of Machine Learning and                    Deep Learning, Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I                    have learned about Comparing Word Sequences, Soft Alignment, Multi Layer Perceptron or MLP                    Classifier, Aggregating Comparison Vectors, Linear Layer and Concatenation, Decomposable Attention                    Model, Embedding Layer and few more topics related to the same from here. ğŸ“” In case you                    want to see the Notebook with proper documentation is here:Â https://lnkd.in/d5wmqeqâœ… I                    have presented the implementation of Comparing Class, Aggregating Class and Decomposable Attention                    Model using PyTorch here in the snapshot. I hope you will gain some insights and work on the same. I                    hope you will also spend some time learning the Topics from the Book mentioned below. Excited about                    the days ahead !!ğŸ“š Dive into Deep Leaving :Â https://d2l.ai/index.htmlğŸ“’ Journey on                    Machine Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Sentiment Analysis with                    Neural Networks :Â https://lnkd.in/ddPCySmğŸ“” Natural Language Inference                    :Â https://lnkd.in/dc4FQtd#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6805707848899674112/,"ğŸ† Day 181 ofÂ #300DaysOfData!ğŸ’» Natural Language Inference:Natural Language                    Inference is a study where a hypothesis can be inferred from a premise where both are a text                    sequence. It determines the logical relationship between a pair of text sequences.ğŸ¯ On my                    Journey of Machine Learning and Deep Learning, Today I have read and implemented from the Book ""Dive                    into Deep Learning"". Here, I have learned about Natural Language Inference using Attention Model,                    Multi Layer Perceptron or MLP with Attention Mechanisms, Alignment of Premises and Hypotheses, Word                    Embeddings and Attention Weights and few more topics related to the same from here. ğŸ“” In                    case you want to see the Notebook with proper documentation is                    here:Â https://lnkd.in/d5wmqeqâœ… I have presented the implementation of MLP and Attention                    Mechanism using PyTorch here in the snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the Topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Sentiment Analysis with Neural Networks                    :Â https://lnkd.in/ddPCySmğŸ“” Natural Language Inference                    :Â https://lnkd.in/dc4FQtd#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6805335538728505344/,"ğŸ† Day 180 ofÂ #300DaysOfData!ğŸ’» Natural Language Inference:Natural Language                    Inference is a study where a hypothesis can be inferred from a premise where both are a text                    sequence. It determines the logical relationship between a pair of text sequences.ğŸ¯ On my                    Journey of Machine Learning and Deep Learning, Today I have read and implemented from the Book ""Dive                    into Deep Learning"". Here, I have learned about Natural Language Inference and SNLI Dataset,                    Premises, Hypotheses and Labels, Vocabulary, Padding and Truncation of Sequences, Dataset and                    DataLoader Module and few more topics related to the same from here. Apart from here, I have also                    read about Confusion Matrix and Classification Reports, Frequency Distribution and Word Cloud of                    Text Data. ğŸ“” In case you want to see the Notebook with proper documentation is here:                    https://lnkd.in/dgXnZukâœ… I have presented the implementation of Loading SNLI Dataset using                    PyTorch here in the snapshots. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the Topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ“š Dive into Deep Leaving :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine                    Learning and Deep Learning :Â https://lnkd.in/d-aDKvqğŸ“’ Sentiment Analysis with Neural                    Networks :Â https://lnkd.in/ddPCySmğŸ“” Natural Language Inference                    :Â https://lnkd.in/dc4FQtd#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6805098112533704704/,"ğŸ† Day 179 ofÂ #300DaysOfData!ğŸ’» Natural Language Inference : Natural Language                    Inference is a study where a hypothesis can be inferred from a premise where both are a text                    sequence. It determines the logical relationship between a pair of text sequences. ğŸ¯ On my                    Journey of Machine Learning and Deep Learning, Today I have read and implemented from the Book ""Dive                    into Deep Learning"". Here, I have learned about Natural Language Inference and Dataset, Premise,                    Hypothesis or Entailment, Contradiction and Neutral, The Stanford Natural Language Inference                    Dataset, Reading SNLI Dataset and few more topics related to the same from here. ğŸ“” In case                    you want to see the Notebook with proper documentation is here: https://lnkd.in/dWaNnXNâœ… I                    have presented the implementation of Reading SNLI Dataset using PyTorch here in the snapshot. I hope                    you will gain some insights and work on the same. I hope you will also spend some time learning the                    Topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Dive into Deep                    Leaving :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“’ Sentiment Analysis with Neural Networks                    :Â https://lnkd.in/ddPCySmğŸ“” Natural Language Inference :                    https://lnkd.in/dc4FQtd#66daysofdata #machinelearning #deeplearning",True
/feed/update/urn:li:activity:6804356482197426176/,"ğŸ† Day 178 ofÂ #300DaysOfData!ğŸ’» Sentiment Analysis :Sentiment AnalysisÂ is the                    use of natural language processing, text analysis, computational linguistics, and biometrics to                    systematically identify, extract, quantify and study affective states and subjective information. It                    is widely applied to voice of the customer materials such as reviews and survey responses, online                    and social media and healthcare materials for applications that range from marketing to customer                    service to clinical medicine.ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Word                    Vectors and Vocabulary, Training and Evaluating Bidirectional RNN Model, Sentiment Analysis and One                    Dimensional Convolutional Neural Networks, One Dimensional Cross Correlation Operation, Max Over                    Time Pooling Layer, The Text CNN Model, RELU Activation Function and Dropout Layer and few more                    topics related to the same from here. ğŸ“” In case you want to see the Notebook with proper                    documentation is here : https://lnkd.in/dr_8Zxsâœ…Â I have presented the implementation of                    Text Convolutional Neural Networks using PyTorch here in the snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the Topics from the                    Book mentioned below. Excited about the days ahead !!ğŸ“š Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“” Sentiment Analysis with Neural Networks                    :Â https://lnkd.in/ddPCySm#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6803927410661171201/,"ğŸ† Day 177 ofÂ #300DaysOfData!ğŸ’» Sentiment Analysis : Sentiment AnalysisÂ is the                    use of natural language processing, text analysis, computational linguistics, and biometrics to                    systematically identify, extract, quantify and study affective states and subjective information. It                    is widely applied to voice of the customer materials such as reviews and survey responses, online                    and social media and healthcare materials for applications that range from marketing to customer                    service to clinical medicine.ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Creating Data Iterations, Tokenization and Vocabulary, Truncating and Padding, Recurrent Neural                    Networks Model and Sentiment Analysis, Pretrained Word Vectors and Glove, Bidirectional LSTM and                    Embedding Layer, Linear Layer and Decoding, Encoding and Sequence Data, Xavier Initialization and                    few more topics related to the same from here. ğŸ“” In case you want to see the Notebook with                    proper documentation is here:Â https://lnkd.in/dwyVbqTâœ…Â I have presented the                    implementation of Bidirectional Recurrent Neural Networks Model using PyTorch here in the snapshot.                    I hope you will gain some insights and work on the same. I hope you will also spend some time                    learning the Topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Dive                    into Deep Leaving :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep                    Learning :Â https://lnkd.in/d-aDKvqğŸ“” Sentiment Analysis with Neural Networks                    :Â https://lnkd.in/ddPCySm#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6803282279931117568/,"ğŸ† Day 176 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Natural Language Processing Applications, NLP Architecture and Pretraining, Sentiment Analysis                    and Dataset, Text Classification, Tokenization and Vocabulary, Padding Tokens to Same Length and few                    more topics related to the same from here. Apart from that I have also learned about Named Entity                    Recognition, Frequency Distribution, NLTK, Extending Lists and few more topics related to the same                    from here. ğŸ“” In case you want to see the Notebook with proper documentation is here:                    https://lnkd.in/d4NHNB6âœ… Â I have presented the implementation of Reading the Dataset,                    Tokenization and Vocabulary and Padding to Fixed Length using PyTorch here in the snapshot. I hope                    you will gain some insights and work on the same. I hope you will also spend some time learning the                    Topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Dive into Deep                    Leaving :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“” Sentiment Analysis with Neural Networks :                    https://lnkd.in/ddPCySm#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6802954031057666051/,"ğŸ† Day 175 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Pretraining BERT Model, Cross Entropy Loss Function, Adam Optimization Function, Zeroing                    Gradients, Back Propagation and Optimization, Masked Language Modeling Loss and Next Sentence                    Prediction Loss and few more topics related to the same from here. âœ… I have presented the                    implementation of Pretraining BERT Model, Getting Loss from BERT Model and Training a Neural                    Networks Model using PyTorch here in the snapshots. I hope you will gain some insights and work on                    the same. I hope you will also spend some time learning the Topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6802621626690232320/,"ğŸ† Day 174 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Pretraining BERT Model, Next Sentence Prediction Task and Masked Language Modeling Task,                    Transforming Text into Pretraining Dataset and few more topics related to the same from here. I have                    also learned about Scorer and Example Instances of SpaCy Model, Long Short Term Memory Neural                    Networks, Smiles Vectorizer, Feed Forward Neural Networks and few more topics related to the same.                    âœ¨ I have presented the implementation of Transforming Text into Pretraining Dataset using                    PyTorch here in the snapshots. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the Topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ“š Dive into Deep Leaving :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine                    Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66daysofdataÂ #machinelearningÂ #deeplearning",True
/feed/update/urn:li:activity:6802167174355025920/,"ğŸ† Day 173 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Pretraining BERT Model and Dataset, Defining Helper Functions for Pretraining Tasks,                    Generating Next Sentence Prediction Task, Generating Masked Language Modeling Task, Sequence Tokens                    and few more topics related to the same from here. ğŸ¯ I have also spend some time reading                    the Book ""Speech and Language Processing"". Here, I have read about Learning Skip Gram Embeddings,                    Binary Classifier, Target and Context Embedding, Visualizing Embeddings, Semantic Properties of                    Embeddings and few more topics related to the same from here. âœ¨ I have presented the                    implementation of Generating Next Sentence Prediction Task and Generating Masked Language Modeling                    Task using PyTorch here in the snapshots. I hope you will gain some insights and work on the same. I                    hope you will also spend some time learning the Topics from the Book mentioned below. Excited about                    the days ahead !!ğŸ“š Dive into Deep Leaving :Â https://d2l.ai/index.htmlğŸ“š Speech and                    Language Processing:Â https://lnkd.in/d5n74GeğŸ“’ Journey on Machine Learning and Deep                    Learning :Â https://lnkd.in/d-aDKvq#66daysofdata #machinelearning #deeplearning",True
/feed/update/urn:li:activity:6801821507073929216/,"ğŸ† Day 172 ofÂ #300DaysOfData!ğŸ† Bidirectional Encoder Representations from Transformers                    :ELMO encodes context bidirectionally but uses task specific architectures and GPT is a task                    agnostic but encodes context left to right. BERT encodes context bidirectionally and requires                    minimal architecture changes for a wide range of NLP tasks. The embeddings are the sum of the Token,                    Segment and Positional Embeddings.ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Bidirectional Encoder Representations from Transformers or BERT Architecture, Next Sentence                    Prediction Model, Cross Entropy Loss Function, MLP, BERT Model, Masked Language Modeling, BERT                    Encoder, Pretraining BERT Model and few more topics related to the same from here. âœ¨ I have                    presented the implementation of Next Sentence Prediction and BERT Model using PyTorch here in the                    snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š                    Dive into Deep Leaving :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep                    Learning :Â https://lnkd.in/d-aDKvq#66daysofdata",True
/feed/update/urn:li:activity:6801446696120291328/,"ğŸ† Day 171 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned about BERT                    Encoder Class, Pretraining Tasks, Masked Language Modeling, Multi Layer Perceptron, Forward                    Inference, BERT Input Sequences, Bidirectional Context Encoding and few more topics related to the                    same from here. ğŸ¯ I have also spend some time reading the Book ""Speech and Language                    Processing"". Here, I have learned about Pointwise Mutual Information or PMI, Laplace Smoothing,                    Word2Vec, Skip Gram with Negative Sampling or SGNS, The Classifier, Logistic and Sigmoid Function,                    Cosine Similarity and Dot Product and few more topics related to the same from here. âœ¨ I                    have presented the implementation of Masked Language Modeling and BERT Encoder using PyTorch here in                    the snapshot. I hope you will gain some insights and work on the same. I hope you will also spend                    some time learning the Topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Dive into Deep Leaving :Â https://d2l.ai/index.htmlğŸ“š Speech and Language                    Processing :Â https://lnkd.in/d5n74GeğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66daysofdataÂ #machinelearningÂ #nlp #BERT",True
/feed/update/urn:li:activity:6801080103276175360/,"ğŸ† Day 170 ofÂ #300DaysOfData!ğŸ† Bidirectional Encoder Representations from Transformers                    : ELMO encodes context bidirectionally but uses task specific architectures and GPT is a task                    agnostic but encodes context left to right. BERT encodes context bidirectionally and requires                    minimal architecture changes for a wide range of NLP tasks.ğŸ¯ On my Journey of Machine                    Learning and Deep Learning, Today I have read and implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about BERT Architecture, From Context Independent to Context                    Sensitive, Word Embedding Model and Word2Vec, From Task Specific to Task Agnostic, Embeddings from                    Language Models or ELMO Architecture, Input Representations, Token, Segment and Positional Embedding                    and Learnable Positional Embedding and few more topics related to the same from here. âœ¨ I                    have presented the implementation of BERT Input Representations and BERT Encoder Class using PyTorch                    here in the snapshot. I hope you will gain some insights. I hope you will also spend some time                    learning the Topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Dive                    into Deep Leaving :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep                    Learning :Â https://lnkd.in/d-aDKvq#66daysofdata #machinelearning #nlp #BERT",True
/feed/update/urn:li:activity:6800711770970320897/,"ğŸ† Day 169 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Finding Synonyms and Analogies, Word Embedding Model and Word2Vec, Applying Pretrained Word                    Vectors, Cosine Similarity and few more topics related to the same from here. ğŸ¯ I have also                    spend some time reading the Book ""Speech and Language Processing"". Here, I have read about Cosine                    for measuring similarity, Dot and Inner Products, Weighing terms in the vector, Term Frequency                    Inverse Document Frequency or TFIDF, Collection Frequency, Applications of TFIDF Vector Model and                    few more topics related to the same from here. âœ¨ I have presented the implementation of                    Cosine Similarity and Finding Synonyms and Analogies using PyTorch here in the snapshot. I hope you                    will gain some insights. I hope you will also spend some time learning the Topics from the Book                    mentioned below. Excited about the days ahead !!ğŸ“š Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlğŸ“š Speech and Language Processing                    :Â https://lnkd.in/d5n74GeğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66daysofdata #machinelearning #nlp",True
/feed/update/urn:li:activity:6800358132628377600/,"ğŸ† Day 168 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Subword Embedding, Fast Text and Byte Pair Encoding, Finding Synonyms and Analogies,                    Pretrained Word Vectors, Token Embedding, Central Words and Context Words and few more topics                    related to the same from here. ğŸ¯ I have also spend some time reading the Book ""Speech and                    Language Processing"". Here, I have learned about Words and Vectors, Vectors and Documents, Term                    Document Matrices, Information Retrieval, Row Vector and Context Matrix and few more topics related                    to the same from here. âœ¨ I have presented the implementation of Defining Token Embedding                    Class using PyTorch here in the snapshot. I hope you will gain some insights. I hope you will also                    spend some time learning the Topics from the Book mentioned below. Excited about the days ahead                    !!ğŸ“š Dive into Deep Leaving :Â https://d2l.ai/index.htmlğŸ“š Speech and Language                    Processing :Â https://lnkd.in/d5n74GeğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66daysofdataÂ #machinelearningÂ #nlp",True
/feed/update/urn:li:activity:6799656706734350336/,"ğŸ† Day 167 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Training Skip Gram Model, Loss Function, Applying Word Embedding Model, Negative Sampling,                    Word Embedding with Global Vectors or Glove, Conditional Probability, The Glove Model, Cross Entropy                    Loss Function and few more topics related to the same from here. ğŸ¯ I have also spend some                    time reading the Book ""Speech and Language Processing"". Here, I have learned about Word Relatedness,                    Semantic Field, Semantic Frames and Roles, Connotation and Sentiment, Vector Semantics, Embeddings                    and few more topics related to the same from here. âœ¨ I have presented the implementation of                    Training Word Embedding Model using PyTorch here in the snapshot. I hope you will gain some                    insights. I hope you will also spend some time learning the Topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlğŸ“š Speech and Language Processing                    :Â https://lnkd.in/d5n74GeğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66daysofdata #machinelearning #nlp",True
/feed/update/urn:li:activity:6799304667990896640/,"ğŸ† Day 166 ofÂ #300DaysOfData!ğŸ† Word Embedding : Word Embedding is a term used for                    the representation of words for text analysis typically in the form of a real valued vector that                    encodes the meaning of the word such that the words that are closer in the vector space are expected                    to be similar in meaning.ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Word                    Embedding, Word2Vec, The Skip Gram Model, Embedding Layer, Word Vector, Skip Gram Model Forward                    Calculation, Batch Matrix Multiplication, Binary Cross Entropy Loss Function, Negative Sampling,                    Mask Variables and Padding, Initializing Model Parameters and few more topics related to the same                    from here. âœ¨ I have presented the implementation of Embedding Layer, Skip Gram Model Forward                    Calculation and Binary Cross Entropy Loss Function using PyTorch here in the snapshot. I hope you                    will gain some insights. I hope you will also spend some time learning the Topics from the Book                    mentioned below. Excited about the days ahead !!ğŸ“š Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66daysofdata #machinelearning #nlp",True
/feed/update/urn:li:activity:6798942287402434560/,"ğŸ† Day 165 ofÂ #300DaysOfData!ğŸ† Subsampling : Subsampling is a method that reduces                    data size by selecting a subset of the original data. The subset is specified by choosing a                    parameter. Subsampling attempts to minimize the impact of high frequency words on the training of a                    word embedding model. ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I have                    read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Word                    Embedding, Batches, Loss Function and Padding, Center and Context Words, Negative Sampling, Data                    Loader Instance, Vocabulary, Subsampling, Data Iterations, Mask Variables and few more topics                    related to the same from here. âœ¨ I have presented the implementation of Reading Batches and                    Function for Loading PTB Dataset using PyTorch here in the snapshots. I hope you will gain some                    insights. I hope you will also spend some time learning the Topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvq#66daysofdata #machinelearning #deeplearning",True
/feed/update/urn:li:activity:6798566396478410752/,"ğŸ† Day 164 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Subsampling and Negative Sampling, Word Embedding and Word2Vec, Probability, Reading into                    Batches, Concatenation and Padding, Random Minibatches and few more topics related to the same from                    here. ğŸ¯ I have also spend some time reading the Book ""Speech and Language Processing"".                    Here, I have learned about Vector Semantics and Embeddings, Lexical Semantics, Lemmas and Senses,                    Word Sense Disambiguation, Word Similarity, Principle of Contrast, Representation Learning, Synonymy                    and few more topics related to the same from here. âœ¨ I have presented the implementation                    Negative Sampling using PyTorch here in the snapshot. I hope you will gain some insights. I hope you                    will also spend some time learning the Topics from the Book mentioned below. Excited about the days                    ahead !!ğŸ“š Dive into Deep Leaving : https://d2l.ai/index.htmlğŸ“š Speech and Language                    Processing : https://lnkd.in/d5n74GeğŸ“’ Journey on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66daysofdata #machinelearning #deeplearning",True
/feed/update/urn:li:activity:6798187731852787712/,"ğŸ† Day 163 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Subsampling, Extracting Central Target Words and Context Words, Maximum Context Window Size,                    Penn Tree Bank Dataset and Pretraining Word Embedding and few more topics related to the same from                    here. ğŸ¯ I have also spend some time reading the Book ""Speech and Language Processing"".                    Here, I have learned about Regularization and Overfitting, Manhattan Distance, Lasso and Ridge                    Regression, Multinomial Logistic Regression, Features in MLR, Learning in MLR, Interpreting Models,                    Deriving Gradient Equation and few more topics related to the same from here. ğŸ’» I have                    completed working on Dog Breed Identification: ImageNet Notebook. âœ¨ I have presented the                    implementation of Extracting Central Target Words and Context Words using PyTorch here in the                    snapshot. I hope you will gain some insights. I hope you will also spend some time learning the                    Topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Dive into                    DL:https://d2l.ai/index.htmlğŸ“š Speech and Language Processing:https://lnkd.in/d5n74GeğŸ“” Dog                    Breed Identification:https://lnkd.in/dgdzDmt#66daysofdata",True
/feed/update/urn:li:activity:6797843881661616128/,"ğŸ† Day 162 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Pretrained Text Representations, Word Embedding and Word2Vec, One Hot Vectors, The Skip Gram Model                    and Training, The Continuous Bag of Words Model and Training, Approximate Training, Negative                    Sampling, Hierarchical Softmax, Reading and Processing the Dataset, Subsampling, Vocabulary and few                    more topics related to the same from here. Apart from that, I have also read about Improving                    Chemical Autoencoders Latent Space and Molecular Diversity with Hetero Encoders. ğŸ’» I am                    working on Dog Breed Identification: ImageNet Notebook. The Notebooks is mentioned below.âœ¨ I                    have presented the implementation of Reading and Preprocessing the Dataset, Subsampling and                    Comparison using PyTorch here in the snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the Topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Dive into Deep Leaving:https://d2l.ai/index.htmlğŸ“’                    Journey on Machine Learning and Deep Learning:https://lnkd.in/d-aDKvqğŸ“” Dog Breed                    Identification:https://lnkd.in/dgdzDmt#66daysofdata #machinelearning #deeplearning",True
/feed/update/urn:li:activity:6797440716038524928/,"ğŸ† Day 161 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Defining the Training Functions, Computer Vision, Hyperparameters, Stochastic Gradient Descent                    Optimization Function, Learning Rate Scheduler and Optimization, Training Loss and Validation Loss                    and few more topics related to the same from here. ğŸ¯ I have also spend some time reading                    the Book ""Speech and Language Processing"". Here, I have learned about Gradient for Logistic                    Regression, SGD Algorithm, Minibatch Training and few more topics related to the same from here.                    ğŸ’» I am working on Dog Breed Identification: ImageNet Notebook. The Notebooks is mentioned                    below.âœ¨ I have presented the implementation of Defining the Training Function using PyTorch                    here in the snapshot. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the Topics from the Book mentioned below. Excited about the days                    ahead!!ğŸ“š Dive into Deep Leaving:https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning                    and Deep Learning:https://lnkd.in/d-aDKvqğŸ“š Speech and Language                    Processing:https://lnkd.in/d5n74GeğŸ“” Dog Breed                    Identification:https://lnkd.in/dgdzDmt#66daysofdata",True
/feed/update/urn:li:activity:6797097433441562624/,"ğŸ† Day 160 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about ImageNet Dataset, Obtaining and Organizing the Dataset, Image Augmentation such as Flipping                    and Resizing the Image, Changing Brightness and Contrast of Image, Transfer Learning and Features,                    Normalization of Images and few more topics related to the same from here. ğŸ’» I have                    completed working on Object Recognition on Images: CIFAR10 Notebook. I have started working on Dog                    Breed Identification: ImageNet Notebook. All the Notebooks are mentioned below. âœ¨ I have                    presented the implementation of Image Augmentation and Normalization, Defining Neural Networks Model                    and Loss Function using PyTorch here in the Snapshot. I hope you will gain some insights and work on                    the same. I hope you will also spend some time learning the Topics from the Book mentioned below.                    Excited about the days ahead !!ğŸ“š Dive into Deep                    Leaving:Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“” Object Recognition on                    Images:Â https://lnkd.in/dpvZR5yğŸ“” Dog Breed Identification:                    https://lnkd.in/dgdzDmt#66daysofdata",True
/feed/update/urn:li:activity:6796380544885133312/,"ğŸ† Day 159 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Computer Vision, ResNet Model and Residual Blocks, Xavier Random Initialization, Cross Entropy                    Loss Function, Defining Training Functions, Stochastic Gradient Descent, Learning Rate Scheduler,                    Evaluation Metrics and few more topics related to the same. ğŸ¯ I have also spend some time                    reading the Book ""Speech and Language Processing"". Here, I have learned about Sentiment                    Classification, Learning in Logistic Regression, Conditional MLE, Cost Function and few more topics                    related to the same from here. ğŸ’» I am working on Object Recognition on Images: CIFAR10                    Notebook. The Notebook is mentioned below.âœ¨ I have presented the implementation Defining a                    Training Function using PyTorch here in the Snapshot. Â I hope you will gain some insights and                    work on the same. I hope you will also spend some time learning the Topics from the Book mentioned                    below. Excited about the days ahead !!ğŸ“š Dive into Deep                    Leaving:Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“” Object Recognition on                    Images:Â https://lnkd.in/dpvZR5y#66daysofdata",True
/feed/update/urn:li:activity:6795990578799767552/,"ğŸ† Day 158 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Computer Vision, Image Classification, Image Augmentation and Overfitting, Normalization of                    RGB Channels, Data Loader and Validation Set and few more topics related to the same from here.                    Apart from that, I have learned about Stanford NER Algorithms, NLTK, Named Entity Recognition and                    few more topics related to the same. ğŸ’» I have completed working on Style Transfer using                    Neural Networks Notebook. I have started working on Object Recognition on Images: CIFAR10 Notebook.                    All the Notebooks are mentioned below.âœ¨ I have presented the implementation of Obtaining and                    Organizing the Dataset, Image Augmentation and Normalization using PyTorch here in the Snapshot. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the Topics from the Book mentioned below. Excited about the days ahead !!ğŸ“š Dive into Deep                    Leaving:Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“” Neural Style Transfer                    Notebook:Â https://lnkd.in/dkKqWsQğŸ“” Object Recognition on                    Images:Â https://lnkd.in/dpvZR5y#66daysofdata",True
/feed/update/urn:li:activity:6795692670317555712/,"ğŸ† Day 157 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Computer Vision, Image Classification, CIFAR10 Dataset, Obtaining and Organizing the Dataset,                    Augmentation and few more topics related to the same. Apart from that, I have learned about Data                    Scraping and Scrapy, Named Entity Recognition and SpaCy, Trained Transformer Model using SpaCy,                    Geocoding and few more topics related to the same from here. ğŸ’» I have completed working on                    Style Transfer using Neural Networks Notebook. I have started working on Object Recognition on                    Images: CIFAR10 Notebook. All the Notebooks are mentioned below. âœ¨ I have presented the                    implementation of Obtaining and Organizing the CIFAR10 Dataset here in the Snapshot. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the Topics                    from the Book mentioned above and below. Excited about the days ahead !!ğŸ“š Dive into Deep                    Leaving :Â https://d2l.ai/index.htmlğŸ“’ Journey on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqğŸ“” Neural Style Transfer Notebook                    :Â https://lnkd.in/dkKqWsQğŸ“” Object Recognition on Images:                    https://lnkd.in/dpvZR5y#66daysofdata",True
/feed/update/urn:li:activity:6795280240219324416/,"ğŸ† Day 156 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Creating and Initializing the Composite Images, Synchronization Functions, Adam Optimizer,                    Gram Matrix, Convolutional Neural Networks, Neural Networks Style Transfer, Loss Functions and few                    more topics related to the same from here. ğŸ¯ I have also spend some time reading the Book                    ""Speech and Language Processing"". Here, I have learned about Test Sets and Cross Validation,                    Statistical Significance Testing, Naive Bayes Classifiers, Bootstrapping, Logistic Regression,                    Generative and Discriminative Classifiers, Feature Representation, Sigmoid Classification, Weight                    and Bias Term and few more topics related to the same from here. ğŸ’» I have completed working                    on Style Transfer using Neural Networks. The Notebook is mentioned below but I am still updating.                    ğŸ“š Books:Dive into Deep Leaving :Â https://d2l.ai/index.htmlSpeech and Language                    Processing :Â https://lnkd.in/d5n74GeJourney on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqStyle Transfer Notebook                    :Â https://lnkd.in/dkKqWsQ#66daysofdata #machinelearning",True
/feed/update/urn:li:activity:6794587547495370752/,"ğŸ† Day 155 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Neural                    Networks Style Transfer, Convolutional Neural Networks, Reading the Content and Style Images,                    Preprocessing and Postprocessing the Images, Extracting Image Features, Composite Images, VGG Neural                    Networks, Squared Error Loss Faction, Total Variance Loss Function, Normalization of RGB Channels of                    Images and few more topics related to the same from here. ğŸ’» I am still working on Style                    Transfer using Neural Networks. The Notebook is mentioned below though I am still working on                    it.âœ¨ I have presented the implementation of Function for Extracting Features and Square                    Error Loss Function using PyTorch here in the Snapshot. I hope you will gain some insights and work                    on the same. I hope you will also spend some time learning the Topics from the Book mentioned above                    and below. Excited about the days ahead !!ğŸ“š Books:Dive into Deep Leaving                    :Â https://d2l.ai/index.htmlSpeech and Language Processing                    :Â https://lnkd.in/d5n74GeJourney on Machine Learning and Deep Learning                    :Â https://lnkd.in/d-aDKvqStyle Transfer Notebook                    :Â https://lnkd.in/dkKqWsQ#66DaysOfData",True
/feed/update/urn:li:activity:6794257102643707905/,"ğŸ† Day 154 ofÂ #300DaysOfData!ğŸ‡ Neural Style Transfer Algorithms: It is the task of                    changing the style of an image in one domain to the style of an image in another domain. It                    manipulates images or videos in order to adopt the appearance of another image.ğŸ¯ On my                    Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive                    into Deep Learning"". Here, I have learned about Softmax Cross Entropy Loss Function, Stochastic                    Gradient Descent, CNNs, Neural Networks Style Transfer, Composite Images, RGB Channels,                    Normalization and few more topics related to the same from here. ğŸ¯ I have also spend some                    time reading the Book ""Speech and Language Processing"". Here, I have learned about Optimizing Naive                    Bayes for Sentiment Analysis, Sentiment Lexicons, Naive Bayes as Language Models, Precision, Recall                    and FMeasure, Multi Label and Multinomial Classification and few more topics related to the same                    from here. ğŸ’» I have started working on Style Transfer using Neural Networks. The Notebook                    is mentioned below though I am still working on it. ğŸ“š Books:Dive into DL :                    https://d2l.ai/index.htmlSpeech and Language Processing : https://lnkd.in/d5n74GeJourney on                    ML and DL : https://lnkd.in/d-aDKvqStyle Transfer Notebook : https://lnkd.in/dkKqWsQ",True
/feed/update/urn:li:activity:6793870341581103104/,"ğŸ† Day 153 ofÂ #300DaysOfData!ğŸ‡ Transposed Convolution: Transposed Convolution                    implies that Stride & Padding do not correspond to the number of zeros added around the image                    and the amount of shift in the kernel when sliding it across the input as they would in a                    standardÂ convolutionÂ operation.ğŸ¯ On my Journey of Machine Learning and Deep                    Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have                    learned about Fully Convolutional Neural Networks, Semantic Segmentation Principles, Transposed                    Convolutional Layer, Constructing a Pretrained Neural Networks Model, Global Average Pooling Layer,                    Flattening Layer, Image Processing and Upsampling, Bilinear Interpolation Kernel Function and few                    more topics related to the same from here. âœ¨ I have presented the implementation of Fully                    Convolutional Layer, Pretrained NNs, Bilinear Interpolation Kernel Function and Transposed                    Convolutional Layer using PyTorch here in the Snapshot. I hope you will gain some insights and work                    on the same. I hope you will also spend some time learning the Topics from the Book mentioned above                    and below. Excited about the days ahead !!ğŸ“š Books:Dive into Deep                    Learning:https://d2l.ai/index.htmlJourney on Machine Learning and                    DL:https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6793466490424201216/,"ğŸ† Day 152 ofÂ #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning,                    Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Transposed Convolutional Layer, CNNs, Basic 2D Transposed Convolution, Broadcasting Matrices,                    Kernel Size, Padding, Strides and Channels, Analogy to Matrix Transposition, Matrix Multiplication                    and Matrix Vector Multiplication and few more topics related to the same from here. ğŸ¯ I                    have also spend some time reading the Book ""Speech and Language Processing"". Here, I have learned                    about Naive Bayes and Sentiment Classification, Text Categorization, Spam Detection, Probabilistic                    Classifier, Multinomial NB Classifier, Bag of Words, MLP, Unknown and Stop Words and few more topics                    related to the same from here. âœ¨ I have presented the implementation of Transposed                    Convolution, Padding, Strides and Matrix Multiplication using PyTorch here in the Snapshots. I hope                    you will gain some insights and work on the same. I hope you will also spend some time learning the                    Topics from the Book mentioned above and below. Excited about the days ahead !!ğŸ“š                    Books:Dive into DL:https://d2l.ai/index.htmlSpeech and Language                    Processing:https://lnkd.in/d5n74GeJourney on ML and                    DL:https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6793134938137882624/,"ğŸ† Day 151 ofÂ #300DaysOfData!ğŸ† Sequence to Sequence Model: Sequence to Sequence                    Neural Networks can be built with a modular and reusable Encoder and Decoder Architecture. The                    Encoder Model generates a Thought Vector which is a Dense and fixed Dimension Vector representation                    of the Data. The Decoder Model use Thought Vectors to generate Output Sequences.ğŸ¯ On my                    Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive                    into Deep Learning"". Here, I have learned about Dataset Classes for Custom Semantic Segmentation,                    RGB Channels, Normalization of Images, Random Cropping Operation, Sequence to Sequence Recurrent                    Neural Networks, Label Encoder, One Hot Encoder, Encoding and Vectorization, Long Short Term Memory                    or LSTM and few more topics related to the same from here. âœ¨ I have presented the                    implementation Dataset Classes for Custom Semantic Segmentation using PyTorch here in the Snapshot.                    I hope you will gain some insights and work on the same. I hope you will also spend some time                    learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!ğŸ“š Books:Dive into Deep Learning:Â https://d2l.ai/index.htmlJourney on Machine                    Learning and Deep Learning:Â https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6792779573797900288/,"ğŸ† Day 150 ofÂ #300DaysOfData!ğŸ† Image Segmentation: Image Segmentation is the                    process of partitioning digital image into multiple segments or set of pixels. The goal of                    segmentation is to simplify the representation of image into something meaningful and easier to                    analyze.ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Region Based                    Convolutional Neural Networks, Fast RCNN, Faster RCNN, Mask RCNN, Category Prediction Layer,                    Bounding Boxes Prediction Layer, Support Vector Machines, Rol Pooling Layer and Rol Alignment Layer,                    Pixel Level Semantics, Image Segmentation and Instance Segmentation, Pascal VOC2012 Semantic                    Segmentation, RGB, Data Preprocessing and few more topics related to the same from here. âœ¨ I                    have presented the implementation of Semantic Segmentation and Data Preprocessing using PyTorch here                    in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead !!ğŸ“š Books:Dive into DL:Â https://d2l.ai/index.htmlSpeech and Language                    Processing:Â https://lnkd.in/d5n74GeJourney on ML and DL:Â https://lnkd.in/d-aDKvq",True
/feed/update/urn:li:activity:6792415576045191168/,"ğŸ† Day 149 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Softmax Activation Function, Convolutional Layer, Training the Single Shot Multi Box Detection                    Model, Multi Scale Anchor Boxes, Cross Entropy Loss Function, L1 Normalization Loss Function,                    Average Absolute Error, Accuracy Rate,Category and Offset Losses and few more topics related to                    the same from here. ğŸ¯ I have also spend some time reading the Book ""Speech and Language                    Processing"". Here, I have learned about Backoff and Interpolation, Katz Backoff, Kneser Ney                    Smoothing, Absolute Discounting, The Web and Stupid Backoff, Perplexity Relation to Entropy and few                    more topics related to the same from here. âœ¨ I have presented the implementation of Training                    Single Shot Multi Box Detection Model, Loss and Evaluation Functions using PyTorch here in the                    Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!ğŸ“š Books:Dive into DL: https://d2l.ai/index.htmlSpeech and Language Processing:                    https://lnkd.in/d5n74GeJourney on ML and DL: https://lnkd.in/d-aDKvq",True
/feed/update/urn:li:activity:6792053107560869888/,"ğŸ† Day 148 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Single                    Shot Multi Box Detection Model, Implementation of Tiny SSD Model, Forward Propagation Function, Data                    Reading and Initialization, Object Detection, Multi Scale Feature Block, Global Max Pooling Layer                    and few more topics related to the same from here. ğŸ¯ I have also spend some time reading                    the Book ""Speech and Language Processing"". Here, I have learned about Unknown Words or Out of                    Vocabulary Words, OOV Rate, Smoothing, Laplace Smoothing, Text Classification, Add One Smoothing,                    MLE, Add K Smoothing and few more topics related to the same from here. âœ¨ I have presented                    the implementation of Single Shot Multi Box Detection Model and Dataset Initialization using PyTorch                    here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead !!ğŸ“š Books:Dive into Deep Learning: https://d2l.ai/index.htmlSpeech and                    Language Processing: https://lnkd.in/d5n74GeJourney on Machine Learning and Deep Learning:                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6791686534815879168/,"ğŸ† Day 147 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Single                    Shot Multi Box Detection Algorithm, The Base Neural Network, Height Width Down Sample Block,                    Category Prediction Layer, Bounding Box Prediction Layer, Multiscale Feature Blocks, The Sequential                    API and few more topics related to the same from here. ğŸ¯ I have also spend some time                    reading the Book ""Speech and Language Processing"". Here, I have learned about N Gram Language                    Models, Chain Rule of Probability, Markov Models, Maximum Likelihood Estimation, Relative Frequency,                    Evaluating Language Models, Log Probabilities, Perplexity, Generalization & Zeros, Sparsity and                    few more topics related to the same from here. âœ¨ I have presented the implementation of Base                    SSD Network and Complete SSD Model using PyTorch here in the Snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the Topics from the                    Book mentioned above and below. Excited about the days ahead!!ğŸ“š Books:Dive into Deep                    Learning: https://d2l.ai/index.htmlSpeech and Language Processing:                    https://lnkd.in/d5n74GeJourney on ML and DL: https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6791352104830169088/,"ğŸ† Day 146 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Multiscale Object Detection, Generating Multiple Anchor Boxes, Object Detection, Single Shot                    Multiple Detection Algorithm, Category Prediction Layer, Bounding Boxes Prediction Layer,                    Concatenating Predictions for Multiple Scales, Height and Width Down Sample Block, CNN Layer, RELU                    and Max Pooling Layer and few more topics related to the same from here. ğŸ¯ I have also                    spend some time reading the Book ""Speech and Language Processing"". Here, I have read about Part of                    Speech Tagging, Information Extraction, Named Entity Recognition, Regular Expressions and few more                    topics related to the same from here.âœ¨ I have presented the implementation of Initializing                    Category Prediction Layer and Height & Width Down Sample Block using PyTorch here in the                    Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead!!ğŸ“š Books:Dive into Deep Learning: https://d2l.ai/index.htmlSpeech and                    Language Processing: https://lnkd.in/d5n74GeJourney on ML and DL: https://lnkd.in/d-aDKvq",True
/feed/update/urn:li:activity:6790991753328640001/,"ğŸ† Day 145 of #300DaysOfData!ğŸ† Image Segmentation: Image Segmentation is the process of                    partitioning digital image into multiple segments or set of pixels. The goal of segmentation is to                    simplify the representation of image into something meaningful and easier to analyze. ğŸ¯ On                    my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book                    ""Dive into Deep Learning"". Here, I have learned about Non Maximum Suppression Algorithms, Prediction                    Bounding Boxes, Ground Truth Bounding Boxes, Confidence Level, Batch Size, Intersection Over Union                    Algorithm or Jaccard Index, Aspect Ratios, Bounding Boxes for Prediction, Multi Box Target Function,                    Anchor Boxes and few more topics related to the same from here.âœ¨ I have presented the                    implementation of Initializing Multi Box Anchor Boxes and Initializing Prediction Bounding Boxes                    using PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I hope                    you will also spend some time learning the Topics from the Book mentioned above and below. Excited                    about the days ahead !!ğŸ“š Books :Dive into Deep Learning:                    https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning:                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6790599423916285952/,"ğŸ† Day 144 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Computer Vision, Labeling Training Set Anchor Boxes, Object Detection and Image Recognition, Ground                    Truth Bounding Box Index, Anchor Boxes and Offset Boxes, Intersection Over Union and Jaccard                    Algorithm and few more topics related to the same from here. ğŸ¯ I have also spend some time                    reading the Book ""Speech and Language Processing"". Here, I have learned about Sentence Segmentation,                    The Minimum Edit Distance Algorithm, Viterbi Algorithm, N Gram Language Models, Probability,                    Spelling Correction and Grammatical Error Correction and few more topics related to the same from                    here. âœ¨ I have presented the implementation of Labeling Training Set Anchor Boxes and                    Initializing Offset Boxes using PyTorch here in the Snapshot. I hope you will gain some insights and                    work on the same. I hope you will also spend some time learning the Topics from the Book mentioned                    above and below. Excited about the days ahead !!ğŸ“š Books :Dive into Deep Learning:                    https://d2l.ai/index.htmlSpeech and Language Processing: https://lnkd.in/d5n74GeJourney on                    Machine Learning and DL: https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6790208113246003200/,"ğŸ† Day 143 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Computer Vision, Generating Multiple Anchor Boxes, Batch Size, Coordinate Values, Intersection Over                    Union Algorithm, Jaccard Index, Computation Complexity, Sizes and Ratios and few more topics related                    to the same from here. ğŸ¯ I have also spend some time reading the Book ""Speech and Language                    Processing"". Here, I have learned about Byte Pair Encoding Algorithm for Tokenization, Subword                    Tokens, Wordpiece and Greedy Tokenization Algorithm, Maximum Matching Algorithm, Word Normalization,                    Lemmatization and Stemming, The Porter Stemmer and few more Topics related to the same from here.                    âœ¨ I have presented the implementation of Generating Anchor Boxes and Intersection Over Union                    Algorithm using PyTorch here in the Snapshots. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the Topics from the Book mentioned above and                    below. Excited about the days ahead !!ğŸ“š Books :Dive into Deep Learning:                    https://d2l.ai/index.htmlSpeech and Language Processing: https://lnkd.in/d5n74GeJourney on                    Machine Learning and Deep Learning: https://lnkd.in/d-aDKvq",True
/feed/update/urn:li:activity:6789886986506248192/,"ğŸ† Day 142 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Computer Vision, Anchor Boxes, Object Detection Algorithms, Bounding Boxes, Generating Multiple                    Anchor Boxes, Computation Complexity, Sizes and Ratios and few more topics related to the same from                    here. ğŸ¯ I have also spend some time reading the Book ""Speech and Language Processing"".                    Here, I have learned about Text Normalization, Unix Tools for Crude Tokenization and Normalization,                    Word Tokenization, Named Entity Detection, Penn Treebank Tokenization and few more topics related to                    the same from here. âœ¨ I have presented the implementation of Generating Anchor Boxes, Object                    Detection and Bounding Boxes using PyTorch here in the Snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time learning the Topics from the Book                    mentioned above and below. Excited about the days ahead !!ğŸ“š Books :Dive into Deep                    Learning: https://d2l.ai/index.htmlSpeech and Language Processing:                    https://lnkd.in/d5n74GeJourney on Machine Learning and Deep Learning:                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6789518571232886784/,"ğŸ† Day 141 of #300DaysOfData!ğŸ¯ On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Object                    Detection and Object Recognition, Image Classification and Computer Vision, Images and Bounding                    Boxes, Target Location and Axis Coordinates and few more topics related to the same from here.                    ğŸ¯ I have also spend some time reading the Book ""Speech and Language Processing"". Here, I                    have learned about Regular Expressions, Disjunction, Grouping and Precedence, Precision and Recall,                    Substitution and Capture Groups, Lookahead Assertions, Words, Corpora and few more topics related to                    the same. âœ¨ I have presented the simple implementation of Object Detection and Bounding                    Boxes using PyTorch here in the Snapshots. I hope you will gain some insights and work on the same.                    I hope you will also spend some time learning the Topics from the Book mentioned above and below.                    Excited about the days ahead !!ğŸ“š Books :Dive into Deep Learning:                    https://d2l.ai/index.htmlSpeech and Language Processing: https://lnkd.in/d5n74GeJourney on                    Machine Learning and Deep Learning: https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6789144652835500032/,"ğŸ† Day 140 of #300DaysOfData!âœ¨ On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Fine                    Tuning the Model, Pretrain Neural Networks, Normalization of Images, Mean and Standard Deviation,                    Defining and Initializing the Model, Cross Entropy Loss Function, Data Loader Class, Learning Rate                    and Stochastic Gradient Descent, Model Parameters, Transfer Learning, Source Model and Target Model,                    Weights and Biases and few more topics related to the same from here. âœ¨ I have presented the                    implementation of Normalization of Images, Flipping and Cropping the Images and Training Pretrained                    Model using PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I                    hope you will also spend some time learning the Topics from the Book mentioned above and below.                    Excited about the days ahead !!ğŸ“š Books :Dive into Deep Learning:                    https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning:                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6788796034521812992/,"ğŸ† Day 139 of #300DaysOfData!âœ¨ On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Image                    Augmentation, CIFAR10 Dataset, Using a Multi GPU Training Model, Fine Tuning the Model, Overfitting,                    Pretrain Neural Network, Target Initialization, ResNet Model, ImageNet Dataset, Normalization of RGB                    Images, Mean and Standard Deviation, Torch Vision Module, Flipping and Cropping Images, Adam                    Optimization, Cross Entropy Loss Function and few more topics related to the same from here.                    âœ¨ I have presented the implementation of Training the Model with Image Augmentation and                    Normalization of Images using PyTorch here in the Snapshot. I hope you will gain some insights and                    work on the same. I hope you will also spend some time learning the Topics from the Book mentioned                    above and below. Excited about the days ahead !!ğŸ“š Books :Dive into Deep Learning:                    https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning:                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6788440101018968064/,"ğŸ† Day 138 of #300DaysOfData!âœ¨ On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Computer Vision Applications, Image Augmentation, Deep Neural Networks, Common Image Augmentation                    Method such as Flipping and Cropping, Horizontal Flipping and Vertical Flipping, Changing the Color                    of Images, Overlying Multiple Image Augmentation Methods, CIFAR10 Dataset, Torch Vision Module and                    Random Color Jitter Instance and few more topics related to the same from here. âœ¨ I have                    presented the Implementation of Flipping and Cropping the Images and Changing the Color of Images                    using PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I hope                    you will also spend some time learning the Topics from the Book mentioned above and below. Excited                    about the days ahead !!ğŸ“š Books :Dive into Deep Learning:                    https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning:                    https://lnkd.in/d-aDKvqLeNet Architecture Implementation:                    https://lnkd.in/d9hsATK#66DaysOfData",True
/feed/update/urn:li:activity:6787717870374658049/,"ğŸ† Day 137 of #300DaysOfData!âœ¨ On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Optimization and Synchronization, ResNet Neural Networks Architecture, Convolutional Layer, Batch                    Normalization Layer, Strides and Padding, The Sequential API, Parameter Initialization and                    Logistics, Minibatch Gradient Descent, Training ResNet Model, Stochastic Gradient Descent Optimizer,                    Cross Entropy Loss Function, Back Propagation, Parallelization and few more topics related to the                    same from here. âœ¨ I have presented the implementation of ResNet Architecture, Initialization                    and Training the Model using PyTorch here in the Snapshots. I hope you will gain some insights and                    work on the same. I hope you will also spend some time learning the Topics from the Book mentioned                    above and below. Excited about the days ahead !!ğŸ“š Books :Dive into Deep Learning:                    https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning:                    https://lnkd.in/d-aDKvqLeNet Architecture Implementation:                    https://lnkd.in/d9hsATK#66DaysOfData",True
/feed/update/urn:li:activity:6787359671473729536/,"ğŸ† Day 136 of #300DaysOfData!ğŸ† Adam Optimizer :Adam uses exponential weighted moving                    averages also known as Leaky Averaging to obtain an estimate of both momentum and also the second                    moment of the gradient.âœ¨ On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Training on                    Multiple GPUs, LeNet Architecture, Data Synchronization, Model Parallelism, Data Broadcasting, Data                    Distribution, Optimization Algorithms, Implementation Back Propagation, Model Animation, Cross                    Entropy Loss Function, Convolutional Layer, RELU Activation Function, Matrix Multiplication, Average                    Pooling Layer and few more topics related to the same from here. âœ¨ I have presented the                    implementation of Data Distribution, Data Synchronization and Training Function using PyTorch here                    in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend                    some time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!ğŸ“š Books :Dive into Deep Learning: https://d2l.ai/index.htmlJourney on Machine                    Learning and DL: https://lnkd.in/d-aDKvqLeNet Architecture Implementation:                    https://lnkd.in/d9hsATK#66DaysOfData",True
/feed/update/urn:li:activity:6786984012087869440/,"ğŸ† Day 135 of #300DaysOfData! ğŸ† Adam Optimizer :Adam uses exponential weighted moving                    averages also known as Leaky Averaging to obtain an estimate of both momentum and also the second                    moment of the gradient.âœ¨ On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Asynchronous Computation, Barriers and Blockers, Improving Computation and Memory Footprint,                    Automatic Parallelism, Parallel Computation and Communication, Training on Multiple GPUs, Splitting                    the Problem, Data Parallelism, Network Partitioning, Layer Wise Partitioning, Data Parallel                    Partitioning and few more topics related to the same from here. âœ¨ I have presented the                    implementation of Initializing Model Parameters and Defining LeNet Model using PyTorch here in the                    Snapshot. I am still working on the Implementation of LeNet Model. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the Topics from the                    Book mentioned above and below. Excited about the days ahead !!ğŸ“š Books :Dive into Deep                    Learning: https://d2l.ai/index.htmlJourney on Machine Learning and DL:                    https://lnkd.in/d-aDKvqLeNet Architecture Implementation:                    https://lnkd.in/d9hsATK#66DaysOfData",True
/feed/update/urn:li:activity:6786613592331558913/,"ğŸ† Day 134 of #300DaysOfData!ğŸ† Adam Optimizer :Adam uses exponential weighted moving                    averages also known as Leaky Averaging to obtain an estimate of both momentum and also the second                    moment of the gradient. It combines the features of many optimization algorithms. It uses EWMA on                    minibatch Stochastic Gradient Descent. âœ¨ On my Journey of Machine Learning and Deep                    Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have                    learned about Model Computational Performance, Compilers and Interpreters, Symbolic Programming and                    Imperative Programming, Hybrid Programming, Dynamic Computations Graph, Hybrid Sequential,                    Acceleration by Hybridization, Multi Layer Perceptrons, Asynchronous Computation and few more topics                    related to the same from here. âœ¨ I have presented the implementation of Hybrid Sequential,                    Acceleration by Hybridization and Asynchronous Computation using PyTorch here in the Snapshots. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the Topics from the Book mentioned above and below. Excited about the days ahead !!ğŸ“š Books                    :Dive into Deep Learning: https://d2l.ai/index.htmlJourney on Machine Learning and Deep                    Learning: https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6786281207773581312/,"ğŸ† Day 133 of #300DaysOfData!ğŸ† Adam Optimizer :Adam uses exponential weighted moving                    averages also known as Leaky Averaging to obtain an estimate of both momentum and also the second                    moment of the gradient. It combines the features of many optimization algorithms. It uses EWMA on                    minibatch Stochastic Gradient Descent.âœ¨ On my Journey of Machine Learning and Deep Learning,                    Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Learning Rate Scheduling, Square Root Scheduler, Factor Scheduler, Learning Rate and                    Polynomial Decay, Multi Factor Scheduler, Piecewise Constant, Optimization and Local Minimum, Cosine                    Scheduler and few more topics related to the same from here.  âœ¨ I have presented the                    implementation of Multi Factor Scheduler and Cosine Scheduler using PyTorch here in the Snapshots. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the Topics from the Book mentioned above and below. Excited about the days ahead !!ğŸ“š Books                    :Dive into Deep Learning: https://d2l.ai/index.htmlJourney on Machine Learning and Deep                    Learning: https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6785546237044121600/,"ğŸ† Day 132 of #300DaysOfData!ğŸ† Adam Optimizer : Adam uses exponential weighted moving                    averages also known as Leaky Averaging to obtain an estimate of both momentum and also the second                    moment of the gradient. It combines the features of many optimization algorithms. It uses EWMA on                    minibatch Stochastic Gradient Descent. âœ¨ On my Journey of Machine Learning and Deep                    Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have                    learned about Adam and Yogi Optimization Algorithms, Variance, Minibatch SGD, Learning Rate                    Scheduling, Weight Vectors, Convolutional Layer, Linear Layer, Max Pooling Layer, Sequential API,                    RELU, Cross Entropy Loss, Schedulers, Overfitting and few more topics related to the same from here.                    âœ¨ I have presented the implementation of LeNet Architecture and Yogi Optimization Algorithm                    using PyTorch here in the Snapshots. I hope you will gain some insights and work on the same. I hope                    you will also spend some time learning the Topics from the Book mentioned above and below. Excited                    about the days ahead !!ğŸ“š Books :Dive into Deep Learning:                    https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning:                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6785173205247123456/,"ğŸ† Day 131 of #300DaysOfData!ğŸ† RMSProp Optimization Algorithm :RMSProp is a gradient                    based optimization algorithm that utilizes the magnitude of recent gradients to normalize the                    gradients. It deals with Adagrad's radically diminishing learning rates. It divides the learning                    rate by an exponentially decaying average of squared gradients. âœ¨ On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Adadelta Optimization Algorithms, Learning Rates, Leaky                    Averages, Momentum, Gradient Descent, Concise Implementation of Adadelta, Adam Optimization                    Algorithms, Vectorization and Minibatch SGD, Weighting Parameters, Normalization, Concise                    Implementation of Adam Algorithms and few more topics related to the same from here. âœ¨ I                    have presented the Implementation of Adadelta Optimization Algorithm and Adam Optimization Algorithm                    from scratch using PyTorch here in the Snapshot. I hope you will also spend some time learning the                    Topics from the Book mentioned above and below. Excited about the days ahead !!ğŸ“š Books                    :Dive into Deep Learning: https://d2l.ai/index.htmlJourney on Machine Learning and Deep                    Learning: https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6784437587940515841/,"ğŸ† Day 130 of #300DaysOfData!ğŸ† RMSProp Optimization Algorithm : RMSProp is a gradient                    based optimization algorithm that utilizes the magnitude of recent gradients to normalize the                    gradients. It deals with Adagrad's radically diminishing learning rates. It divides the learning                    rate by an exponentially decaying average of squared gradients. âœ¨ On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about RMSProp Optimization Algorithm, Learning Rate, Leaky Averages                    and Momentum Method, Implementation of RMSProp from scratch, Gradient Descent Algorithm,                    Preconditioning and few more topics related to the same from here. âœ¨ I have presented the                    implementation of RMSProp Optimization Algorithm from scratch using PyTorch here in the Snapshot. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the Topics from the Book mentioned above and below. Excited about the days ahead!!ğŸ“š Books                    :Dive into Deep Learning: https://d2l.ai/index.htmlJourney on Machine Learning and Deep                    Learning: https://lnkd.in/d-aDKvqLogistic Regression:                    https://lnkd.in/d2-VBbt#66DaysOfData",True
/feed/update/urn:li:activity:6784088046171131904/,"ğŸ† Day 129 of #300DaysOfData!ğŸ† Stochastic Gradient Descent :Stochastic Gradient Descent                    is an iterative method for optimizing an objective function with suitable differentiable properties.                    It is a variation of the gradient descent algorithm that calculates the error and updates the                    model.âœ¨ On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Adagrad Optimization                    Algorithms, Sparse Features and Learning Rates, Preconditioning, Stochastic Gradient Descent                    Algorithm, The Algorithms, Implementation of Adagrad from Scratch, Deep Learning and Computational                    Constraints, Learning Rates and few more Topics related to the same from here.âœ¨ I have                    presented the implementation Adagrad Optimization Algorithm from Scratch using PyTorch here in the                    Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead!!ğŸ“š Books :Dive into Deep Learning: https://d2l.ai/index.htmlJourney on ML and                    DL: https://lnkd.in/d-aDKvqLogistic Regression: https://lnkd.in/d2-VBbt#66DaysOfData",True
/feed/update/urn:li:activity:6783699897343340544/,"ğŸ† Day 128 of #300DaysOfData!ğŸ† Stochastic Gradient Descent :Stochastic Gradient Descent                    is an iterative method for optimizing an objective function with suitable differentiable properties.                    It is a variation of the gradient descent algorithm that calculates the error and updates the                    model.âœ¨ On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about The Momentum Method,                    Stochastic Gradient Descent, Leaky Averages, Variance, Accelerated Gradient, An Ill Conditioned                    Problem and Convergence, Effective Sample Weight, Practical Experiments, Implementation of Momentum                    with SGD, Theoretical Analysis, Quadratic Convex Functions, Scalar Functions and few more topics                    related to the same from here. âœ¨ I have presented the implementation of Momentum Method,                    Effective Sample Weight and Scalar Functions using PyTorch here in the Snapshots. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the Topics                    from the Book mentioned above and below. Excited about the days ahead!!ğŸ“š Books:Dive                    into Deep Learning: https://d2l.ai/index.htmlJourney on ML and DL:                    https://lnkd.in/d-aDKvqLogistic Regression: https://lnkd.in/d2-VBbt#66DaysOfData",True
/feed/update/urn:li:activity:6783358419265961984/,"ğŸ† Day 127 of #300DaysOfData!ğŸ† Stochastic Gradient Descent :Stochastic Gradient Descent                    is an iterative method for optimizing an objective function with suitable differentiable properties.                    It is a variation of the gradient descent algorithm that calculates the error and updates the                    model.âœ¨ On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Stochastic Gradient                    Descent, Dynamic Learning Rate, Exponential Decay and Polynomial Decay, Convergence Analysis for                    Convex Objectives, Stochastic Gradient and Finite Samples, Minibatch Stochastic Gradient Descent,                    Vectorization and Caches, Matrix Multiplications, Minibatches, Variance, Implementation of Gradients                    and few more topics related to the same from here. âœ¨ I have presented the implementation of                    Stochastic Gradient Descent and Minibatch Stochastic Gradient Descent using PyTorch here in the                    Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead!!ğŸ“š Books:Dive into Deep Learning: https://d2l.ai/index.htmlJourney on Machine                    Learning and DL: https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6782996902112702464/,"ğŸ† Day 126 of #300DaysOfData!ğŸ† Gradient Descent :Gradient Descent is an optimization                    algorithm which is used to minimize the differentiable function by iteratively moving in the                    direction of steepest descent as defined by the negative of the Gradient.âœ¨ On my Journey of                    Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Multivariate Gradient Descent, Adaptive Methods, Learning                    Rate, Newtons Method, Taylor Expansion, Hessian Function, Gradient and Backpropagation, Nonconvex                    Function, Convergence Analysis, Linear Convergence, Preconditioning, Gradient Descent with Line                    Search, Stochastic Gradient Descent, Loss Functions and few more topics related to the same from                    here. âœ¨ I have presented the Implementation of Newtons Method, Non Convex Functions and                    Stochastic Gradient Descent using PyTorch here in the Snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time learning the Topics from the Book                    mentioned above and below. Excited about the days ahead!!ğŸ“š Books:Dive into Deep                    Learning: https://d2l.ai/index.htmlJourney on Machine Learning and DL:                    https://lnkd.in/d-aDKvqLogistic Regression Implementation:                    https://lnkd.in/d2-VBbt#66DaysOfData",True
/feed/update/urn:li:activity:6782651003943583744/,"ğŸ† Day 125 of #300DaysOfData!ğŸ† Gradient Descent : Gradient Descent is an optimization                    algorithm which is used to minimize the differentiable function by iteratively moving in the                    direction of steepest descent as defined by the negative of the Gradient. âœ¨ On my Journey of                    Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Convexity and Second Derivatives, Constrained Optimization,                    Lagrangian Function and Multipliers, Penalties, Projections, Gradient Clipping, Stochastic Gradient                    Descent, One Dimensional Gradient Descent, Objective Function, Learning Rate, Local Minimum and                    Global Minimum, Multivariate Gradient Descent and few more Topics related to the same from here.                    âœ¨ I have presented the Implementation of One Dimensional Gradient Descent, Local Minima and                    Multivariate Gradient Descent using PyTorch here in the Snapshots. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the Topics from the                    Book mentioned above and below. Excited about the days ahead !!ğŸ“š Books:Dive into Deep                    Learning:https://d2l.ai/index.htmlJourney on Machine Learning and                    DL:https://lnkd.in/d-aDKvqLogistic Regression:https://lnkd.in/d2-VBbt#66DaysOfData",True
/feed/update/urn:li:activity:6782278598943367168/,"ğŸ† Day 124 of #300DaysOfData!ğŸ† Transformer :Transformer is an architecture for                    transforming one sequence into another one with the help of two parts, Encoder and Decoder.âœ¨                    On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book                    ""Dive into Deep Learning"". Here, I have learned about Optimization Algorithms and Deep Learning,                    Objective Function and Minimization, Goal of Optimization, Generalization Error, Training Error,                    Risk Function and Empirical Risk Function, Optimization Challenges, Local Minimum and Global                    Minimum, Saddle Points, Hessian Matrix and Eigenvalues, Vanishing Gradients, Convexity, Convex Sets                    and Functions, Jensen's Inequality and few more Topics related to the same from here. âœ¨ I                    have presented the Implementation of Local Minima, Saddle Points, Vanishing Gradients and Convex                    Functions using PyTorch here in the Snapshots. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the Topics from the Book mentioned above and                    below. Excited about the days ahead !!ğŸ“š Books:Dive into Deep                    Learning:https://d2l.ai/index.htmlJourney on Machine Learning and Deep                    Learning:https://lnkd.in/d-aDKvqLogistic Regression:https://lnkd.in/d2-VBbt#66DaysOfData",True
/feed/update/urn:li:activity:6781962520933474304/,"ğŸ† Day 123 of #300DaysOfData!ğŸ† Transformer :Transformer is an architecture for                    transforming one sequence into another one with the help of two parts, Encoder and Decoder.âœ¨                    On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book                    ""Dive into Deep Learning"". Here, I have learned about Decoder Architecture, Self Attention, Encoder                    Decoder Attention, Position Wise Feed Forward Networks, Residual Connections, Transformer Decoder,                    Embedding Layer, Sequential Blocks, Training the Transformer Architecture and few more Topics                    related to the same from here. I have also read about Logistic Regression, Sigmoid Activation                    Function, Weights Initialization, Gradient Descent, Cost Function and more. âœ¨ I have                    presented the Implementation of Logistic Regression from Scratch using NumPy, Transformer Decoder                    and Training using PyTorch here in the Snapshots. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the Topics from the Book mentioned above and                    below. Excited about the days ahead !!ğŸ“šBooks:Dive into Deep                    Learning:https://d2l.ai/index.htmlJourney on ML and DL:https://lnkd.in/d-aDKvqLogistic                    Regression:https://lnkd.in/d2-VBbtLogistic Regression                    Docs:https://lnkd.in/dNq9qBq#66DaysOfData",True
/feed/update/urn:li:activity:6781556406211354624/,"ğŸ† Day 122 of #300DaysOfData!ğŸ† Transformer Architecture : Transformer is an                    architecture for transforming one sequence into another one with the help of two parts, Encoder and                    Decoder. It makes the use of Self Attention mechanisms. âœ¨ On my Journey of Machine Learning                    and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here,                    I have learned about Transformer, Self Attention, Encoder and Decoder Architecture, Sequence                    Embeddings, Positional Encoding, Position Wise Feed Forward Networks, Residual Connection and Layer                    Normalization, Encoder Block and Multi Head Self Attention, Transformer Decoder, Queries, Keys and                    Values, Scaled Dot Product Attention and few more Topics related to the same from here. âœ¨ I                    have presented the Implementation of Position Wise Feed Forward Networks, Residual Connection and                    Layer Normalization, Encoder, Decoder Block and Transformer Decoder using PyTorch here in the                    Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!ğŸ“š Books:Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6781187478662471684/,"ğŸ† Day 121 of #300DaysOfData!ğŸ† Multi Head Attention :Multi Head Attention is the design                    for attention mechanisms which runs through an attention mechanism several times in parallel.                    Instead of performing single attention pooling, queries, keys and values can be transformed into                    learned linear projections which are fed into attention pooling in parallel.âœ¨ On my Journey                    of Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into                    Deep Learning"". Here, I have learned about Multi Head Attention, Queries, Keys and Values, Attention                    Pooling, Scaled Dot Product Attention, Self Attention and Positional Encoding, Recurrent Neural                    Networks, Intra Attention, Comparing CNNs, RNNs and Self Attention, Padding Tokens, Absolute                    Positional Information, Relative Positional Information and few more Topics related to the same from                    here. âœ¨ I have presented the Implementation of Positional Encoding using PyTorch here in the                    Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!ğŸ“š Books:Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and DL : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6780841402256396289/,"ğŸ† Day 120 of #300DaysOfData!ğŸ† Multi Head Attention : Multi Head Attention is the                    design for attention mechanisms which runs through an attention mechanism several times in parallel.                    Instead of performing single attention pooling, queries, keys and values can be transformed into                    learned linear projections which are fed into attention pooling in parallel.âœ¨ On my Journey                    of Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into                    Deep Learning"". Here, I have learned about Bahdanau Attention, Recurrent Neural Networks Encoder                    Decoder Architecture, Training the Sequence to Sequence Model, Embedding Layer, Attention Weights,                    GRU, Heatmaps, Multi Head Attention, Queries, Keys and Values, Attention Pooling, Additive Attention                    and Scaled Dot Product Attention, Transpose Functions and few more Topics related to the same from                    here. âœ¨ I have presented the Implementation Multi Head Attention using PyTorch here in the                    Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!ğŸ“š Books: Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and DL: https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6780470091243905024/,"Day 119 of #300DaysOfData!Attention Pooling :Attention Pooling selectively aggregates                    values or sensory inputs to produce the output. It implies the interaction between queries or                    volitional cues and keys or non volitional cues. Attention Pooling is the weighted average of the                    training outputs. It can be parametric or nonparametric.On my Journey of Machine Learning                    and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here,                    I have learned about Scaled Dot Product Attention, Queries, Keys and Values, Additive Attention,                    Attention Pooling, Bahdanau Attention, RNN Encoder Decoder Architecture, Hidden States, Embedding,                    Defining Decoder with Attention, Sequence to Sequence Attention Decoder and few more Topics related                    to the same from here. I have presented the Implementation of Scaled Dot Product Attention                    and Sequence to Sequence Attention Decoder Model using PyTorch here in the Snapshots. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    Topics from the Book mentioned above and below. Excited about the days ahead !!Book:Dive                    into Deep Learning : https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6780109264267571200/,"Day 118 of #300DaysOfData!Attention Pooling :Attention Pooling selectively aggregates                    values or sensory inputs to produce the output. It implies the interaction between queries or                    volitional cues and keys or non volitional cues. Attention Pooling is the weighted average of the                    training outputs. It can be parametric or nonparametric. On my Journey of Machine Learning                    and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here,                    I have learned about Attention Scoring Functions, Gaussian Kernel, Attention Weights, Softmax                    Activation Function, Masked Softmax Operation, Text Sequences, Probability Distribution, Additive                    Attention, Queries, Keys and Values, Tanh Activation Function, Dropout and Linear Layer, Attention                    Pooling and few more Topics related to the same from here. I have presented the                    Implementation of Masked Softmax Operation and Additive Attention using PyTorch here in the                    Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book:Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6779749451717197825/,"Day 117 of #300DaysOfData!Attention Pooling : Attention Pooling selectively aggregates                    values or sensory inputs to produce the output. It implies the interaction between queries and keys.                    On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from                    the Book ""Dive into Deep Learning"". Here, I have learned about Attention Pooling or Nadaraya Watson                    Kernel Regression, Queries or Volitional Cues and Keys or Non Volitional Cues, Generating the                    Dataset, Average Pooling, Non Parametric Attention Pooling, Attention Weight, Gaussian Kernel,                    Parametric Attention Pooling, Batch Matrix Multiplication, Defining the Model, Training the Model,                    Stochastic Gradient Descent, MSE Loss Function and few more Topics related to the same from here.                    I have presented the Implementation of Attention Mechanisms, Non Parametric Attention                    Pooling, Batch Matrix Multiplication, NW Kernel Regression Model, Training and Prediction using                    PyTorch here in the Snapshots. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the Topics from the Book mentioned above and below. Excited about                    the days ahead !!Book:Dive into Deep Learning : https://d2l.ai/index.htmlJourney on                    ML and DL : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6779365916632203264/,"Day 116 of #300DaysOfData!Sequence Search : Greedy Search is the conditional probability                    of generating an output sequence based on the input sequence. Beam Search is an improved version of                    Greedy Search with a hyperparameter named beam size. On my Journey of Machine Learning and                    Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I                    have learned about Softmax Cross Entropy Loss Function, Sequence Masking, Teacher Forcing, Training                    and Prediction, Evaluation of Predicted Sequences, BLEU or Bilingual Evaluation Understudy, RNN                    Encoder Decoder, Beam Search, Greedy Search, Exhaustive Search, Attention Mechanisms, Attention                    Cues, Nonvolitional Cue and Volitional Cue, Queries, Keys and Values, Attention Pooling and few more                    Topics related to the same from here. I have presented the Implementation of Sequence                    Masking, Softmax Cross Entropy Loss, Training RNN Encoder Decoder Model and BLEU using PyTorch here                    in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead !!Book:Dive into Deep Learning : https://d2l.ai/index.htmlJourney on ML and                    Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6779012019728216064/,"Day 115 of #300DaysOfData!Encoder and Decoder Architecture :Encoder takes a variable                    length sequence as the input and transforms it into a state with a fixed shape. Decoder maps the                    encoded state of a fixed shape to a variable length sequence. On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Encoder and Decoder Architectures, Machine Translation Model,                    Sequence Transduction Models, Forward Propagation Function, Sequence to Sequence Learning, Recurrent                    Neural Networks, Embedding Layer, Gated Recurrent Units or GRU Layers, Hidden States and Units, RNN                    Encoder and Decoder Architecture, Vocabulary and few more Topics related to the same from here.                    I have presented the Implementation of Encoder, Decoder Architectures and RNN Encoder                    Decoder for Sequence to Sequence Learning using PyTorch here in the Snapshots. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the Topics from                    the Book mentioned above and below. Excited about the days ahead !!Book:Dive into Deep                    Learning : https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6778658340579885057/,"Day 114 of #300DaysOfData!Long Short Term Memory :Long Short Term Memory or LSTM is a                    type of Recurrent Neural Networks capable of learning order dependence in sequence prediction                    problems. LSTM has Input Gates, Forget Gates and Output Gates that control the flow of                    information.On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Deep Recurrent                    Neural Networks, Functional Dependencies, Bidirectional Recurrent Neural Networks, Dynamic                    Programming in Hidden Markov Models, Bidirectional Model, Computational Cost and Applications,                    Machine Translation and Dataset, Preprocessing the Dataset, Tokenization, Vocabulary, Padding Text                    Sequences and few more Topics related to the same from here. I have presented the                    Implementations of Downloading the Dataset, Preprocessing, Tokenization and Vocabulary using PyTorch                    here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead !! Book:Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6777934441890234368/,"Day 113 of #300DaysOfData!Long Short Term Memory : Long Short Term Memory or LSTM is a                    type of Recurrent Neural Networks capable of learning order dependence in sequence prediction                    problems. LSTM has Input Gates, Forget Gates and Output Gates that control the flow of information.                    On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from                    the Book ""Dive into Deep Learning"". Here, I have learned about Long Short Term Memory or LSTM, Gated                    Memory Cell, Input Gate, Forget Gate and Output Gate, Candidate Memory Cell, Tanh Activation                    Function, Sigmoid Activation Function, Memory Cell, Hidden State, Initializing Model Parameters,                    Defining the LSTM Model, Training and Prediction, Gated Recurrent Units or GRUs, Gaussian                    Distribution and few more Topics related to the same from here. I have presented the                    Implementation of Long Short Term Memory or LSTM Model, Training and Prediction using PyTorch here                    in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead !!Book:Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6777577123235291136/,"Day 112 of #300DaysOfData!Gated Recurrent Units : Gated Recurrent Units or GRUs are a                    gating mechanisms in Recurrent Neural Networks in which hidden state should be updated and also when                    it should be reset. It aims to solve the vanishing gradient problem which comes with standard RNNs.                    On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from                    the Book ""Dive into Deep Learning"". Here, I have learned about Modern Recurrent Neural Networks,                    Gradient Clipping, Gated Recurrent Units or GRUs, Memory cell, Gated Hidden State, Reset Gate and                    Update Gate, Broadcasting, Candidate Hidden State, Hadamard Product Operator, Hidden State,                    Initializing Model Parameters, Defining the GRU Model, Training and Prediction and few more Topics                    related to the same from here. I have presented the Implementation of Gated Recurrent Units,                    GRU Model, Training and Prediction using PyTorch here in the Snapshots. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the Topics from the                    Book mentioned above and below. Excited about the days ahead !!Book:Dive into Deep                    Learning : https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6777209728859705344/,"Day 111 of #300DaysOfData!Recurrent Neural Networks :Recurrent Neural Networks are the                    networks that uses recurrent computation for hidden states. The hidden state of an RNN can capture                    historical information of the sequence up to the current time step.On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Implementation of Recurrent Neural Networks, Defining the RNN                    Model, Training and Prediction, Backpropagation through Time, Exploding Gradients, Vanishing                    Gradients, Analysis of Gradients in RNNs, Full Computation, Truncating Time Steps, Randomized                    Truncation, Gradient Computing strategies in RNNs, Activation Functions, Regular Truncation and few                    more Topics related to the same from here. I have presented the Implementation of Recurrent                    Neural Networks, Training and Prediction using PyTorch here in the Snapshots. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the Topics from                    the Book mentioned above and below. Excited about the days ahead !!Book:Dive into Deep                    Learning : https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6776835752358633472/,"Day 110 of #300DaysOfData!Recurrent Neural Networks :Recurrent Neural Networks are the                    networks that uses recurrent computation for hidden states. The hidden state of an RNN can capture                    historical information of the sequence up to the current time step. On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Recurrent Neural Networks or RNN, Hidden State, Neural                    Networks without Hidden States, RNNs with Hidden States, RNN Layers, RNN based Character Level                    Language Models, Perplexity, Implementation of RNN from Scratch, One Hot Encoding, Vocabulary,                    Initializing the Model Parameters, RNN Model, Minibatch and Tanh Activation Function, Prediction and                    Warm up period, Gradient Clipping, Backpropagation and few more Topics related to the same from                    here. I have presented the Implementation RNN Model, Gradient Clipping and Training the                    Model using PyTorch here in the Snapshots. I hope you will gain some insights and work on the same.                    I hope you will also spend some time learning the Topics from the Book mentioned above and below.                    Excited about the days ahead !!Book:Dive into Deep Learning :                    https://d2l.ai/index.htmlJourney on ML and DL : https://lnkd.in/d-aDKvq#66DayOfData",True
/feed/update/urn:li:activity:6776460091576086528/,"Day 109 of #300DaysOfData!Sequential Partitioning :Sequential Partitioning is the                    strategy that preserves the order of split subsequences when iterating over minibatches. It ensures                    that the subsequences from two adjacent minibatches during iteration are adjacent in the original                    sequence. On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Language Models and                    Sequence Dataset, Conditional Probability, Laplace Smoothing, Markov Models and N Grams, Unigram,                    Bigram and Trigram Models, Natural Language Statistics, Stop words, Word Frequencies, Zipf's Law,                    Reading Long Sequence Data, Minibatches, Random Sampling, Sequential Partitioning and few more                    Topics related to the same from here. I have presented the Implementation of Unigram, Bigram                    and Trigram Model Frequencies, Random Sampling and Sequential Partitioning using PyTorch here in the                    Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book:Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6776111136732516352/,"Day 108 of #300DaysOfData!Tokenization and Vocabulary :Tokenization is the splitting of                    a string or text into a list of tokens. Vocabulary is the dictionary that maps string tokens into                    numerical indices. On my Journey of Machine Learning and Deep Learning, Today I have read                    and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Text                    Preprocessing, Corpus of Text, Tokenization Function, Sequence Models and Dataset, Vocabulary,                    Dictionary, Multilayer Perceptron, One step ahead prediction, Multi step ahead prediction, Tensors,                    Recurrent Neural Networks and few more Topics related to the same from here. I have                    presented the Implementation of Reading the Dataset, Tokenization and Vocabulary using PyTorch here                    in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend                    some time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book:Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DayOfData",True
/feed/update/urn:li:activity:6775746106086199297/,"Day 107 of #300DaysOfData!Sequence Models :The prediction beyond the known observations                    is called Extrapolation. The estimating between the existing observations is called Interpolation.                    Sequence Models require specialized statistical tools for estimation such as Auto Regressive Models.                    On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from                    the Book ""Dive into Deep Learning"". Here, I have learned about DenseNet Model, Convolutional Layers,                    Recurrent Neural Networks, Sequence Models, Interpolation and Extrapolation, Statistical Tools,                    Autoregressive Models, Latent Autoregressive Models, Markov Models, Reinforcement Learning                    Algorithms, Causality, Conditional Probability Distribution, Training the MLP, One step ahead                    prediction and few more Topics related to the same from here. I have presented the                    Implementation of DenseNet Architectures and Simple Implementation of RNNs using PyTorch here in the                    Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book:Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6775640050689421312/,"I really appreciate the support and appreciation of you all. I will continue sharing my Data Science                    and Machine Learning workflow. It wouldn't be possible without you guys. Thanks a lot, LinkedIn for                    being such an amazing platform. Let's grow and achieve success together. Have a great week ahead                    !!",False
/feed/update/urn:li:activity:6775396932413079553/,"Day 106 of #300DaysOfData!Batch Normalization :Batch Normalization continuously adjusts                    the intermediate output of the neural network by utilizing the mean and standard deviation of the                    minibatch so that the values of the intermediate output are more stable.On my Journey of                    Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Densely Connected Neural Networks or DenseNet, Dense Blocks,                    Batch Normalization, Activation Functions and Convolutional Layer, Transition Layer, Residual                    Networks or ResNet, Function Classes, Residual Blocks, Residual Mapping, Residual Connection, ResNet                    Model, Maximum and Average Pooling Layers, Training the Model and few more Topics related to the                    same from here. I have presented the Implementation of ResNet Architecture and ResNet Model                    using PyTorch here in the Snapshots. I hope you will gain some insights and work on the same. I hope                    you will also spend some time learning the Topics from the Book mentioned above and below. Excited                    about the days ahead !!Book:Dive into Deep Learning :                    https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6775030511761932288/,"Day 105 of #300DaysOfData!Batch Normalization :Batch Normalization continuously adjusts                    the intermediate output of the neural network by utilizing the mean and standard deviation of the                    minibatch so that the values of the intermediate output are more stable. On my Journey of                    Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Batch Normalization, Training Deep Neural Networks, Scale                    Parameter and Shift Parameter, Batch Normalization Layers, Fully Connected Layers, Convolutional                    Layers, Batch Normalization during Prediction, Tensors, Mean and Variance, Applying BN in LeNet,                    Concise Implementation of BN using high level API, Internal Covariate Shift, Dropout Layer, Residual                    Networks or ResNet, Function Classes, Residual Blocks and few more Topics related to the same from                    here. I have presented the Implementation of Batch Normalization Architecture using PyTorch                    here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead !!Book:Dive into Deep Learning : https://d2l.ai/index.htmlJourney on ML and DL                    : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6774665456155410432/,"Day 104 of #300DaysOfData!VGG Networks :VGG Networks construct a network using reusable                    convolutional blocks. VGG Models are defined by the number of convolutional layers and output                    channels in each block. On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Network In                    Network or NIN Architecture, NIN Blocks and Model, Convolutional Layer, RELU Activation Function,                    The Sequential and Functional API, Global Average Pooling Layer, Networks with Parallel                    Concatenations or GoogLeNet, Inception Blocks, GoogLeNet Model and Architecture, Maximum Pooling                    Layer, Training the Model and few more Topics related to the same from here. I have                    presented the Implementation of NIN Block and Model, Inception Block and GoogLeNet Model using                    PyTorch here in the Snapshots. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the Topics from the Book mentioned above and below. Excited about                    the days ahead !!Book:Dive into Deep Learning : https://d2l.ai/index.htmlJourney on                    Machine Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6774299172460867584/,"Day 103 of #300DaysOfData!VGG Networks :VGG Networks construct a network using reusable                    convolutional blocks. VGG Models are defined by the number of convolutional layers and output                    channels in each block. On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Convolutional Neural Networks, Supervised Learning, Deep CNN and AlexNet, Support Vector Machine and                    Features, Learning Representations, Data and Hardware Accelerator Problems, Architectures of LeNet                    and AlexNet, Activation Functions such as ReLU, Networks using CNN Blocks, VGG Neural Networks                    Architecture, Padding and Pooling, Convolutional Layers, Dropout, Dense and Linear Layers and few                    more Topics related to the same from here. I have presented the Implementation of AlexNet                    Architecture and VGG Networks Architecture along with CNN Blocks using PyTorch here in the                    Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book:Dive into Deep Learning:https://d2l.ai/index.htmlJourney on Machine Learning                    and Deep Learning:https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6773954392023420928/,"Day 102 of #300DaysOfData!Maximum Pooling :Pooling Operators consist of a fixed shape                    window that is slid over all the regions in the input according to its stride computing a single                    output for each location which is either maximum or the average value of the elements in the pooling                    window. On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Padding and Stride,                    Strided Convolutions, Cross Correlations, Multiple Input and Multiple Output Channels, Convolutional                    Layer, Maximum Pooling Layer and Average Pooling Layer, Pooling Window and Operators, Convolutional                    Neural Networks, LeNet Architecture, Supervised Learning, Convolutional Encoder, Sigmoid Activation                    Function and few more Topics related to the same from here. I have presented the                    Implementation of CNN, Implementation of Padding, Stride and Pooling Layers, Multiple Channels using                    PyTorch here in the Snapshots. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the Topics from the Book mentioned above and below. Excited about                    the days ahead !!Book:Dive into Deep Learning:https://d2l.ai/index.htmlJourney on ML                    and DL:https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6773583248611250176/,"Day 101 of #300DaysOfData!Invariance and Locality Principle :Translation Invariance                    principle states that out network should respond similarly to the same patch regardless of where it                    appears in the image. Locality Principle states that the network should focus on local regions                    without regard to the contents of the image in distant regions.On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Convolutional Neural Networks, Convolutions for Images, The                    Cross Correlation Operation, Convolutional Layers, Constructor and Forward Propagation Function,                    Weight and Bias, Object Edge Detection in Images, Learning a Kernel, Back Propagation, Feature Map                    and Receptive Field, Kernel Parameters and few more Topics related to the same from here. I                    have presented the Implementation of Cross Correlation Operation, Convolutional Layers and Learning                    a Kernel using PyTorch here in the Snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the Topics from the Book mentioned above and                    below. Excited about the days ahead !!Book:Dive into Deep                    Learning:https://d2l.ai/index.htmlJourney on ML and                    DL:https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6773219287982202880/,"Day 100 of #300DaysOfData!Invariance and Locality Principle :Translation Invariance                    principle states that out network should respond similarly to the same patch regardless of where it                    appears in the image. Locality Principle states that the network should focus on local regions                    without regard to the contents of the image in distant regions. On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Fully Connected Layers to Convolutions, Translation                    Invariance, Locality Principle, Constraining the MLP, Convolutional Neural Networks, Cross                    Correlation, Images and Channels, File IO, Loading and Saving Tensors, Loading and Saving Model                    Parameters, Custom Layers, Layers with Parameters and few more Topics related to the same from here.                    I have presented the Implementation of Layers with Parameters, Loading and Saving the                    Tensors and Model Parameters using PyTorch here in the Snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time learning the Topics from the Book                    mentioned above and below. Excited about the days ahead !!Book:Dive into Deep                    Learning:https://d2l.ai/index.htmlJourney on ML and DL:                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6772856758818885632/,"Day 99 of #300DaysOfData!Constant Parameters :Constant Parameters are the terms that are                    neither the result of the previous layers nor updatable parameters in the Neural Networks.On                    my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book                    ""Dive into Deep Learning"". Here, I have learned about Parameter Management, Parameter Access,                    Targeted Parameters, Collecting Parameters from Nested Block, Parameter Initialization, Custom                    Initialization, Tied Parameters, Deferred Initialization, Multi Layer Perceptrons, Input Dimensions,                    Defining Custom Layers, Layers without Parameters, Forward Propagation Function, Constant                    Parameters, Xavier Initializer, Weight and Bias and few more Topics related to the same from here.                    I have presented the Implementation of Parameter Access, Parameter Initialization, Tied                    Parameters and Layers without Parameters using PyTorch here in the Snapshots. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the Topics from                    the Book mentioned above and below. Excited about the days ahead !!Book :Dive into Deep                    Learning : https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6772504507793326080/,"Day 98 of #300DaysOfData!Constant Parameters : Constant Parameters are the terms that                    are neither the result of the previous layers nor updatable parameters in the Neural                    Networks.On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about K-Fold Cross                    Validation, Training and Predictions, Hyperparameters Optimization, Deep Learning Computation,                    Layers and Blocks, Softmax Regression, Multi Layer Perceptrons, ResNet Architecture, Forward and                    Backward Propagation Function, RELU Activation Function, The Sequential Block Implementation, MLP                    Implementation, Constant Parameters and few more Topics related to the same from here. I                    have presented the Implementation of MLP, The Sequential API Class and Forward Propagation Function                    using PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I hope                    you will also spend some time learning the Topics from the Book mentioned above and below. Excited                    about the days ahead !!The Notebook with complete documentation is here :                    https://lnkd.in/dFspBKgBook :Dive into Deep Learning :                    https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6772141946988183552/,"Day 97 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Training                    and Building Deep Networks, Downloading and Caching Datasets, Data Preprocessing, Regression                    Problems, Accessing and Reading the Dataset, Numerical and Discrete Categorical Features,                    Optimization and Variance, Arrays and Tensors, Simple Linear Model, The Sequential API, Root Mean                    Squared Error, Adam Optimizer, Hyperparameter Tuning, K-Fold Cross Validation, Training and                    Validation Error, Model Selection, Overfitting and Regularization and few more Topics related to the                    same from here. I have presented the Implementation of Simple Linear Model, Root Mean                    Squared Error, Training Function and K-Fold Cross Validation using PyTorch here in the Snapshots. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the Topics from the Book mentioned above and below. Excited about the days ahead !!The                    Notebook with complete documentation till now is here: https://lnkd.in/dbvJUCnBook :Dive                    into Deep Learning : https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6771748351823093762/,"Day 96 of #300DaysOfData!Co-adaption :Co-adaption is the condition in neural network                    which is characterized by a state in which each layer relies on the specific pattern of the                    activations in the previous layer.On my Journey of Machine Learning and Deep Learning, Today                    I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Forward Propagation, Backward Propagation and Computational Graphs, Numerical Stability, Vanishing                    and Exploding Gradients, Breaking the Symmetry, Parameter Initialization, Environment and                    Distribution Shift, Covariate Shift, Label Shift, Concept Shift, Non stationary Distributions,                    Empirical Risk and True Risk, Batch Learning, Online Learning, Reinforcement Learning and few more                    Topics related to the same from here. I have presented the Implementation of Data                    Preprocessing and Data Preparation using PyTorch here in the Snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the Topics from the                    Book mentioned above and below. Excited about the days ahead !!The Notebook with complete                    documentation is here: https://lnkd.in/dbvJUCnBook :Dive into Deep Learning :                    https://d2l.ai/index.htmlJourney on ML and DL : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6771612005737500672/,"I am always willing to share my story and here it comes. I am so thankful to Wilson Man, CPA, CMA                    for writing about my story. I would only imagine our short conversation would narrate such an                    amazing story. I really appreciate it, Wilson Man, CPA, CMA.",False
/feed/update/urn:li:activity:6771045401698021378/,"Day 95 of #300DaysOfData!Dropout and Co-adaption:Dropout is the process of injecting                    noise while computing each internal layer during forward propagation. Co-adaption is the condition                    in neural network which is characterized by a state in which each layer relies on the specific                    pattern of the activations in the previous layer.On my Journey of Machine Learning and Deep                    Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have                    learned about Dropout, Overfitting, Generalization Error, Bias and Variance Tradeoff, Robustness                    through Perturbations, L2 Regularization and Weight Decay, Co-adaption, Dropout Probability, Dropout                    Layer, Fashion MNIST Dataset, Activation Functions, Stochastic Gradient Descent, The Sequential and                    Functional API and few more Topics related to the same from here. I have presented the                    Implementation of Dropout Layer, Training and Testing the Model using PyTorch here in the Snapshots.                    I hope you will gain some insights and work on the same. I hope you will also spend some time                    learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book :Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6770695901854883840/,"Day 94 of #300DaysOfData!Multi Layer Perceptrons :The simplest deep neural networks are                    called Multi Layer Perceptrons. They consist of multiple layers of neurons each fully connected to                    those in layers below from which they receive input and above which in turn influence. On my                    Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive                    into Deep Learning"". Here, I have learned about High Dimensional Linear Regression, Model                    Parameters, Defining L2 Normalization Penalty, Defining the Training Loop, Regularization and Weight                    Decay, Dropout and Overfitting, Bias and Variance Tradeoff, Gaussian Distributions, Stochastic                    Gradient Descent, Training Error and Test Error and few more Topics related to the same from here.                    I have presented the Implementation of High Dimensional Linear Regression, Model Parameters,                    L2 Normalization Penalty, Regularization and Weight Decay using PyTorch here in the Snapshots. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the Topics from the Book mentioned above and below. Excited about the days ahead !!Book                    :Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine Learning and Deep                    Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6770326363690815488/,"Day 93 of #300DaysOfData!Multi Layer Perceptrons :The simplest deep neural networks are                    called Multi Layer Perceptrons. They consist of multiple layers of Neurons. On my Journey of                    Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Model Selection, Underfitting, Overfitting, Training Error and                    Generalization Error, Statistical Learning Theory, Model Complexity, Early Stopping, Training,                    Testing and Validation Dataset, K-Fold Cross Validation, Dataset Size, Polynomial Regression,                    Generating the Dataset, Training and Testing the Model, Third Order Polynomial Function Fitting,                    Linear Function Fitting, High Order Polynomial Function Fitting, Weight Decay, Normalization and few                    more Topics related to the same from here. I have presented the Implementation of Generating                    the Dataset, Defining the Training Function and Polynomial Function Fitting using PyTorch here in                    the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend                    some time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book :Dive into Deep Learning : https://d2l.ai/index.htmlJourney on ML and DL :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6769955329380773888/,"Day 92 of #300DaysOfData!Activation Functions :Activation Functions decide whether a                    neuron should be activated or not by calculating the weighted sum and further adding bias with it.                    They are differentiable operators.On my Journey of Machine Learning and Deep Learning, Today                    I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about                    Implementation of Multi Layer Perceptrons, Initializing Model Parameters, RELU Activation Functions,                    Cross Entropy Loss Function, Training the Model, Fully Connected Layers, Simple Linear Layer,                    Softmax Regression and Function, Stochastic Gradient Descent, Sequential API, High Level APIs,                    Learning Rate, Weights and Biases, Tensors, Hyperparameters and few more Topics related to the same                    from here. I have presented the Implementation of Multi Layer Perceptrons, RELU Activation                    Function, Training the Model and Model Evaluations using PyTorch here in the Snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    Topics from the Book mentioned above and below. Excited about the days ahead !! Book                    :Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine Learning and Deep                    Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6769575826510905344/,"Day 91 of #300DaysOfData!Activation Functions :Activation Functions decide whether a                    neuron should be activated or not by calculating the weighted sum and further adding bias with it.                    They are differentiable operators. On my Journey of Machine Learning and Deep Learning,                    Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned                    about Cross Entropy Loss Function, Classification Accuracy and Training, Softmax Regression, Model                    Parameters, Optimization Algorithms, Multi Layer Perceptrons, Hidden Layers, Linear Models Problems,                    From Linear to Nonlinear Models, Universal Approximators, Activation Functions like RELU Function,                    Sigmoid Function, Tanh Function, Derivatives and Gradients and few more Topics related to the same                    from here. I have presented the Implementation of Softmax Regression Model, Classification                    Accuracy, RELU Function, Sigmoid Function, Tanh Function along with Visualizations using PyTorch                    here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead !!Book :Dive into Deep Learning : https://d2l.ai/index.htmlJourney on ML and                    DL : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6769215523155652608/,"Day 90 of #300DaysOfData!Linear Regression:Linear Regression is a linear approach to                    modelling the relationship between a scalar response and one or more explanatory variables also                    known as dependent variables and independent variables.On my Journey of Machine Learning and                    Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I                    have learned about Softmax Regression, Classification Problem, Network Architecture,                    Parameterization Cost of Fully Connected Layers, Softmax Operation, Vectorization for Minibatches,                    Loss Function, Log Likelihood, Softmax and Derivatives, Cross Entropy Loss, Information Theory                    Basics, Entropy and Surprisal, Model Prediction and Evaluation, The Image Classification Dataset and                    few more Topics related to the same from here. I have presented the Implementation of Image                    Classification Dataset, Visualization, Softmax Regression and Operation along with Model Parameters                    using PyTorch here in the Snapshots. I hope you will gain some insights and work on the same. I hope                    you will also spend some time learning the Topics from the Book mentioned above and below. Excited                    about the days ahead !!Book :Dive into Deep Learning :                    https://d2l.ai/index.htmlJourney on ML and DL : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6768858815229136896/,"Day 89 of #300DaysOfData!Linear Regression:Linear Regression is a linear approach to                    modelling the relationship between a scalar response and one or more explanatory variables also                    known as dependent variables and independent variables.On my Journey of Machine Learning and                    Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I                    have learned about Linear Regression, Defining the Loss Function, Defining the Optimization                    Algorithm, Minibatch Stochastic Gradient Descent, Training the Model, Tensors and Differentiation,                    Concise Implementation of Linear Regression, Generating the Synthetic Dataset, Model Evaluation and                    few more Topics related to the same from here. I have presented the Implementation of                    Defining the Loss Function, Minibatch Stochastic Gradient Descent, Training and Evaluating the                    Model, Concise Implementation of Linear Regression and Reading the Dataset using PyTorch here in the                    Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book :Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6768505454164250624/,"Day 88 of #300DaysOfData!Hyperparameters :The parameters that are tunable but not                    updated in the training loop are called Hyperparameters. On my Journey of Machine Learning                    and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here,                    I have learned about Linear Regression Implementation From Scratch, Data Pipeline, Deep Learning                    Frameworks, Generating the Artificial Dataset, Scatter Plot and Correlation, Reading the Dataset,                    Minibatches, Features and Labels, Parallel Computing, Initializing the Model Parameters, Minibatch                    Stochastic Gradient Descent, Defining the Simple Linear Regression Model, Broadcasting Mechanism,                    Vectors and Scalars and few more Topics related to the same from here. I have presented the                    Implementation of Generating the Synthetic Dataset, Generating the Scatter Plot, Reading the                    Dataset, Initializing the Model Parameters and Defining the Linear Regression Model using PyTorch                    here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead !!Book :Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and DL : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6768145725130842112/,"Day 87 of #300DaysOfData!Hyperparameters :The parameters that are tunable but not                    updated in the training loop are called Hyperparameters. Hyperparameters Tuning is the process by                    which hyperparameters are chosen and typically requires adjusting based on the results of the                    Training Loop. On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Linear Regression,                    Basic Elements of Linear Regression, Linear Model and Transformation, Loss Function, Analytic                    Solution, Minibatch Stochastic Gradient Descent, Making Predictions with the Learned Model,                    Vectorization of Speed, The Normal Distribution and Squared Loss, Linear Regression to Deep Neural                    Networks, Biological Interpretation, Hyperparameters Tuning and few more Topics related to the same                    from here. I have presented the Implementation of Vectorization of Speed and Normal                    Distributions using Python here in the Snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the Topics from the Book mentioned above and                    below. Excited about the days ahead !!Book :Dive into Deep Learning :                    https://d2l.ai/index.htmlJourney on ML and DL : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6767815879007834112/,"Day 86 of #300DaysOfData!Method of Exhaustion :The ancient process of finding the area                    of curved shapes such as circle by inscribing the polygons in such shapes which better approximate                    the circle is called the Method of Exhaustion.On my Journey of Machine Learning and Deep                    Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have                    learned about Probabilities, Basic Probability Theory, Sampling, Multinomial Distribution, Axioms of                    Probability Theory, Random Variables, Dealing with Multiple Random Variables, Joint Probability,                    Conditional Probability, Bayes Theorem, Marginalization, Independence and Dependence, Expectation                    and Variance, Finding Classes and Functions in a Module and few more Topics related to the same from                    here. I have presented the Implementation of Multinomial Distribution, Visualization of                    Probabilities, Derivatives and Differentiation using PyTorch here in the Snapshot. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the Topics                    from the Book mentioned above and below. Excited about the days ahead !!Book :Dive into                    Deep Learning : https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6767431588088172545/,"Day 85 of #300DaysOfData!Method of Exhaustion : The ancient process of finding the area                    of curved shapes such as circle by inscribing the polygons in such shapes which better approximate                    the circle is called the Method of Exhaustion. On my Journey of Machine Learning and Deep                    Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have                    learned about Matrix Multiplication, L1, L2 and Frobenius Normalization, Calculus, Method of                    Exhaustion, Derivatives and Differentiation, Partial Derivatives, Gradient Descents, Chain Rule,                    Automatic Differentiation, Backward for Non Scalar Variables, Detaching Computation,                    Backpropagation, Computing the Gradient with Control Flow and few more Topics related to the same                    from here. I have presented the Implementation of Matrix Multiplication, L1, L2 and                    Frobenius Normalization, Derivatives and Differentiation, Automatic Differentiation and Computing                    the Gradient using PyTorch here in the Snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the Topics from the Book mentioned above and                    below. Excited about the days ahead !!Book :Dive into Deep Learning :                    https://d2l.ai/index.htmlJourney on ML and DL : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6767059334661726209/,"Day 84 of #300DaysOfData!Tensors :Tensors refer to algebraic objects describing the n                    dimensional arrays with an arbitrary number of axes. Vectors are first order Tensors and Matrices                    are second order Tensors.On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented from the Book ""Dive into Deep Learning"". Here, I have learned about Data                    Processing, Reading the Dataset, Handling the Missing Data, Categorical Data, Conversion to the                    Tensor Format, Linear Algebra such as Scalars, Vectors, Length, Dimensionality and Shape, Matrices,                    Symmetric Matrix, Tensors, Basic Properties of Tensor Arithmetic, Reduction, Non Reduction Sum, Dot                    Products, Matrix Vector Products and few more Topics related to the same from here. I have                    presented the Implementation of Data Processing, Handling the Missing Data, Scalars, Vectors,                    Matrices and Dot Products using PyTorch here in the Snapshot. I hope you will gain some insights and                    work on the same. I hope you will also spend some time learning the Topics from the Book mentioned                    above and below. Excited about the days ahead !! Book :Dive into Deep Learning :                    https://d2l.ai/index.htmlJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6766695937218768896/,"Day 83 of #300DaysOfData!Reinforcement Learning :Reinforcement Learning gives a very                    general statement of problem in which an agent interacts with the environment over a series of time                    steps and receives some observation and must choose action. On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented from the Book ""Dive into Deep                    Learning"". Here, I have learned about Search Algorithms, Recommender Systems, Sequence Learning,                    Tagging and Parsing, Machine Translation, Unsupervised Learning, Interacting with an Environment and                    Reinforcement Learning, Data Manipulation, Mathematical Operations, Broadcasting Mechanisms,                    Indexing and Slicing, Saving Memory in Tensors, Conversion to Other Datatypes and few more Topics                    related to the same from here. I have presented the Implementation of Mathematical                    Operations, Tensors Concatenation, Broadcasting Mechanisms and Datatypes Conversion using PyTorch                    here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead !!Book :Dive into Deep Learning : https://d2l.ai/index.htmlJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6765973746479067136/,"Day 82 of #300DaysOfData!Tagging Algorithms :The problem of learning to predict classes                    that are not mutually exclusive is called Multilabel Classification. Auto Tagging Problems are best                    described as Multilabel Classification Problems. On my Journey of Machine Learning and Deep                    Learning, Today I have read and Implemented from the Book ""Dive into Deep Learning"". Here, I have                    learned about A Motivating Example on Machine Learning, Learning Algorithms, Training Process, Data,                    Features, Models, Objective Functions, Optimization Algorithms, Supervised Learning, Regression,                    Binary, Multiclass and Hierarchical Classification, Cross Entropy and Mean Squared Error Loss                    Functions, Gradient Descent, Tagging Algorithms and few more Topics related to the same from here.                    I have presented the Implementation of Preparing the Data, Normalization, Removing Low                    Variance Features and Data Loaders using PyTorch here in the Snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the Topics from the                    Book mentioned above and below. Excited about the days ahead !! Book :Dive into Deep                    Learning : https://d2l.ai/index.htmlJourney on ML and DL :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6765617486344806400/,"Day 81 of #300DaysOfData!Voxel and Nodules :A Voxel is the 3D equivalent to the familiar                    2D pixel. It encloses a volume of space rather than an area. A mass of tissue made of proliferating                    cell in the lung is called a Tumor. A small Tumor just a few millimeters wide is called a Nodules.                    On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from                    the Book ""Deep Learning with PyTorch"". Here, I have learned about PyTorch Dataset Instance                    Implementation, LUNA Dataset Class, Cross Entropy Loss, Positive and Negative Nodules, Arrays and                    Tensors, Caching Candidate Arrays, Training and Validation Datasets, Data Visualization and few more                    Topics related to the same from here. Besides I have also learned about about Normalization of Data,                    Variance Threshold, RDKIT Library and few more Topics related to the same. I have presented                    the Implementation of Preparing the LUNA Dataset using PyTorch here in the Snapshots. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    Topics from the Book mentioned above and below. Excited about the days ahead !! Book                    :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on ML and DL :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6764920280084180993/,"Day 80 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned about Loading                    the Individual CT Scans Dataset, 3D Nodules Density Data, SimpleITK Library, Hounsfield Units,                    Voxels, Batch Normalization, Loading a Nodule using the Patient Coordinate System, Converting                    between Millimeters and Voxel Addresses, Array Coordinates, Matrix Multiplication and few more                    Topics related to the same from here. Besides I have also learned about Auto Encoders using LSTM,                    Stateful Decoder Model and Data Visualization.I have continued working with LUNA Dataset                    which stands for Lung Nodule Analysis 2016. I am also working on another Project recently. I                    have presented the Implementation of Conversion between Patient Coordinates and Arrays Coordinates                    on CT Scans Dataset using PyTorch here in the Snapshot. I hope you will gain some insights and work                    on the same. I hope you will also spend some time learning the Topics from the Book mentioned above                    and below. Excited about the days ahead !! Incase you want to see my Notebook here :                    https://lnkd.in/dgcFu9vBook :Deep Learning with PyTorch :                    https://lnkd.in/dYDJpGMJourney on ML and DL : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6764527413893337088/,"Day 79 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned about Data                    Loading and Parsing the Data, CT Scan Dataset, Data Pipeline and few more Topics related to the same                    from here. Besides, I have also learned about Auto Encoders, Recurrent Neural Networks and Long                    Short Term Memory or LSTM, Data Processing, One Hot Encoding, Random Splitting of Training and                    Validation Dataset and few more. I have continued working with LUNA Dataset which stands for                    Lung Nodule Analysis 2016. The LUNA Grand Challenge is the combination of an open dataset with high                    quality labels of patient CT scans: many with lung nodules and a public ranking of classifiers                    against the data. I have presented the simple Implementation of Data Preparation using                    PyTorch here in the Snapshots. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the Topics from the Book mentioned above and below. Excited about                    the days ahead !!Incase you want to see my Notebook here :                    https://lnkd.in/dgcFu9vBook :Deep Learning with PyTorch :                    https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6764156377255432192/,"Day 78 of #300DaysOfData!Voxel :A Voxel is the 3D equivalent to the familiar 2D pixel.                    It encloses a volume of space rather than an area. On my Journey of Machine Learning and                    Deep Learning, Today I have read and Implemented from the Book ""Deep Learning with PyTorch"". Here, I                    have learned about CT Scan Dataset, Voxel, Segmentation, Grouping and Classification, Nodules, 3D                    Convolutions, Neural Networks, Downloading the LUNA Dataset, Data Loading, Parsing the Data,                    Training and Validation Set and few more Topics related to the same from here. I have                    started working with LUNA Dataset which stands for Lung Nodule Analysis 2016. The LUNA Grand                    Challenge is the combination of an open dataset with high quality labels of patient CT scans: many                    with lung nodules and a public ranking of classifiers against the data. I have just started working                    on it. I have presented the Implementation of Preparing the Data using PyTorch here in the                    Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Incase you want to see my Notebook here : https://lnkd.in/dgcFu9vBook :Deep                    Learning with PyTorch : https://lnkd.in/dYDJpGM#66DaysOfData",True
/feed/update/urn:li:activity:6763455538639454208/,"Day 77 of #300DaysOfData!Identity Mapping :When the output of the first activations is                    used as the input of the last in addition to the standard feed forward path then it is called the                    Identity Mapping. Identity Mapping alleviate the issues of vanishing gradients. On my                    Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Deep                    Learning with PyTorch"". Here, I have learned about Convolutional Neural Networks, Skip Connections,                    ResNet Architecture, Simple Linear Layer, Max Pooling Layer, Identity Mapping, Highway Networks,                    UNet Model, Dense Networks and Very Deep Neural Networks, Sequential and Functional API, Forward and                    Backpropagation, Torch Vision Module and Sub Modules, Batch Normalization Layer, Custom                    Initializations and few more Topics related to the same from here. I have presented the                    Implementation of ResNet Architecture and Very Deep Neural Networks using PyTorch here in the                    Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6763083351176839170/,"Day 76 of #300DaysOfData!L2 Regularization :L2 Regularization is the sum of the squares                    of all the weights in the Model whereas L1 Regularization is the sum of the absolute values of all                    the weights in the Model. L2 Regularization is also referred to as Weight Decay.On my                    Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Deep                    Learning with PyTorch"". Here, I have learned about Dropout Module, Batch Normalization and Non                    Linear Activation Functions, Regularization and Principled Augmentation, Convolutional Neural                    Networks, Minibatch and Standard Deviation, Deep Neural Networks and Depth Module, Skip Connections                    Mechanism, ReLU Activation Function, Implementation of Functional API and few more Topics related to                    the same from here. I have presented the Implementation of Batch Normalization and Deep                    Neural Networks and Depth Module using PyTorch here in the Snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the Topics from the                    Book mentioned above and below. Excited about the days ahead !! Book :Deep Learning with                    PyTorch : https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData #machinelearning",True
/feed/update/urn:li:activity:6762748294046224385/,"Day 75 of #300DaysOfData!L2 Regularization :L2 Regularization is the sum of the squares                    of all the weights in the Model whereas L1 Regularization is the sum of the absolute values of all                    the weights in the Model. L2 Regularization is also referred to as Weight Decay. On my                    Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Deep                    Learning with PyTorch"". Here, I have learned about Convolutional Neural Network, L2 Regularization                    and L1 Regularization, Optimization and Generalization, Weight Decay, The PyTorch NN Module and Sub                    Modules, Stochastic Gradient Descent Optimizer, Overfitting and Dropout, Deep Neural Networks,                    Randomization and few more Topics related to the same from here. I have presented the                    Implementation of L2 Regularization and Dropout Layer using PyTorch here in the Snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    Topics from the Book mentioned above and below. Excited about the days ahead !!Book                    :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep                    Learning : https://lnkd.in/d-aDKvq#66DaysOfData #machinelearning",True
/feed/update/urn:li:activity:6761973876671971328/,"Day 74 of #300DaysOfData!Down Sampling :Down Sampling is the scaling of an Image by half                    which is equivalent of taking four neighboring pixels as input and producing one pixel as Output.                    Down Sampling principle can be implemented in different ways such as Max Pooling. On my                    Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Deep                    Learning with PyTorch"". Here, I have learned about Saving and Loading the Model, Weights and                    Parameters of the Model, Training the Model on GPU, The Torch NN Module and Sub Modules, Map                    Location Keyword, Designing Model, Long Short Term Memory or LSTM, Adding Memory Capacity or Width                    to the Network, Feed Forward Network, Overfitting and few more Topics related to the same from here.                    I have presented the Implementation of Adding Memory Capacity or Width to the Network using                    PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the Topics from the Book mentioned above and below. Excited about                    the days ahead !!Book :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney                    on Machine Learning and Deep Learning : https://lnkd.in/d-aDKvqWorking on the exams as well                    !! #66DaysOfData #ExamDays",True
/feed/update/urn:li:activity:6760893830872096768/,"Day 73 of #300DaysOfData!Down Sampling :Down Sampling is the scaling of an Image by half                    which is equivalent of taking four neighboring pixels as input and producing one pixel as Output.                    Down Sampling principle can be implemented in different ways.On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented from the Book ""Deep Learning with                    PyTorch"". Here, I have learned about The Torch NN Module, The Functional API, Convolutional Neural                    Network and The Training, The Data Loader Module, Forward and Backward Pass of the Network,                    Stochastic Gradient Descent Optimizer, Zeroing the Gradients, Cross Entropy Loss Function, Model                    Evaluation and Gradient Descent and few more Topics related to the same from here. I have                    presented the Implementation of Training Loop and Model Evaluation using PyTorch here in the                    Snapshot. Actually, It is the continuation of yesterday's Snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the Topics from the                    Book mentioned above and below. Excited about the days ahead !!Book :Deep Learning with                    PyTorch : https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData #ExamDays",True
/feed/update/urn:li:activity:6760532777252225025/,"Day 72 of #300DaysOfData!Down Sampling :Down Sampling is the scaling of an Image by half                    which is equivalent of taking four neighboring pixels as input and producing one pixel as Output.                    Down Sampling principle can be implemented in different ways.On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented from the Book ""Deep Learning with                    PyTorch"". Here, I have learned about Sub Classing the NN Module, The Sequential or The Modular API,                    Forward Function, Linear Model, Max Pooling Layer, Padding the Data, Convolutional Neural Network                    Architecture, ResNet, Kernel Size and Attributes, Tanh Activation Function, Model Parameters, The                    Functional API, Stateless Modules and few more Topics related to the same from here. I have                    presented the Implementation of Sub Classing the NN Module using The Sequential API and The                    Functional API using PyTorch here in the Snapshot. I hope you will gain some insights and work on                    the same. I hope you will also spend some time learning the Topics from the Book mentioned above and                    below. Excited about the days ahead !!Book :Deep Learning with PyTorch :                    https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData #ExamDays",True
/feed/update/urn:li:activity:6760187561857048576/,"Day 71 of #300DaysOfData!Down Sampling :Down Sampling is the scaling of an Image by half                    which is equivalent of taking four neighboring pixels as input and producing one pixel as Output.                    Down Sampling principle can be implemented in different ways. On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented from the Book ""Deep Learning with                    PyTorch"". Here, I have learned about Kernel Size, Padding the Image, Edge Detection Kernel, Locality                    and Translation Invariant, Learning Rate and Weight Update, Max Pooling Layer and Down Sampling,                    Stride, Convolutional Neural Networks, Receptive Field, Tanh Activation Function, Simple Linear                    Model, Sequential Model, Parameters of the Model and few more Topics related to the same from                    here.I have presented the Implementation of Convolutional Neural Network, Plotting the Image                    and Inspecting the Parameters of the Model using PyTorch here in the Snapshot. I hope you will gain                    some insights and work on the same. I hope you will also spend some time learning the Topics from                    the Book mentioned above and below. Excited about the days ahead !! Book :Deep Learning                    with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData #ExamDays",True
/feed/update/urn:li:activity:6759818203729297408/,"Day 70 of #300DaysOfData!Translational Invariance : Translational Invariance makes the                    Convolutional Neural Network invariant to translation which means that if we translate the Inputs                    then the CNN will still be able to detect the class to which the Input belongs. On my                    Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book ""Deep                    Learning with PyTorch"". Here, I have started reading the Topic Using Convolutions to Generalize. I                    have learned about Convolutional Neural Network, Translation Invariant, Weights and Biases, Discrete                    Cross Correlations, Locality or Local Operations on Neighborhood Data, Model Parameters, Multi                    Channel Image, Padding the Boundary, Kernel Size, Detecting Features with Convolutions and few more                    Topics related to the same. I have presented the simple Implementation of CNN and Building                    the Data using PyTorch here in the Snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the Topics from the Book mentioned above and                    below. Excited about the days ahead !!Book :Deep Learning with PyTorch :                    https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData #ExamDays",True
/feed/update/urn:li:activity:6759458165445677056/,"Day 69 of #300DaysOfData!Cross Entropy Loss :Cross Entropy Loss is a negative log                    likelihood of the predicted distribution under the target distribution as an outcome. The                    combination of Log Softmax Function and NLL Loss Function is equivalent to using Cross Entropy Loss.                    On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from                    the Book ""Deep Learning with PyTorch"". Here, I have learned about Gradient Descent, Minibatches and                    Data Loader, Stochastic Gradient Descent, Neural Network Model, Log Softmax Function, NLL Loss                    Function, Cross Entropy Loss Function, Trainable Parameters, Weights an Biases, Translation                    Invariant, Data Augment, Torch Vision and NN Modules and few more Topics related to the same from                    here. I have presented the Implementation of Building Deep Neural Network, Training Loop and                    Model Evaluation using PyTorch here in the Snapshot. I hope you will gain some insights and work on                    the same. I hope you will also spend some time learning the Topics from the Book mentioned above and                    below. Excited about the days ahead !!Book :Deep Learning with PyTorch :                    https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData #ExamDays",True
/feed/update/urn:li:activity:6759083688866148352/,"Day 68 of #300DaysOfData!Softmax Function :Softmax Function is a type of function that                    takes a vector of values and produces another vector of the same dimension where the values satisfy                    the constraints presented as Probabilities. Softmax is a monotone function that the lower values in                    the input will correspond to lower values in the output. On my Journey of Machine Learning                    and Deep Learning, Today I have read and Implemented from the Book ""Deep Learning with PyTorch"".                    Here, I have learned about Representing Output as Probabilities and Softmax Function, PyTorch's NN                    Module, Backpropagation, A Loss for Classification, MSE Loss, Negative Log Likelihood or NLL Loss,                    Log Softmax Function, Training the Classifier, Stochastic Gradient Descent, Hyperparameters,                    Minibatches and few more Topics related to the same from here. I have presented the                    Implementation of Softmax Function, Building Neural Network Model and Training Loop using PyTorch                    here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead !!Book :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on ML and                    DL : https://lnkd.in/d-aDKvq#66DaysOfData #ExamDays",True
/feed/update/urn:li:activity:6758724639909797888/,"Day 67 of #300DaysOfData!Computer Vision :Computer Vision is an Interdisciplinary                    scientific field that deals with how computers can gain high level understanding from digital images                    or videos.On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned about Permutation                    Function, Normalizing the Data, Stacking, Mean and Standard Deviation, Torch Vision Module and                    Submodules, CIFAR10 Dataset, PIL Package, Image Recognition, Building the Dataset, Building a fully                    connected Neural Networks Model, Sequential Model, Simple Linear Model, Classification and                    Regression Problems, One Hot Encoding and Softmax and few more Topics related to the same from here.                    I have presented the Implementation of Normalizing the Data, Building the Dataset and Neural                    Network Model using Torch Vision Modules here in the Snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time learning the Topics from the Book                    mentioned above and below. Excited about the days ahead !!Book :Deep Learning with                    PyTorch : https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData #ExamDays",True
/feed/update/urn:li:activity:6758362813594378240/,"Day 66 of #300DaysOfData!Computer Vision :Computer Vision is an Interdisciplinary                    scientific field that deals with how computers can gain high level understanding from digital images                    or videos. It seeks to understand and automate tasks that the human visual system can do. On                    my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from the Book                    ""Deep Learning with PyTorch"". Here, I have started the new Topic Learning From Images. I have                    learned about Simple Image Recognition, CIFAR10 which is a Dataset of Tiny Images, Torch Vision                    Module, The Dataset Class, Iterable Dataset, Python Imaging Library or PIL Package, Dataset                    Transforms, Arrays and Tensors, Permute Function and few more Topics related to the same. I                    have presented the simple Implementation of Torch Vision Module along with CIFAR10 Dataset using                    PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the Topics from the Book mentioned above and below. Excited about                    the days ahead !!Book :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney                    on Machine Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData #ExamDays",True
/feed/update/urn:li:activity:6757635751418064896/,"Day 65 of #300DaysOfData!Activation Functions :Activation Functions are Nonlinear which                    allows the overall network to approximate more complex functions. They are differentiable so that                    Gradients can be computed through them.On my Journey of Machine Learning and Deep Learning,                    Today I have read and Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned                    about The PyTorch NN Module, Simple Linear Model, Batching Input Data, Optimizing Batches, Mean                    Square Error Loss Function, Training Loop, Neural Networks, Sequential Model, Tanh Activation                    Function, Inspecting Parameters, Weights and Biases, OrderedDict Module, Comparing to the Linear                    Model, Overfitting and few more Topics related to the same form here. I have presented the                    simple Implementation of Sequential Model and OrderedDict Submodule using PyTorch here in the                    Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData #ExamDays #machinelearning                    #deeplearning #datascience",True
/feed/update/urn:li:activity:6757277873448472577/,"Day 64 of #300DaysOfData!Activation Functions :Activation Functions are Nonlinear which                    allows the overall network to approximate more complex functions. They are differentiable so that                    Gradients can be computed through them.On my Journey of Machine Learning and Deep Learning,                    Today I have read and Implemented from the Book ""Deep Learning with PyTorch"". Here, I am learning to                    use a Neural Network to fit the Data, Artificial Neurons, The Learning Process and Loss Function,                    Non Linear Activation Functions, Weights and Biases, Composing a Multilayer Network, Understanding                    the Error Function, Capping and Compressing the Output Range, Tanh and ReLU Activations, Choosing                    the Activation Functions, The PyTorch NN Module and few more Topics related to the same from here.                    I have presented the simple Implementation of Linear Model and Training Loop using PyTorch                    here in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also                    spend some time learning the Topics from the Book mentioned above and below. Excited about the days                    ahead !!Book :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData #ExamDays",True
/feed/update/urn:li:activity:6756563903615926272/,"Day 63 of #300DaysOfData!Stochastic Gradient Descent :Stochastic Gradient Descent or SGD                    comes from the fact that the Gradient is typically obtained by averaging over a random subset of all                    Input samples. On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned about Optimizers,                    Vanilla Gradient Descent Optimization, Stochastic Gradient Descent, Momentum Argument, Minibatch,                    Learning Rate and Params, Optim Module, Neural Network Models, Adam Optimizers, Backpropagation,                    Optimizing Weights, Training, Validation and Overfitting, Evaluating the Training Loss, Generalizing                    to the Validation Set, Overfitting and Penalization Terms and few more Topics related to the same                    from here. I have presented the Implementation of SGD and Adam Optimizer along with the                    Training Loop here in the Snapshots. It is the continuation of the previous Snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    Topics from the Book mentioned above and below. Excited about the days ahead !! Book                    :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep                    Learning : https://lnkd.in/d-aDKvq#66DaysOfData #ExamDays",True
/feed/update/urn:li:activity:6756227185289265152/,"Day 62 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned about Gradient                    Descent, PyTorch's Autograd and Backpropagation, Chain Rule and Tensors, Grad Attribute and                    Parameters, Simple Linear Function and Simple Loss Function, Accumulating Grad Functions, Zeroing                    the Gradients, Autograd Enabled Training Loop, Optimizers and Vanilla Gradient Descent and Optim                    Submodule of Torch and few more Topics related to the same from here. I have presented the                    simple Implementation of Linear Model and Loss Function, Autograd Enabled Training Loop using                    PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the Topics from the Book mentioned above and below. Excited about                    the days ahead !!Book :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney                    on Machine Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData #ExamDays                    #machinelearning #deeplearning",True
/feed/update/urn:li:activity:6755855892807208960/,"Day 61 of #300DaysOfData!Hyperparameter Tuning :Hyperparameter Tuning refers to the                    Training of Model's parameters and hyperparameters control how the Training goes. Hyperparameters                    are generally set manually. On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned about                    Gradient Descent, Optimizing the Training Loop, Overtraining, Convergence and Divergence, Learning                    Rate, Hyperparameter Tuning, Normalizing the Inputs, Visualization or Plotting the Data, Argument                    Unpacking, PyTorch's Autograd and Backpropagation, Chain Rule, Linear Model and few more Topics                    related to the same from here. I have presented the simple Implementation of Training Loop                    and Gradient Descent along with Visualization using PyTorch here in the Snapshot. I hope you will                    gain some insights and work on the same. I hope you will also spend some time learning the Topics                    from the Book mentioned above and below. Because of my exams, I will spend less Time in Machine                    Learning for a couple of weeks and I will update here as soon as possible. Excited about the days                    ahead !!Book :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine                    Learning and DL : https://lnkd.in/d-aDKvq#66DaysOfData #Exam",True
/feed/update/urn:li:activity:6755099540791037952/,"Day 60 of #300DaysOfData!Gradient Descent :Gradient Descent is the first order iterative                    Optimization Algorithm for finding a local minimum of a Differentiable Function. Simply, Gradient is                    the derivates of the Function with respect to each Parameter. On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented from the Book ""Deep Learning with                    PyTorch"". Here, I have learned about Cost Function or Loss Function, Optimizing Parameters using                    Gradient Descent, Decreasing Loss Function, Parameter Estimation, Mechanics of Learning, Scaling                    Factor and Learning Rate, Evaluations of Model, Computing the Derivative of Loss Function and Linear                    Function, Defining Gradient Function, Partial Derivative and Iterating the Model, The Training Loop                    and few more Topics related to the same from here. I have presented the Implementation of                    Loss Function, Computing Derivatives, Gradient Function and Training Loop here in the Snapshot. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the Topics from the Book mentioned above and below. Excited about the days ahead !!Book                    :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep                    Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6754765069252341760/,"Day 59 of #300DaysOfData!Loss Function :Loss Function is a function that computes a                    single numerical value that the learning process will attempt to minimize. The calculation of loss                    typically involves taking the difference between the desired outputs for some training samples.                    On my Journey of Machine Learning and Deep Learning, Today I have read and Implemented from                    the Book ""Deep Learning with PyTorch"". Here, I have learned about One Hot Encoding and Vectors, Data                    Representation using Tensors, Text Embeddings, Natural Language Processing, The Mechanics of                    Learning, Johannes Kepler's Lesson in Modeling, Eccentricity, Parameter Estimation, Weight, Bias and                    Gradients, Simple Linear Model, Loss Function or Cost Function, Mean Square Loss, Broadcasting and                    few more Topics related to the same from here. I have presented the simple Implementation of                    Representing Text, Mechanics of Learning and Simple Linear Model using PyTorch here in the Snapshot.                    I hope you will gain some insights and work on the same. I hope you will also spend some time                    learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6754046002853892096/,"Day 58 of #300DaysOfData!Encoding and ASCII :Every written characters is represented by                    a code which refers to a sequence of bits of appropriate length so that each character can be                    uniquely identified and it is called Encoding.On my Journey of Machine Learning and Deep                    Learning, Today I have read and Implemented from the Book ""Deep Learning with PyTorch"". Here, I have                    learned about Working with Time Series Data, Ordinal Variables, One Hot Encoding and Concatenation,                    Unsqueeze and Singleton Dimension, Mean, Standard Deviation and Rescaling Variables, Text                    Representation, Natural Language Processing and Recurrent Neural Networks, Converting the Text into                    Numbers, Project Gutenberg Corpus, One Hot Encoding of Characters, Encoding and ASCII, Embeddings                    and Processing the Text and few more Topics related to the same from here. I have presented                    the Implementation of Time Series Data and Text Representation using PyTorch here in the Snapshot. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the Topics from the Book mentioned above and below. Excited about the days ahead !! Book                    :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep                    Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6753682149120983041/,"Day 57 of #300DaysOfData!Continuous, Ordinal and Categorical Values :Continuous Values                    are the values which can be counted and measured along with units. Ordinal Values are the continuous                    values with no fixed relationships between values. Categorical Values are the enumerations of                    possibilities.On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned about Continuous and                    Categorical Data, PyTorch Tensor API, Finding Thresholds in Tabular Data, Advanced Indexing, Working                    with Time Series Data, Adding Time Dimension in Data, Shaping the Data by Time Period, Tensors and                    Arrays and few more Topics related to the same from here. I have presented the                    Implementation of Working with Categorical Data, Time Series Data and Finding Thresholds using                    PyTorch here in the Snapshot. I hope you will gain some insights and work on the same. I hope you                    will also spend some time learning the Topics from the Book mentioned above and below. Excited about                    the days ahead !!Book :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney                    on Machine Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData                    #machinelearning #deeplearning #datascience",True
/feed/update/urn:li:activity:6753295907116089344/,"Day 56 of #300DaysOfData!Continuous, Ordinal and Categorical Values :Continuous Values                    are the values which can be counted and measured along with units. Ordinal Values are the continuous                    values with no fixed relationships between values. Categorical Values are the enumerations of                    possibilities. On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned about Normalizing the                    Image Data, Working with 3D Images or Volumetric Image Data, Representing the Tabular Data, Loading                    the Data Tensors using Numpy, Continuous Values, Ordinal Values, Categorical Values, Ratio Scale and                    Interval Scale, Nominal Scale, One Hot Encoding and Embeddings, Singleton Dimensions and few more                    Topics related to the same from here. I have presented the Implementation of Normalizing the                    Image Data, Volumetric Data, Tabular Data and One Hot Encoding using PyTorch here in the Snapshots.                    I hope you will gain some insights and work on the same. I hope you will also spend some time                    learning the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine                    Learning and DL : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6752950480365752320/,"Day 55 of #300DaysOfData!Encoding Color Channels :The most common way to encode Colors                    into numbers is RGB where a color is defined by three numbers representing the Intensity of Red,                    Green and Blue.On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned about Tensors Metadata                    such as Size, Offset and Stride, Transposing Tensors without Copying, Transposing in Higher                    Dimensions, Contiguous Tensors, Managing Tensors Device Attribute such as moving to GPU and CPU,                    Numpy Interoperability, Generalized Tensors, Serializing Tensors, Data Representation using Tensors,                    Working with Images, Adding Color Channels, Changing the Layout and few more Topics related to the                    same from here. I have presented the Implementation of Working with Images such as Changing                    the Layout and Permute method along with Contiguous Tensors using PyTorch here in the Snapshot. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the Topics from the Book mentioned above and below. Excited about the days ahead !!Book                    :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep                    Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6752569863706841088/,"Day 54 of #300DaysOfData!Tensors and Multi Dimensional Arrays :Tensors are the                    Fundamental Data Structure in PyTorch. A Tensor is an array that is a Data Structure which stores a                    collection of numbers that are accessible individually using a index and that can be indexed with                    multiple indices.On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned about Named Tensors,                    Changing the names of Named Tensors, Broadcasting Tensors, Unnamed Dimensions, Tensor Element Types,                    Specifying the Numeric Data Type, The Tensor API, Creation Operations, Indexing, Random Sampling,                    Serialization, Parallelism, Tensors Storage, Referencing Storage, Indexing into Storage and few more                    Topics related to the same from here. I have presented the simple Implementation of Named                    Tensors, Tensor Datatype Attributes and Tensor API using PyTorch here in the Snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    Topics from the Book mentioned above and below. Excited about the days ahead !!Book                    :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep                    Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6752211857840320512/,"Day 53 of #300DaysOfData!Tensors and Multi Dimensional Arrays :Tensors are the                    Fundamental Data Structure in PyTorch. A Tensor is an array that is a Data Structure which stores a                    collection of numbers that are accessible individually using a index and that can be indexed with                    multiple indices. On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned about A Pretrained                    Neural Network that describes the scenes, NeuralTalk2 Model, Recurrent Neural Network, Torch Hub,                    Fundamental Building Block: Tensors, The world as Floating Point Numbers, Multidimensional Arrays                    and Tensors, Lists and Indexing Tensors, Named Tensors, Einsum, Broadcasting and few more Topics                    related to the same from here. I have presented the simple Implementation of Indexing                    Tensors and Named Tensors using PyTorch here in the Snapshot. I hope you will gain some insights and                    work on the same. I hope you will also spend some time learning the Topics from the Book mentioned                    above and below. Excited about the days ahead !!Book :Deep Learning with PyTorch :                    https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData #machinelearning #deeplearning",True
/feed/update/urn:li:activity:6751855099326304256/,"Day 52 of #300DaysOfData!The GAN Game :GAN stands for Generative Adversarial Network                    where Generative means something being created, Adversarial means the two Neural Networks are                    competing to out smart the other and well Network means Neural Networks. A Cycle GAN can turn Images                    of one Domain into Images of another Domain without the need for us to explicitly provide matching                    pairs in the Training set. On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented from the Book ""Deep Learning with PyTorch"". Here, I have learned about                    Pretrained Models, Generative Adversarial Network or GAN, ResNet Generator and Discriminator Models,                    Cycle GAN Architecture, Torch Vision Module, Deep Fakes, A Neural Network that turns Horses into                    Zebras and few more Topics related to the same from here. I have presented the                    Implementation of Cycle GAN that turns Horses into Zebras using PyTorch here in the Snapshots. I                    hope you will gain some insights and work on the same. I hope you will also spend some time learning                    the Topics from the Book mentioned above and below. Excited about the days ahead !!Book                    :Deep Learning with PyTorch : https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep                    Learning : https://lnkd.in/d-aDKvq#66DaysOfData #machinelearning",True
/feed/update/urn:li:activity:6751505217180712960/,"Day 51 of #300DaysOfData!Deep Learning :Deep Learning is the general class of Algorithms                    which falls under Artificial Intelligence and deals with training Mathematical entities named Deep                    Neural Networks by presenting the instructive examples. It uses large amounts of Data to approximate                    Complex Functions.On my Journey of Machine Learning and Deep Learning, Today I have started                    reading and Implementing from the Book ""Deep Learning with PyTorch"". Here, I have learned about Core                    PyTorch, Deep Learning Introduction and Revolution, Tensors and Arrays, Deep Learning Competitive                    Landscape, Utility Libraries, Pretrained Neural Network that recognizes the subject of an Image,                    ImageNet, Image Recognition, AlexNet and ResNet, Torch Vision Module and few more Topics related to                    the same from here.I have presented the Implementation of Obtaining Pretrained Neural                    Networks for Image Recognition using PyTorch here in the Snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time learning the Topics from the                    Book mentioned above and below. Excited about the days ahead !!Book :Deep Learning with                    PyTorch : https://lnkd.in/dYDJpGMJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6751141888847290368/,"Day 50 of #300DaysOfData!Categorical Data and Sparse Tensors :Categorical Data refers to                    Input Features that represent one or more Discrete items from a finite set of choices. Sparse                    Tensors are the tensors with very few non zero elements. On my Journey of Machine Learning                    and Deep Learning, Today I have learned from ""Machine Learning Crash Course"" of Google. Here, I have                    learned and Implemented about Neural Networks, Hidden Layers and Activation Functions, Nonlinear                    Classification and Feature Crosses, Sigmoid Function, Rectified Linear Unit or ReLU,                    Backpropagation, Vanishing and Exploding Gradients, Dropout Regularization, Multi Class Neural                    Networks, Softmax, Logistic Regression, Embeddings, Collaborative Filtering, Sparse Features,                    Principal Component Analysis, Word2Vec and few more Topics related to the same. I have                    presented the simple Implementation of Deep Neural Networks in Multi Class Classification here in                    the Snapshots. I hope you will gain some insights and work on the same. I hope you will also spend                    some time learning the Topics from the Course mentioned above and below. Excited about the days                    ahead !!Course:Machine Learning Crash Course : https://lnkd.in/dPmJbHVJourney on                    Machine Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6750781759438262272/,"Day 49 of #300DaysOfData!Prediction Bias :Prediction Bias is a quantity that measures                    how far apart is the average of predictions from the average of labels in Dataset. Prediction Bias                    is completely a different quantity than Bias. On my Journey of Machine Learning and Deep                    Learning, Today I have learned from ""Machine Learning Crash Course"" of Google. Here, I have learned                    and Implemented about Logistic Regression and Calculating Probability, Sigmoid Function, Binary                    Classification, Log Loss and Regularization, Early Stopping, L1 and L2 Regularization,                    Classification and Thresholding, Confusion Matrix, Class Imbalance and Accuracy, Precision and                    Recall, ROC Curve, Area Under Curve or AUC, Prediction Bias, Calibration Layer, Bucketing, Sparsity,                    Feature Cross and One Hot Encoding and few more Topics related to the same. I have presented                    the simple Implementation of Normalization and Binary Classification using Keras here in the                    Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Course mentioned above and below. Excited about the days ahead                    !!Course:Machine Learning Crash Course : https://lnkd.in/dPmJbHVJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6750399636969684992/,"Day 48 of #300DaysOfData!Scaling Features :Scaling means converting floating point                    Feature Values from their Natural range into Standard range such as 0 to 1. If the Feature set                    contains multiple Features, then Feature Scaling helps Gradient Descent to converge more quickly.                    On my Journey of Machine Learning and Deep Learning, Today I have learned from ""Machine                    Learning Crash Course"" of Google. Here, I have learned and Implemented about Scaling Feature Values,                    Handling Extreme Outliers, Binning, Scrubbing the Data, Standard Deviation, Feature Cross and                    Synthetic Feature, Encoding Nonlinearity, Stochastic Gradient Descent, Cross Product, Crossing One                    Hot Vectors, Regularization For Simplicity, Generalization Curve, L2 Regularization, Early Stopping,                    Lambda and Learning Rate and few more Topics related to the same. I have presented the                    simple Implementation of Linear Regression Model using Sequential API here in the Snapshots. I hope                    you will gain some insights and work on the same. I hope you will also spend some time learning the                    Topics from the Course mentioned above and below. Excited about the days ahead                    !!Course:Machine Learning Crash Course : https://lnkd.in/dPmJbHVJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6750060908380213248/,"Day 47 of #300DaysOfData!Feature Vector and Feature Engineering :Feature Engineering                    means transforming Raw Data into Feature Vector, which is the set of Floating values comprising the                    examples of the Dataset. On my Journey of Machine Learning and Deep Learning, Today I have                    learned from ""Machine Learning Crash Course"" of Google. Here, I have learned and Implemented about                    Generalization of Model, Overfitting, Gradient Descent and Loss, Statistical and Computational                    Learning Theories, Stationarity of Data, Splitting of Data and Validation Set, Representation and                    Feature Engineering, Feature Vector, Categorical Features and Vocabulary, One Hot Encoding and                    Sparse Representation, Qualities of Good Features and few more Topics related to the same.I                    have presented the simple Implementation of RNN along with GRU here in the Snapshots. I hope you                    will gain some insights and work on the same. I hope you will also spend some time learning the                    Topics from the Course and Book mentioned above and below. Excited about the days ahead                    !!Course:Machine Learning Crash Course : https://lnkd.in/dPmJbHVJourney on Machine                    Learning and Deep Learning : https://lnkd.in/d-aDKvq#66DaysOfData #deeplearning #datascience",True
/feed/update/urn:li:activity:6749710174082220032/,"Day 46 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    learned from ""Machine Learning Crash Course"" of Google. Here, I have learned and Implemented about                    Learning Rate or Step size, Hyperparameters in Machine Learning Algorithms, Regression, Gradient                    Descent, Optimizing Learning Rate, Stochastic Gradient Descent or SGD, Batch and Batch Size,                    Minibatch Stochastic Gradient Descent, Convergence, Hierarchy of TensorFlow Toolkits and few more                    Topics related to the same. I have also spend some time in reading the Book ""Speech and                    Language Processing"". Here, I have read about Regular Expressions and Patterns, Precision and                    Recall, Kleene Star, Aliases for Common Characters, RE Operators for Counting and few more Topics                    related to the same. I have presented the simple Implementation of Recurrent Neural Network                    and Deep RNN using Keras here in the Snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time learning the Topics from the Course and Book mentioned                    above and below. Excited about the days ahead !!Course and Book :Machine Learning Crash                    Course : https://lnkd.in/dPmJbHVSpeech and Language Processing :                    https://lnkd.in/d5n74GeJourney on ML and DL : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6748945748223643648/,"Day 45 of #300DaysOfData!Empirical Risk Minimization:Training a Model means learning                    good values for all the weights and the biases from Labeled examples. In Supervised Learning, a                    Machine Learning Algorithm builds a Model by examining many examples and attempting to find a Model                    that minimizes loss which is called Empirical Risk Minimization. On my Journey of Machine                    Learning and Deep Learning, Today I have started learning from the ""Machine Learning Crash Course""                    of Google. Here, I have learned about Machine Learning Philosophy, Fundamentals of Machine Learning                    and Uses, Labels and Features, Labeled and Unlabeled Example, Models and Inference, Regression and                    Classification, Linear Regression, Weights and Bias, Training and Loss, Empirical Risk Minimization,                    Mean Squared Error or MSE, Reducing Loss, Gradient Descent and few more Topics related to the same.                    I have presented the simple Implementation of Basic Recurrent Neural Network here in the                    Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend some                    time learning the Topics from the Course mentioned above and below. Excited about the days ahead                    !!Machine Learning Crash Course : https://lnkd.in/dPmJbHVJourney on Machine Learning and                    DL : https://lnkd.in/d-aDKvq#66DaysOfData",True
/feed/update/urn:li:activity:6748604654344515584/,"Day 44 of #300DaysOfData!Semantic Segmentation : In Semantic Segmentation, each pixel is                    classified according to the class of the object it belongs to but the different objects of the same                    class are not distinguished. On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented about Classification and Localization, Crowdsourcing in Computer Vision,                    Intersection Over Union metric, Object Detection, Fully Convolutional Networks or FCNs, VALID                    Padding, You Only Look Once or YOLO Architecture, Mean Average Precision or MAP, Convolutional                    Neural Networks, Semantic Segmentation and few more Topics related to the same from the Book ""Hands                    On Machine Learning with Scikit Learn, Keras and TensorFlow"". I have just completed learning from                    this Book. I have presented the Implementation of Classification and Localization along with                    the Visualization here in the Snapshots. I hope you will gain some insights and work on the same. I                    hope you will also spend some time reading the Topics from the Book mentioned above and below.                    Excited about the days ahead !!Book : Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlowJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #datascience #66DaysOfData",True
/feed/update/urn:li:activity:6748243009592160256/,"Day 43 of #300DaysOfData!Xception Model :Xception which stands for Extreme Inception is                    a variant of GoogLeNet Architecture which was proposed in 2016 by FranÃ§ois Chollet. It merges the                    ideas of GoogLeNet and ResNet Architecture but it replaces the Inception modules with a special type                    of layer called a Depthwise Separable Convolution. On my Journey of Machine Learning and                    Deep Learning, Today I have read and Implemented about Using Pretrained Models from Keras, GoogLeNet                    and Residual Network or ResNet, ImageNet, Pretrained Models for Transfer Learning, Xception Model,                    Convolutional Neural Network, Batching, Prefetching, Global Average Pooling and few more Topics                    related to the same from the Book ""Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlow"". I have presented the Implementation of Pretrained Models such as ResNet and                    Xception for Transfer Learning here in the Snapshots. I hope you will gain some insights and work on                    the same. I hope you will also spend some time reading the Topics from the Book mentioned above and                    below. Excited about the days ahead !!Book : Hands On Machine Learning with Scikit Learn,                    Keras and TensorFlowJourney on Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #datascience #DaysOfData",True
/feed/update/urn:li:activity:6747858389759471616/,"Day 42 of #300DaysOfData!ResNet :Residual Network or ResNet won the ILSVRC 2015                    Challenge, developed by Kaiming He, using an extremely deep CNN composed of 152 Layers. This Network                    uses the Skip connections which is also called Shortcut connections: The signal feeding into a layer                    is also added to the output of a layer located a bit higher up the stack.On my Journey of                    Machine Learning and Deep Learning, Today I have read and Implemented about LeNet-5 Architecture,                    AlexNet CNN Architecture, Data Augmentation, Local Response Normalization, GoogLeNet Architecture,                    Inception Module, VGGNet, Residual Network or ResNet, Residual Learning, Xception or Extreme                    Inception, Squeeze and Excitation Network or SENet and few more Topics related to the same from the                    Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".I have presented                    the Implementation of ResNet 34 CNN using Keras here in the Snapshot. I hope you will gain some                    insights and work on the same. I hope you will also spend some time reading the Topics from the Book                    mentioned above and below. Excited about the days ahead !!Book : Hands On Machine Learning                    with Scikit Learn, Keras and TensorFlowJourney of ML and DL :                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #DaysOfData #datascience",True
/feed/update/urn:li:activity:6747496391448268801/,"Day 41 of #300DaysOfData!Convolutional Layer :The most important building block of CNN                    is the Convolutional Layer. Neurons in the first Convolutional Layer are not connected to every                    single pixel in the Input Image but only to pixels in their respective fields. Similarly, each                    Neurons in second CL is connected only to neurons located within a small rectangle in the first                    layer. On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented about Deep Computer Vision using Convolutional Neural Networks, The Architecture of the                    Visual Cortex, Convolutional Layer, Zero Padding, Filters, Stacking Multiple Feature Maps, Padding,                    Memory Requirements, Pooling Layer, Invariance, Convolutional Neural Network Architectures and few                    more Topics related to the same from the Book ""Hands On Machine Learning with Scikit Learn, Keras                    and TensorFlow"".I have presented the simple Implementation of Convolutional Neural Network                    Architecture here in the Snapshot. I hope you will gain some insights and work on the same. I hope                    you will also spend some time reading the Topics from the Book mentioned above and below. Excited                    about the days ahead !!Book : Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlowJourney of ML and DL : https://lnkd.in/d-aDKvq#ML",True
/feed/update/urn:li:activity:6747132097732866048/,"Day 40 of #300DaysOfData!Embedding and Representation Learning :An Embedding is a                    trainable dense vector that represents a category. The better the representation of the categories,                    the easier it will be for the Neural Network to make accurate predictions, so Embeddings must make                    the useful representations of the categories. This is called Representation Learning. On my                    Journey of Machine Learning and Deep Learning, Today I have read and Implemented about The Features                    API, Column Transformer, Numerical and Categorical Features, Crossed Categorical Features, Encoding                    Categorical Features using One Hot Vectors and Embeddings, Representation Learning, Word Embeddings,                    Using Feature Columns for Parsing, Using Feature Columns in the Models and few more Topics related                    to the same from the Book ""Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlow"".I have presented the simple Implementation of The Features API in Numerical and                    Categorical Columns along with Parsing here in the Snapshots. I hope you will also spend some time                    reading the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book : Hands On Machine Learning with Scikit Learn, Keras and TensorFlowJourney of ML                    and DL : https://lnkd.in/d-aDKvq#machinelearning #deeplearning",True
/feed/update/urn:li:activity:6746402109123514368/,"Day 39 of #300DaysOfData!Prefetching and Data API :Prefetching is the loading of the                    resource before it is required to decrease the time waiting for that resource. In other words, while                    the Training Algorithm is working on one batch the dataset will already be working in parallel on                    getting the next batch ready which will improve the performance dramatically. On my Journey                    of Machine Learning and Deep Learning, Today I have read and Implemented about Loading and                    Preprocessing Data using TensorFlow, The Data API, Chaining Transformations, Shuffling the Dataset,                    Gradient Descent, Interleaving Lines From Multiple Files, Parallelism, Preprocessing the Dataset,                    Decoding, Prefetching, Multithreading and few more Topics related to the same from the Book ""Hands                    On Machine Learning with Scikit Learn, Keras and TensorFlow"".I have presented the simple                    Implementation of Data API using TensorFlow here in the Snapshot. I hope you will gain some insights                    and work on the same. I hope you will also spend some time reading the Topics from the Book                    mentioned above and below. Excited about the days ahead !!Book : Hands On Machine Learning                    with Scikit Learn, Keras and TensorFlowJourney of ML and DL :                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #DaysOfData #datascience",True
/feed/update/urn:li:activity:6746037075776172033/,"Day 38 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented about Custom Activation Functions, Initializers, Regularizers and Constraints,                    Custom Metrics, MAE and MSE, Streaming Metric, Custom Layers, Custom Models, Losses and Metrics                    based on Models Internals and few more Topics related to the same from the Book ""Hands On Machine                    Learning with Scikit Learn, Keras and TensorFlow"".I have also started reading a Book ""Speech                    and Language Processing"". Here, I have read about Regular Expressions, Basic Regular Expression                    Patterns, Disjunction, Range, Kleene Star, Wildcard Expression, Grouping and Precedence, Operator                    Hierarchy, Greedy and Non Greedy matching, Sequence and Anchors, Counters and few more Topics                    related to the same. I have presented the Implementation of Custom Activation Functions,                    Initializers, Regularizers, Constraints and Custom Metrics here in the Snapshots. I hope you will                    also spend some time reading the Topics from the Book mentioned above and below. Excited about the                    days ahead !!Books :Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlowSpeech and Language Processing : https://lnkd.in/d5n74GeJourney on ML and DL :                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #DS #DaysofData",True
/feed/update/urn:li:activity:6745708204421873664/,"Day 37 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented about Custom Models and Training with TensorFlow, High Level Deep Learning                    APIs, IO and Preprocessing, Lower Level Deep Learning APIs, Deployment and Optimization, TensorFlow                    Architecture, Tensors and Operations, Keras Low Level API, Tensors and Numpy, Sparse Tensors,                    Arrays, String Tensors, Custom Loss Functions, Saving and Loading the Models containing Custom                    Components and few more Topics related to the same from the Book ""Hands On Machine Learning with                    Scikit Learn, Keras and TensorFlow"".I have also started reading a Book ""Speech and Language                    Processing"". Here, I have read about Regular Expressions, Text Normalization, Tokenization,                    Lemmatization, Stemming, Sentence Segmentation, Edit Distance and few more Topics related to the                    same. I have presented the simple Implementation of Custom Loss Function here in the                    Snapshot. I hope you will also spend some time reading the Topics from the Books mentioned above and                    below. Excited about the days ahead !!Books : Hands On Machine Learning with Scikit                    Learn, Keras and TensorFlowSpeech and Language Processing : https://lnkd.in/d5n74GeJourney                    on ML and DL : https://lnkd.in/d-aDKvq#machinelearning #deeplearning #DS",True
/feed/update/urn:li:activity:6745320232082014209/,"Day 36 of #300DaysOfData!Deep Neural Networks :The best Deep Neural Networks                    configurations which will work fine in most cases without requiring much Hyperparameter Tuning is                    here: Kernel Initializer as LeCun Initialization, Activation Function as SELU, Normalization as                    None, Regularization as Early Stopping, Optimizer as Nadam, Learning Rate Schedule as Performance                    Scheduling. On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented about Avoiding Overfitting Through Regularization, L1 and L2 Regularization, Dropout                    Regularization, Self Normalization, Batch Normalization, Monte Carlo Dropout, Max Norm                    Regularization, Activation Functions like SELU and Leaky ReLU, Nadam Optimization and few more                    Topics related to the same from the Book ""Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlow"".I have presented the Implementation of L2 Regularization and Dropout                    Regularization using Keras here in the Snapshot. I hope you will gain some insights and work on the                    same. I hope you will also spend some time reading the Topics from the Book mentioned above and                    below. Excited about the days ahead !!Book : Hands On Machine Learning with Scikit Learn,                    Keras and TensorFlowJourney of ML and DL : https://lnkd.in/d-aDKvq#machinelearning #DL",True
/feed/update/urn:li:activity:6744947301585850368/,"Day 35 of #300DaysOfData!Adam Optimization :Adam which stands for Adaptive Moment                    Estimation combines the ideas of Momentum Optimization and RMSProp where Momentum Optimization keeps                    track of an exponentially decaying average of past gradients and RMSProp keeps track of an                    exponentially decaying average of past squared gradients. On my Journey of Machine Learning                    and Deep Learning, Today I have read and Implemented about AdaGrad Algorithm, Gradient Descent,                    RMSProp Algorithm, Adaptive Moment Estimation or Adam Optimization, Adamax, Nadam Optimization,                    Training Sparse Models, Dual Averaging, Learning Rate Scheduling, Power Scheduling, Exponential                    Scheduling, Piecewise Constant Scheduling, Performance Scheduling and few more Topics related to the                    same from the Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".I                    have presented the Implementation of Exponential Scheduling and Piecewise Constant Scheduling here                    in the Snapshot. I hope you will gain some insights and work on the same. I hope you will also spend                    some time reading the Topics from the Book mentioned above and below. Excited about the days ahead                    !!Book : Hands On Machine Learning with Scikit Learn, Keras and TensorFlowJourney of ML                    and DL : https://lnkd.in/d-aDKvq#machinelearning #DL #DS",True
/feed/update/urn:li:activity:6744586744097734656/,"Day 34 of #300DaysOfData!Gradient Clipping :Gradient Clipping is the Technique to lessen                    the exploding Gradients problem which simply clip the Gradients during backpropagation so that they                    never exceed some threshold and it is mostly used in Recurrent Neural Networks. On my                    Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Gradient                    Clipping, Batch Normalization, Reusing Pretrained Layers, Deep Neural Networks and Transfer                    Learning, Unsupervised Pretraining, Restricted Boltzmann Machines, Pretraining on an Auxiliary Task,                    Self Supervised Learning, Faster Optimizers, Gradient Descent Optimizer, Momentum Optimization,                    Nesterov Accelerated Gradient and few more Topics related to the same from the Book ""Hands On                    Machine Learning with Scikit Learn, Keras and TensorFlow"".I have presented the simple                    Implementation of Transfer Learning using Keras and Sequential API here in the Snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time reading the                    Topics from the Book mentioned above and below. Excited about the days ahead !!Book : Hands                    On Machine Learning with Scikit Learn, Keras and TensorFlowJourney of ML and DL :                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #datascience #DaysOfData",True
/feed/update/urn:li:activity:6744219845371424769/,"Day 33 of #300DaysOfData!Vanishing Gradient :During Backpropagation and calculating                    Gradients, it often gets smaller and smaller as the Algorithms progresses down to the lower layers                    which prevents the Training to converge to the good solution. This leads to Vanishing Gradient                    Problem. On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented about Training Deep Neural Networks, Vanishing and Exploding Gradient Problems, Glorot                    and He Initialization, Non Saturating Activation Functions, Batch Normalization and its                    Implementation, Logistic and Sigmoid Activation Function, SELU Activation Function, ReLU Activation                    Function and Variants, Leaky ReLU and Parametric Leaky ReLU and few more Topics related to the same                    from the Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".I have                    presented the Implementation of Leaky ReLU and Batch Normalization here in the Snapshot. I hope you                    will gain some insights and work on the same. I hope you will also spend some time reading the                    Topics from the Book mentioned above and below. Excited about the days ahead !!Incase you                    want to read my Journey on Machine Learning and Deep Learning here :                    https://lnkd.in/d-aDKvqBook :Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlow",True
/feed/update/urn:li:activity:6743895390870896640/,"Day 32 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented about Visualization using TensorBoard, Learning Curves, Fine Tuning Neural                    Network Hyperparameters, Randomized Search CV, Regressor, Libraries to optimize Hyperparameters such                    as Hyperopt, Talos and few more, Number of Hidden Layers, Number of Neurons per Hidden Layer,                    Learning Rate, Batch size and Other Hyperparameters and few more Topics related to the same from the                    Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".I have also spend                    some time reading the Paper which is named as ""Practical Recommendations for Gradient based Training                    of Deep Architectures"". Here, I have read about Deep Learning and Greedy Layer Wise Pretraining,                    Online Learning and Optimization of Generalization Error and few more related to the same.I                    have presented the Implementation of Tuning Hyperparameters, Keras Regressors and Randomized Search                    CV here in the Snapshots. I hope you will gain some insights and work on the same. I hope you will                    also spend some time reading the Topics from the Book mentioned above and below. Excited about the                    days ahead !!Book : Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlowPaper : https://lnkd.in/dJHsJj2",True
/feed/update/urn:li:activity:6743507669669175296/,"Day 31 of #300DaysOfData!Callbacks and Early Stopping :Early Stopping is a method that                    allows you to specify an arbitrarily large number of Training epochs and stopping once the Model                    stops improving on the validation dataset. On my Journey of Machine Learning and Deep                    Learning, Today I have read and Implemented about Building Dynamic Models using the Sub Classing                    API, Sequential API and Functional API, Saving and Restoring the Model, Using Callbacks, Model                    Checkpoints, Early Stopping, Weights and Biases and few more Topics related to the same from the                    Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".I have presented                    the Implementation of Building Dynamic Models using the Sub Classing API along with the                    Implementation of Using Callbacks and Early Stopping here in the Snapshots. I hope you will gain                    some insights and work on the same. I hope you will also spend some time reading the Topics from the                    Book mentioned above and below. Excited about the days ahead !!Incase you want to read my                    Journey on Machine Learning and Deep Learning here : https://lnkd.in/d-aDKvqBook :Hands                    On Machine Learning with Scikit Learn, Keras and TensorFlow#machinelearning #deeplearning                    #datascience #DaysOfData",True
/feed/update/urn:li:activity:6742421026451279872/,"Day 30 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented about Building the Complex Models using Functional API, Deep Neural Network                    Architecture, ReLU Activation Function, Handling Multiple Inputs in the Model, Mean Squared Error                    Loss Function and Stochastic Gradient Descent Optimizer, Handling Multiple Outputs or Auxiliary                    Output for Regularization and few more Topics related to the same from the Book ""Hands On Machine                    Learning with Scikit Learn, Keras and TensorFlow"".I have presented the Implementation of                    Handling Multiple Inputs using Keras Functional API along with the Implementation of Handling                    Multiple Outputs or Auxiliary Output for Regularization using the same here in the Snapshot. I hope                    you will gain some insights and work on the same. I hope you will also spend some time reading the                    Topics from the Book mentioned above and below. I am excited about the days ahead !!Incase                    you want to read my Journey on Machine Learning and Deep Learning here :                    https://lnkd.in/d-aDKvqBook :Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlow#machinelearning #deeplearning #datascience #DaysOfData",True
/feed/update/urn:li:activity:6742056919982211072/,"Day 29 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented about Creating the Model using Sequential API, Compiling the Model, Loss                    Function and Activation Function, Training and Evaluating the Model, Learning Curves, Using the                    Model to make Predictions, Building the Regression MLP using the Sequential API, Building Complex                    Models using the Functional API, Deep Neural Networks and few more Topics related to the same from                    the Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".I have                    presented the Implementation of Building Regression MLP using Sequential API and Functional API here                    in the Snapshots. I hope you will gain some insights and you will spend some time working on the                    same. I hope you will also spend some time reading and Implementing the Topics from the Book                    mentioned above. I am excited about the Days ahead !!Incase you want to read my Journey on                    Machine Learning and Deep Learning, I have been Documenting here :                    https://lnkd.in/d-aDKvqBook : Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlow#machinelearning #deeplearning #datascience #DaysOfData",True
/feed/update/urn:li:activity:6741693260340699136/,"Day 28 of #300DaysOfData!Rectified Linear Unit Function or ReLU :It is a continuous but                    not differentiable at 0 where the slope changes abruptly and makes the Gradient Descent bounce                    around. It works very well and has the advantage of fast to compute.On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented about Introduction to Artificial                    Neural Networks with Keras, Biological Neurons, Logical Computations with Neurons, The Perceptron,                    Hebbian Learning, Multi Layer Perceptron and Backpropagation, Gradient Descent, Hyperbolic Tangent                    Function and Rectified Linear Unit Function, Regression MLPs, Classification MLPs, Softmax                    Activation and few more Topics related to the same from the Book ""Hands On Machine Learning with                    Scikit Learn, Keras and TensorFlow"".I have presented the Implementation of Building an Image                    Classifier using the Sequential API along with Visualization using Keras here in the Snapshots. I                    hope you will spend some time working on the same and reading the Topics and Book mentioned above. I                    am excited about the days ahead !!Book : Hands On Machine Learning with Scikit Learn, Keras                    and TensorFlowJourney of Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #datascience #DaysOfData",True
/feed/update/urn:li:activity:6741338016515682304/,"Day 27 of #300DaysOfData!Anomaly Detection :Anomaly Detection also called Outlier                    Detection is the task of detecting instances that deviate strongly from the norm. These instances                    are called anomalies or outliers while the normal instances are called inliers. It is useful in                    Fraud Detection and more.On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented about Gaussian Mixture Models, Anomaly Detection using Gaussian Mixtures,                    Novelty Detection, Selecting the Number of Clusters, Bayesian Information Criterion, Akaike                    Information Criterion, Likelihood Function, Bayesian Gaussian Mixture Models, Fast MCD, Isolation                    Forest, Local Outlier Factor, One Class SVM and few more Topics related to the same from the Book                    ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"". I have just started Neural                    Networks and Deep Learning from this Book.I have presented the Implementation of Gaussian                    Mixture Model along with Visualizations using Python here in the Snapshots. I hope you will spend                    some time working on the same and reading the Topics and Book mentioned above. Excited about the                    days ahead !!Book: Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlowJourney of ML and DL: https://lnkd.in/d-aDKvq#machinelearning #DL #DS",True
/feed/update/urn:li:activity:6740965159897980928/,"Day 26 of #300DaysOfData!Gaussian Mixtures Model :A Gaussian Mixture Model or GMM is a                    probabilistic Model that assumes that the instances were generated from the mixture of several                    Gaussian distributions whose parameters are unknown. All the instances generated from a single                    Gaussian Distributions form a cluster that typically looks like an Ellipsoid. On my Journey                    of Machine Learning and Deep Learning, Today I have read and Implemented about using Clustering                    Algorithms for Semi Supervised Learning, Active Learning and Uncertainty Sampling, DBSCAN,                    Agglomerative Clustering, Birch Algorithms, Mean Shift and Affinity Propagation Algorithms, Spectral                    Clustering, Gaussian Mixtures Model, Expectation Maximization Algorithm and few more Topics related                    to the same from the Book ""Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlow"".I have presented the Implementation of Clustering Algorithms for Semi supervised                    Learning and DBSCAN along with Visualizations using Python here in the Snapshots. I hope you will                    spend some time working on the same and reading the Topics and Book mentioned above. Excited about                    the days ahead !!Book: Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlowJourney of ML and DL: https://lnkd.in/d-aDKvq#machinelearning #DL #DS",True
/feed/update/urn:li:activity:6740606438772613120/,"Day 25 of #300DaysOfData!Image Segmentation :Image Segmentation is the task of                    partitioning an Image into multiple segments. In Semantic Segmentation, all the pixels that are part                    of the same object type get assigned to the same segment. In Instance Segmentation, all pixels that                    are part of the individual object are assigned to the same segment. On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented about KMeans Algorithms, Centroid                    Initialization, Accelerated KMeans and Mini Batch KMeans, Finding the Optimal Numbers of Clusters,                    Elbow rule and Silhouette Coefficient score, Limitations of KMeans, Using Clustering for Image                    Segmentation and Preprocessing such as Dimensionality Reduction and few more Topics related to the                    same from the Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".I                    have presented the Implementation of Clustering Algorithms for Image Segmentation and Preprocessing                    along with Visualizations using Python here in the Snapshots. I hope you will spend some time                    working on the same and reading the Topics and Book mentioned above. Excited about the days ahead                    !!Book: Hands On Machine Learning with Scikit Learn, Keras and TensorFlowJourney of ML                    and DL: https://lnkd.in/d-aDKvq#machinelearning #DL #DS",True
/feed/update/urn:li:activity:6740239995958775808/,"Day 24 of #300DaysOfData!Clustering :Clustering Algorithms are the algorithms whose goal                    is to group similar instances together into Clusters. It is a great tool for Data Analysis, Customer                    Segmentation, Recommender Systems, Search Engines, Image Segmentation, Dimensionality Reduction and                    many more. On my Journey of Machine Learning and Deep Learning, Today I have read and                    Implemented about Kernel Principal Component Analysis, Selecting a Kernel and Tuning                    Hyperparameters, Pipeline and Grid Search, Locally Linear Embedding, Dimensionality Reduction                    Techniques such as Multi Dimensional Scaling, Isomap and Linear Discriminant Analysis, Unsupervised                    Learning such as Clustering and KMeans Clustering Algorithm and few more Topics related to the same                    from the Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".I have                    presented the Implementation of Kernel PCA and Grid Search CV, and KMeans Clustering Algorithm along                    with a Visualization using Python here in the Snapshots. I hope you will spend some time working on                    the same and reading the Topics and Book mentioned above. Excited about the days ahead                    !!Book: Hands On Machine Learning with Scikit Learn, Keras and TensorFlowJourney of ML                    and DL: https://lnkd.in/d-aDKvq#machinelearning #deeplearning",True
/feed/update/urn:li:activity:6739886262078988288/,"Day 23 of #300DaysOfData!Incremental PCA :Incremental PCA or IPCA Algorithms are the                    algorithms in which we can split the Training set into mini batches and feed an IPCA Algorithm one                    mini batch at a time. It is useful for large Training sets and also to apply PCA online. On                    my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Principal                    Component Analysis or PCA, Preserving the Variance, Principal Components, Projecting Down the                    Dimensions, Explained Variance Ratio, Choosing the Right Number of Dimensions, PCA for Compression                    and Decompression, Reconstruction Error, Randomized PCA, SVD, Incremental PCA and few more Topics                    related to the same from the Book ""Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlow"".I have presented the Implementation of PCA, Randomized PCA and Incremental PCA                    along with Visualizations using Scikit Learn here in the Snapshots. I hope you will spend some time                    working on the same. I hope you will also spend some time reading the Topics and Book mentioned                    above. Excited about the days ahead !!Incase you want to read how I had started in Machine                    Learning and Deep Learning: https://lnkd.in/d-aDKvqBook: Hands On Machine Learning with                    Scikit Learn, Keras and TensorFlow#machinelearning #DL #DS",True
/feed/update/urn:li:activity:6739519461235150848/,"Day 22 of #300DaysOfData!Manifold Learning :Manifold Learning refers to the                    Dimensionality Reduction Algorithms that work by modeling the manifold on which the training                    instances lie which relies on manifold hypothesis which holds that most real world high dimensional                    datasets to a much lower dimensional manifold. On my Journey of Machine Learning and Deep                    Learning, Today I have read and Implemented about Gradient Boosting, Early Stopping, Stochastic                    Gradient Boosting, Extreme Gradient Boosting or XGBoost, Stacking and Blending, Dimensionality                    Reduction, Curse of Dimensionality, Approaches for Dimensionality Reduction, Projection and Manifold                    Learning and few more Topics related to the same from the Book ""Hands On Machine Learning with                    Scikit Learn, Keras and TensorFlow"".I have presented the Implementation of Gradient Boosting                    with Early Stopping along with Visualization using Scikit Learn here in the Snapshots. I hope you                    will spend some time working on the same and reading the Topics and Book mentioned above. Excited                    about the days ahead !! Book : Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlowJourney of ML and DL: https://lnkd.in/d-aDKvq#machinelearning #deeplearning                    #datascience #DaysOfData",True
/feed/update/urn:li:activity:6739186375024754688/,"Day 21 of #300DaysOfData!Bagging and Pasting :It refers to the approach which uses the                    same Training Algorithm for every predictor but to train them on different random subsets of the                    Training set. When sampling is performed with replacement, it is called Bagging and when sampling is                    performed without replacement, it is called Pasting.On my Journey of Machine Learning and                    Deep Learning, Today I have read and Implemented about Ensemble Learning and Random Forests, Voting                    Classifiers, Bagging and Pasting in Scikit Learn, Out of Bag Evaluation, Random Patches and Random                    Subspaces, Random Forests, Extremely Randomized Trees Ensemble, Feature Importance, Boosting,                    AdaBoost, Gradient Boosting and few more Topics related to the same from the Book ""Hands On Machine                    Learning with Scikit Learn, Keras and TensorFlow"".I have presented the Implementation of                    Bagging Ensembles, Decision Trees, Random Forest Classifier, Feature Importance, AdaBoost Classifier                    and Gradient Boosting using Python here in the Snapshots. I hope you will spend some time working on                    the same and reading the Topics and Book mentioned above. Excited about the days ahead                    !!Book : Hands On Machine Learning with Scikit Learn, Keras and TensorFlowJourney of ML                    and DL: https://lnkd.in/d-aDKvq#machinelearning #DL",True
/feed/update/urn:li:activity:6738828793022033920/,"Day 20 of #300DaysOfData!The CART Training Algorithm :The Algorithm which represents                    Scikit Learn's Implementation of the Classification and Regression Tree or CART Training algorithm                    to train Decision Trees also called Growing Trees. It's working principle is splitting the Training                    set into two subsets using a feature and a threshold. On my Journey of Machine Learning and                    Deep Learning, Today I have read and Implemented about Decision Functions and Predictions, Decision                    Trees, Decision Tree Classifier, Making Predictions, Gini Impurity, White Box Models and Black Box                    Models, Estimating Class Probabilities, The CART Training Algorithm, Computational Complexities,                    Entropy, Regularization Hyperparameters, Decision Tree Regressor, Cost Function and Instability from                    the Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"". I have                    presented the simple Implementation of Decision Tree Classifier and Decision Tree Regressor along                    with Visualization of the same using Python here in the Snapshots. I hope you will spend some time                    working on the same and reading the Topics and Book mentioned above. Excited about the days ahead                    !!Book : Hands On Machine Learning with Scikit Learn, Keras and TensorFlowJourney of ML                    and DL: https://lnkd.in/d-aDKvq#ML #DL #DS",True
/feed/update/urn:li:activity:6738466048938389504/,"Day 19 of #300DaysOfData!Voting Classifiers :Voting Classifiers are the classifiers                    which aggregates the predictions of different Classifiers and predicts the class that gets the most                    votes. The majority vote classifier is called a Hard Voting Classifier. On my Journey of                    Machine Learning and Deep Learning, Today I have read and Implemented about Ensemble Learning and                    Random Forests, Voting Classifiers such as Hard Voting and Soft Voting Classifiers and few more                    topics related to the same. Actually, I have also started working on a Research Project with an                    amazing team which will take a couple of weeks or more. I think I won't be regular on my Machine                    Learning and Deep Learning Journey for a while so I am writing it as a short notice for those who                    has been active since the start of the Journey. I have presented the Implementation of Hard                    Voting and Soft Voting Classifiers using Scikit Learn here in the Snapshots. I hope you will spend                    some time working on the same and reading the Topics mentioned above. Excited about the days ahead                    !!Journey of Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #datascience",True
/feed/update/urn:li:activity:6738071226654195712/,"Day 18 of #300DaysOfData!Support Vector Machines :A Support Vector Machines or SVM is a                    very powerful and versatile Machine Learning model which is capable of performing Linear and                    Nonlinear Classification, Regression and even outlier detection. SVMs are particularly well suited                    for classification of complex but medium sized datasets. On my Journey of Machine Learning                    and Deep Learning, Today I have read and Implemented about Support Vector Machines, Linear SVM                    Classification, Soft Margin Classification, Nonlinear SVM Classification, Polynomial Regression,                    Polynomial Kernel, Adding Similarity Features, Gaussian RBF Kernel, Computational Complexity, SVM                    Regression which is Linear as well Nonlinear and few more Topics related to the same from the Book                    ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".I have presented the                    Implementation of Nonlinear SVM Classification using SVC and Linear SVC along with Visualization                    using Python here in the Snapshots. I hope you will spend some time working on the same and reading                    the Topics and Book mentioned above. Excited about the days ahead !!Book : Hands On Machine                    Learning with Scikit Learn, Keras and TensorFlowJourney of ML and DL:                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #datascience",True
/feed/update/urn:li:activity:6737702277294899200/,"Day 17 of #300DaysOfData!Elastic Net :Elastic Net is a middle grouped between Ridge                    Regression and Lasso Regression. The regularization term ""r"" is a simple mix of both Ridge and                    Lasso's regularization terms. When r equals 0, it is equivalent to Ridge and when r equals 1, it is                    equivalent to Lasso Regression.On my Journey of Machine Learning and Deep Learning, Today I                    have read and Implemented about Lasso Regression, Elastic Net, Early Stopping, SGD Regressor,                    Logistic Regression, Estimating Probabilities, Training and Cost Function, Sigmoid Function,                    Decision Boundaries, Softmax Regression or Multinomial Logistic Regression, Cross Entropy and few                    more Topics related to the same from the Book ""Hands On Machine Learning with Scikit Learn, Keras                    and TensorFlow"". I have just started reading the Topic Support Vector Machines. I have                    presented the simple Implementation of Lasso Regression, Elastic Net, Early Stopping, Logistic                    Regression and Softmax Regression using Scikit Learn here in the Snapshots. I hope you will spend                    some time working on the same and reading the Topics and Book mentioned above. Excited about the                    days ahead !!Book : Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlowJourney of ML and DL: https://lnkd.in/d-aDKvq#machinelearning #DL #DS",True
/feed/update/urn:li:activity:6737363256572178432/,"Day 16 of #300DaysOfData!Ridge Regression :Ridge Regression is a regularized Linear                    Regression viz. a regularization term is added to the cost function which forces the learning                    algorithm to not only fit the Data but also keep the model weights as small as possible.On                    my Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Training                    the Models, Linear Regression, The Normal Equations and Computational Complexity, Cost Function and                    Gradient Descent such as Batch Gradient Descent, Convergence Rate, Stochastic Gradient Descent, Mini                    batch Gradient Descent, Polynomial Regression and Poly Features, Learning Curves, Bias and Variance                    Tradeoff, Regularized Linear Models such as Ridge Regression and few more related to the same from                    the Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".I have                    presented the Implementation of Polynomial Regression, Learning Curves and Ridge Regression along                    with Visualization using Python here in the Snapshots. I hope you will spend some time working on                    the same and reading the Topics and Book mentioned above. Excited about the days ahead                    !!Book : Hands On Machine Learning with Scikit Learn, Keras and TensorFlowJourney of ML                    and DL: https://lnkd.in/d-aDKvq#machinelearning #DaysOfData #DL",True
/feed/update/urn:li:activity:6737019559938678784/,"Day 15 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented about The ROC Curve, Random Forest Classifier, SGD Classifier, Multi Class                    Classification, One vs One and One vs All Strategies, Cross Validation, Error Analysis using                    Confusion Matrix, Multi Class Classification, KNeighbors Classifier, Multi Output Classification,                    Noises, Precision and Recall Tradeoff and few more Topics related to the same from the Book ""Hands                    On Machine Learning with Scikit Learn, Keras and TensorFlow"". I have completed the Topic                    Classification from this Book.I have presented the Implementation of The ROC Curve, Random                    Forest Classifier in Multi Class Classification, The One vs One Strategy, Standard Scaler, Error                    Analysis, Multi Label Classification and Multi Output Classification using Scikit Learn here in the                    Snapshots. I hope you will also work on the same. I hope you will also spend some time reading the                    Topics and Book mentioned above. I am excited about the days ahead !!Book : Hands On Machine                    Learning with Scikit Learn, Keras and TensorFlowJourney of Machine Learning and Deep Learning:                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #DaysOfData",True
/feed/update/urn:li:activity:6736645562759712768/,"Day 14 of #300DaysOfData!Confusion Matrix :Confusion Matrix is a better way to evaluate                    the performance of a Classifier. The general idea of Confusion Matrix is to count the number of                    times instances of Class A are classified as Class B. This approach requires to have a set of                    predictions so that they can be compared to the actual targets. On my Journey of Machine                    Learning and Deep Learning, Today I have read and Implemented about Classification, Training a                    Binary Classifier using Stochastic Gradient Descent, Measuring Accuracy using Cross Validation,                    Implementation of CV, Confusion Matrix, Precision and Recall and their Curves and few more Topics                    related to the same from the Book ""Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlow"". I have presented the Implementation of SGD Classifier in MNIST Dataset along                    with Precision and Recall using Python here in the Snapshots. I have also presented the curves of                    Precision and Recall here. I hope you will spend some time working on the same and reading the                    Topics and Book mentioned above. I am excited about the days ahead !!Book : Hands On Machine                    Learning with Scikit Learn, Keras and TensorFlowJourney of Machine Learning and Deep Learning:                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #DaysOfData",True
/feed/update/urn:li:activity:6736263118252457984/,"Day 13 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    learned and Implemented about Random Forest Regressor, Ensemble Learning, Tuning the Model, Grid                    Search, Randomized Search, Analyzing the Best Models and Errors, Model Evaluation, Cross Validation                    and few more Topics related to the same from the Book ""Hands On Machine Learning with Scikit Learn,                    Keras and TensorFlow"".I have completed working with ""California Housing Prices"" Dataset                    which is included in this Book. This Dataset was based on Data from the 1990 California Census. I                    have built a Model using Random Forest Regressor of California Housing Prices Dataset to predict the                    price of the Houses in California.I have presented the Implementation of Random Forest                    Regressor and Tuning the Model with Grid Search and Randomized Search along with Cross Validation                    using Python here in the Snapshot. I hope you will spend some time working on the same and reading                    the Topics and Book mentioned above. Excited about the days ahead !!The Notebook with                    complete Documentation is here: https://lnkd.in/gukDqZvBook : Hands On Machine Learning with                    Scikit Learn, Keras and TensorFlowJourney of Machine Learning and Deep Learning:                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #DaysOfData",True
/feed/update/urn:li:activity:6735896275771764736/,"Day 12 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    learned and Implemented about Preparing the Data for Machine Learning Algorithms, Data Cleaning,                    Simple Imputer, Ordinal Encoder, OneHot Encoder, Feature Scaling, Transformation Pipeline, Standard                    Scaler, Column Transformer, Linear Regression, Decision Tree Regressor and Cross Validation from the                    Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".I have continued                    working with ""California Housing Prices"" Dataset which is included in this Book. This Dataset was                    based on Data from the 1990 California Census. I will build a Model of Housing Prices in California                    in this Project. The Notebook contains almost every Topics mentioned above. I have presented                    the Implementation of Data Preparation, Handling missing values, OneHot Encoder, Column Transformer,                    Linear Regression, Decision Tree Regressor along with Cross Validation using Python here in the                    Snapshots. I hope you will spend some time working on the same and reading the Topics and Book                    mentioned above. Excited about the days ahead !!The Notebook with complete Documentation                    till now is here: https://lnkd.in/gukDqZvBook: Hands On Machine Learning with Scikit Learn,                    Keras and TensorFlow#machinelearning #DL",True
/feed/update/urn:li:activity:6735530083441872896/,"Day 11 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    learned and Implemented about Creating categories from attributes, Stratified Sampling, Visualizing                    Data to gain insights, Scatter Plots, Correlations, Scatter Matrix and Attribute Combinations from                    the Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".I have                    continued working with ""California Housing Prices"" Dataset which is included in this Book. This                    Dataset was based on Data from the 1990 California Census. I will build a Model of Housing Prices in                    California in this Project. I am still working on the same.I have presented the                    Implementation of Stratified Sampling, Correlations using Scatter Matrix and Attribute combinations                    using Python here in the Snapshots. I have also presented the Snapshots of Correlations using                    Scatter plots here. I hope you will spend some time working on the same and reading the Topics and                    Book mentioned above. Excited about the days ahead !!Incase you want to see my Notebook                    although I am still working on it. The Notebook with complete Documentation till now is here:                    https://lnkd.in/gukDqZvBook: Hands On Machine Learning with Scikit Learn, Keras and                    TensorFlow Journey of ML and DL: https://lnkd.in/d-aDKvq#machinelearning #DL",True
/feed/update/urn:li:activity:6735205993069727744/,"Day 10 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read about the Main Challenges of Machine Learning such as Insufficient Quantity of Training Data,                    Non representative Training Data, Poor Quality Data, Irrelevant Features, Overfitting and                    Underfitting the Training Data and Testing and Validating, Hyperparameter Tuning and Model Selection                    and Data Mismatch from the Book ""Hands On Machine Learning with Scikit Learn, Keras and TensorFlow"".                    I have started working on ""California Housing Prices"" Dataset which is included in this                    Book. I will build a Model of Housing Prices in California in this Project. I have presented                    the simple Implementation of Data Processing and few techniques of EDA using Python here in the                    Snapshot. I have also presented the Implementation of Sweetviz Library for Analysis here. I really                    appreciate Chanin Nantasenamat for sharing about this Library in one of his videos. I hope you will                    also spend some time reading the Topics and Book mentioned above. Excited about the days                    ahead!!Book:Hands On Machine Learning with Scikit Learn, Keras and TensorFlowChanin                    Nantasenamat Video: https://lnkd.in/dhcm_ZpIncase you want to see my Notebook:                    https://lnkd.in/da_8qtW#machinelearning #deeplearning #datascience #DaysOfData",True
/feed/update/urn:li:activity:6734796720862973952/,"Day 9 of #300DaysOfData!Reinforcement Learning :In Reinforcement Learning, The Learning                    system called an agent in a particular context can observe the environment, select and perform                    actions and get rewards in return or penalties in the form of negative rewards. It must learn by                    itself what is the best policy to get the most reward over time. On my Journey of Machine                    Learning and Deep Learning, Today I have started reading and Implementing from the Book ""Hands On                    Machine Learning with Scikit Learn, Keras and TensorFlow"". I have read briefly about The Machine                    Learning Landscape viz. Types of Machine Learning Systems such as Supervised and Unsupervised                    Learning, Semisupervised Learning, Reinforcement Learning, Batch Learning and Online Learning,                    Instance Based Learning and Model Based Learning from this Book.I have presented the simple                    Implementation of Linear Regression and KNearest Neighbors along with a simple plot using Python                    here in the Snapshot. I hope you will also spend some time reading the Topics and Book mentioned                    above. Excited about the days ahead !!Book :""Hands On Machine Learning with Scikit                    Learn, Keras and TensorFlow""Journey of Machine Learning and Deep Learning :                    https://lnkd.in/d-aDKvq#machinelearning #deeplearning #DaysOfData",True
/feed/update/urn:li:activity:6734440932294307840/,"Day 8 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented about Neural Networks from the Book ""Machine Learning From Scratch"". I have                    read about Model Structure, Communication between Layers, Activation Functions such as ReLU,                    Sigmoid, The Linear Activation Function, Optimization, Back Propagation, Calculating Gradients,                    Chain Rule and Observations, Loss Functions along with Construction using The Loop Approach and The                    Matrix Approach and Implementation of the same from this Book. I have also read the Book ""A                    Comprehensive Guide to Machine Learning"" which focuses on Mathematics and Theory behind the Topics.                    I have read about Convolutional Neural Networks and Layers, Pooling Layers, Back Propagation for                    CNN, ResNet and Visual Understanding of CNNs from this Book. Besides, I have seen a couple of videos                    of Neural Networks and Deep Learning. I have presented the simple Implementation of Neural                    Networks with The Functional API and The Sequential API using TensorFlow here in the Snapshot. I                    hope you will also spend some time reading the Topics and Books mentioned above. Excited about the                    days ahead !!Books:Machine Learning from                    Scratch:https://lnkd.in/dEHs4XaComprehensive Guide to ML:Free!!#machinelearning                    #deeplearning",True
/feed/update/urn:li:activity:6734067611534946304/,"Day 7 of #300DaysOfData!Tree Ensemble Methods :Ensemble Methods combine the outputs of                    multiple simple Models which is often called Learners in order to create the fine Model with low                    variance. Due to their high variance, a decision trees often fail to reach a level of precision                    comparable to other predictive algorithms and Ensemble Methods minimize the variance. On my                    Journey of Machine Learning and Deep Learning, Today I have read and Implemented about Tree Ensemble                    Methods such as Bagging for Decision Trees, Bootstrapping, Random Forests and Procedure, Boosting,                    AdaBoost for Binary Classification, Weighted Classification Trees, The Discrete AdaBoost Algorithm                    and AdaBoost for Regression along with Construction and Implementation of the same from the Book                    ""Machine Learning From Scratch"".I have presented the Implementation of Bagging, Random                    Forests and AdaBoost along with different base estimators using Python here in the Snapshot. I hope                    you will also spend some time reading the Topics and Book mentioned above. Excited about the days                    ahead !!Books:Machine Learning from Scratch : https://lnkd.in/dEHs4XaJourney of                    Machine Learning and Deep Learning : https://lnkd.in/d-aDKvq#machinelearning #deeplearning                    #DaysOfData",True
/feed/update/urn:li:activity:6733714115484758016/,"Day 6 of #300DaysOfData!Decision Trees : A Decision Tree is an interpretable machine                    learning for Regression and Classification. It is a flow chart like structure in which each internal                    node represents a Test on an attribute and each branch represents the outcome of the Test.On                    my Journey of Machine Learning and Deep Learning, Today I have read about Decision Trees such as                    Regression Trees and Classification Trees, Building Trees, Making Splits and Predictions,                    Hyperparameters, Pruning and Regularization along with Construction and Implementation of the same                    from the Book ""Machine Learning From Scratch"".I have also read the Book ""A Comprehensive                    Guide to Machine Learning"" which focuses on Mathematics and Theory behind the Topics. I have read                    about Decision Tree Learning, Entropy and Information, Gini Impurity, Stopping Criteria, Random                    Forests, Boosting and AdaBoost, Gradient Boosting, KMeans Clustering from this Book. I have                    presented the Implementation of Regression Trees and Classification Trees using Python here in the                    Snapshot. I hope you will also spend some time reading the Topics and Books mentioned above. Excited                    about the days ahead !!Books:Machine Learning from                    Scratch:https://lnkd.in/dEHs4XaComprehensive Guide to ML:Free!!#machinelearning                    #deeplearning",True
/feed/update/urn:li:activity:6733401035705987072/,"Day 5 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented about Generative Classifiers such as Linear Discriminative Analysis or LDA,                    Quadratic Discriminative Analysis or QDA, Naive Bayes, Parameter Estimation and Data Likelihood                    along with Construction and Implementation of the same from the Book ""Machine Learning From                    Scratch"".I have also read the Book ""A Comprehensive Guide to Machine Learning"" which focuses                    on Mathematics and Theory behind the Topics. I have read about Generative and Discriminative                    Classification, Bayes Decision Rule, Least Squares Support Vector Machines, Feature Extension,                    Neural Network Extension, Binary and Multiclass Logistic Regression, Loss Function, Training,                    Multiclass Extension, Gaussian Discriminant Analysis, QDA and LDA Classification and Support Vector                    Machines from this Book. I have presented the Implementation of LDA, QDA and Naive Bayes                    along with Visualizations using Python here in the Snapshot. I hope you will also spend some time                    reading the Topics and Books mentioned above. Excited about the days ahead !!Books                    :Machine Learning from Scratch: https://lnkd.in/dEHs4XaComprehensive Guide to ML:                    Free!!Journey in ML and DL: https://lnkd.in/d-aDKvq#machinelearning #deeplearning",True
/feed/update/urn:li:activity:6733004909915992064/,"Day 4 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented about Discriminative Classifiers such as Binary and Multiclass Logistic                    Regression, The Perceptron Algorithm, Parameter Estimation, Fishers Linear Discriminant and Fisher                    Criterion along with Construction and Implementation of the same from the Book ""Machine Learning                    From Scratch"".I have also read the Book ""A Comprehensive Guide to Machine Learning"". I have                    read about Kernels and Ridge Regression, Linear Algebra Derivation, Computational Analysis, Sparse                    Least Squares, Orthogonal Matching Pursuit, Total Least Squares, Low rank Formulation,                    Dimensionality Reduction, Principal Component Analysis, Projection, Changing Coordinates, Minimizing                    Reconstruction Errors and Probabilistic PCA from this Book. I have presented the                    Implementation of Binary and Multiclass Logistic Regression, The Perceptron Algorithm and Fishers                    Linear Discriminant using Python here in the Snapshot. I hope you will also spend some time reading                    the Topics and Books mentioned above. Excited about the days ahead !!Books :Machine                    Learning from Scratch: https://lnkd.in/dEHs4XaComprehensive Guide to ML:Free!!Journey in ML                    and DL: https://lnkd.in/d-aDKvq#machinelearning #deeplearning #DaysOfData",True
/feed/update/urn:li:activity:6732627136630296576/,"Day 3 of #300DaysOfData!On my Journey of Machine Learning and Deep Learning, Today I have                    read and Implemented about Regularized Regression such as Ridge Regression and Lasso Regression,                    Bayesian Regression, GLMs, Poisson Regression along with Construction and Implementation of the same                    from the Book ""Machine Learning From Scratch"".I have also read the Book ""A Comprehensive                    Guide to Machine Learning"" which focuses on Mathematics and Theory behind the Topics. I have read                    about Maximum Likelihood Estimation or MLE and Maximum a Posteriori or MAE for Regression,                    Probabilistic Model, Bias Variance Tradeoff, Metrics, Bias Variance Decomposition, Alternative                    Decomposition, Multivariate Gaussians, Estimating Gaussians from Data, Weighted Least Squares, Ridge                    Regression, and Generalized Least Squares from this Book. I have presented the                    Implementation of Ridge Regression, Lasso Regression along with Cross Validation, Bayesian                    Regression and Poisson Regression using Python here in the Snapshot. I hope you will also spend some                    time reading the Topics and Books mentioned above. Excited about the days ahead !!Books                    :Machine Learning from Scratch: https://lnkd.in/dEHs4XaComprehensive Guide to ML:                    Free!!Journey in ML and DL: https://lnkd.in/d-aDKvq#machinelearning #deeplearning",True
/feed/update/urn:li:activity:6732269354034966528/,"Day 2 of #300DaysOfData!Ordinary Linear Regression :Linear Regression is a linear                    approach to modelling the relationships between a scalar response or dependent variable and one or                    more explanatory variables or independent variables. On my Journey of Machine Learning and                    Deep Learning, Today I have read and Implemented about Ordinary Linear Regression, Parameter                    Estimation, Minimizing Loss and Maximizing Likelihood along with the Construction and Implementation                    of the LR from the Book ""Machine Learning From Scratch"". I have also started reading the                    Book ""A Comprehensive Guide to Machine Learning"" which focuses on Mathematics and Theory behind the                    Topics. I have read about Regression, Ordinary Least Squares, Vector Calculus, Orthogonal                    Projection, Ridge Regression, Feature Engineering, Fitting Ellipses, Polynomial Features,                    Hyperparameters and Validation, Errors and Cross Validation from this book.I have presented                    the Implementation of Linear Regression along with Visualizations using Python here in the                    Snapshots. I hope you will also spend some time reading the Topics and Books mentioned above.                    Excited about the days ahead !! Books :Machine Learning from Scratch :                    https://lnkd.in/dEHs4XaComprehensive Guide to ML : Free!!#machinelearning #deeplearning                    #DaysOfData",True
/feed/update/urn:li:activity:6731904417722834945/,"Day 1 of #300DaysOfData!Gradient Descent and Cross Validation :Gradient Descent is an                    iterative approach to approximating the Parameters that minimize a Differentiable Loss Function.                    Cross Validation is a resampling procedure used to evaluate Machine Learning Models on a limited                    Data sample which has a parameter that splits the data into number of groups.On my Journey                    of Machine Learning and Deep Learning, Today I have read in brief about the fundamental Topics such                    as Calculus, Matrices, Matrix Calculus, Random Variables, Density Functions, Distributions,                    Independence, Maximum Likelihood Estimation and Conditional Probability. I have also read and                    Implemented about Gradient Descent and Cross Validation. I am starting this Journey from Scratch and                    I am following the Book, ""Machine Learning From Scratch"". I have presented the                    Implementation of Gradient Descent and Cross Validation here in the Snapshots. I hope you will also                    spend some time reading the Topics from the Book mentioned above. I am excited about the days to                    come !!Machine Learning from Scratch : https://lnkd.in/dEHs4Xa#machinelearning                    #deeplearning #300DaysOfData",True
/feed/update/urn:li:activity:6731554942433153024/,"Day 66 of #66DaysOfData! with Ken JeeToday I have read the Paper which is named as ""Finding                    Universal Grammatical Relations in Multilingual BERT"". Here, I have read about BERT, Cross Lingual                    Probing, Universal Grammatical Relations, Finer Grained Analysis, Determiners and various Topics                    related to the same. I have also spent some time going through the Documentation of my Journey in                    GitHub.Today, I have completed this Journey and I am so grateful and thankful to meet lots                    of amazing persons along the way. Actually, Ken Jee is one of the amazing persons who started this                    Journey. He has always been supportive and a great example to the Data Science Aspirants and I am so                    thankful to him.I have been Learning and Implementing Natural Language Processing for couple                    of months now. I will continue my Learning with new Journey and new Topics. I hope you have also                    learned something interesting in Natural Language Processing until this day. Personally, I am so                    thankful to you, the one who has come along this way and supported me since the start of the                    Journey. We will start to learn new Topics with new hope and energy. Thank you !!Incase you                    want to read my Journey :Journey of 66Days in Natural Language Processing :                    https://lnkd.in/dSSFhnt#datascience #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6731188629588193280/,"Day 65 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read the Papers about Zero Shot Learning and Syntax Sensitive Dependencies in LSTM. I have read                    the Paper which is named as ""Zero Shot Learning Through Cross Modal Transfer"". Here, I have read                    about Zero Shot and One Shot Learning, Word and Image Representations, Zero Shot Learning Models,                    Novelty Detection and various Topics related to the same.I have also read another Paper                    which is named as ""Assessing the Ability of LSTMs to Learn Syntax Sensitive Dependencies"". Here, I                    have read about LSTM Model and Baselines, Number Prediction, Relative Clauses, Language Modeling and                    various Topics related to the same.I have presented the Implementation of Greedy Decoding                    and Iterators using PyTorch here in the Snapshot. Actually, It is the continuation of previous                    Snapshots. I hope you will also spend some time reading about the Topics and Papers mentioned above.                    Excited about the days ahead!!Incase you want to read the Papers : Zero Shot Learning :                    https://lnkd.in/duz3pFvSyntax Sensitive LSTM : https://lnkd.in/dCDgdNGJourney of 66Days in                    NLP : https://lnkd.in/dSSFhnt#datascience #66DaysOfData #nlp #machinelearning",False
/feed/update/urn:li:activity:6730828193604870144/,"Day 64 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read the Papers about Long Short Term Memory and Stanford CoreNLP Toolkit. I have read the                    Paper which is named as ""Improved Semantic Representations From Tree Structured Long Short Term                    Memory Networks"". Here, I have read about Bidirectional LSTM and Multilayer LSTM, Tree Structured                    LSTM, Semantic Relatedness and Sentiment Classification and various Topics related to the same.                    I have read another Paper which is named as ""The Stanford CoreNLP Natural Language                    Processing Toolkit"". Here, I have read about Model Design and Development, Elementary Usage,                    Annotators, Tokenization and Lemmatization and various Topics related to the same. I have                    presented the Implementation of Data Generator, Loss Computation and Greedy Decoding in Transformer                    Model using PyTorch here in the Snapshot. Actually, It is the continuation of previous Snapshots. I                    hope you will also spend some time reading about the Topics and Papers mentioned above. Excited                    about the days ahead!!Incase you want to read the Papers:Structured LSTM :                    https://lnkd.in/dVY_RfKStanford CoreNLP : https://lnkd.in/d2g5ZzkJourney of 66Days in NLP :                    https://lnkd.in/dSSFhnt#nlp #66DaysOfData #datascience",False
/feed/update/urn:li:activity:6730452769964101632/,"Day 63 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read the Papers about Stanza: NLP Toolkit and Transfer Learning to study Linguistic Structure.                    I have read the Paper which is named as ""Stanza: A Python Natural Language Processing Toolkit for                    Many Human Languages"". Here, I have read about Neural Multilingual NLP Pipeline, Dependency Parsing                    and Lemmatization, NER and various Topics related to the same.I have also read the Paper                    which is named as ""Learning Music Helps you Read: Using Transfer to Study Linguistic Structure in                    Language Models"". Here, I have read about LSTM Architecture and Training, Random Baselines,                    Recursive Structure, Non Linguistic Structure and various Topics related to the same.I have                    presented the Implementation of Label Smoothing and Feed Forward using PyTorch here in the                    Snapshots. Actually, It is the continuation of previous Snapshots. I hope you will also spend some                    time reading about the Topics and Papers mentioned above. Excited about the days                    ahead!!Incase you want to read the Papers: Stanza: NLP Toolkit :                    https://lnkd.in/dJ6UkR6Transfer Learning and Linguistic Structure :                    https://lnkd.in/d33YHUg#66DaysOfData #nlp #deeplearning",False
/feed/update/urn:li:activity:6730094154380001280/,"Day 62 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read the Papers about Dynamic Memory Networks and Recursive Neural Networks. I have read the                    Paper which is named as ""Dynamic Memory Networks for Natural Language Processing"". Here, I have read                    about the Dynamic Memory Networks and Episodic Memory Module, Attention and Memory Update Mechanism                    and various Topics related to the same. I have also read the Paper which is named as                    ""Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"". Here, I have read                    about Semantic Vector Space and Compositionality in Vector Space, Recursive Neural Tensor Networks,                    Backpropagation and various Topics related to the same.  I have presented the Implementation                    of Optimizer along with Learning Rates using PyTorch here in the Snapshots. Actually, It is the                    continuation of previous Snapshots. I hope you will also spend some time reading about the Topics                    and Papers mentioned above. Excited about the days ahead!!Incase you want to read the                    Papers:Recursive Neural Networks : https://lnkd.in/dbkWW5HDynamic Memory Networks :                    https://lnkd.in/dws-BsUJourney of 66Days in Natural Language                    Processing:https://lnkd.in/dSSFhnt#nlp #66DaysOfData",False
/feed/update/urn:li:activity:6729736713771487232/,"Day 61 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read the Paper about Neural Machine Translation with Attention and Dynamic Memory Network. I                    have read the Paper which is named as ""Neural Machine Translation By Jointly Learning To Align and                    Translate"". Here, I have read the Topics related to Neural Machine Translation such as RNN Encoder                    and Decoder, Translation and to Align and Qualitative and Quantitative Analysis and more related to                    the same. I have also started reading another Paper which is named as ""Dynamic Memory Networks for                    Natural Language Processing"". I have just started reading this Paper and I will complete it soon.                    I have presented the Implementation of Training the Loop of Transformer Model along with the                    Implementation of Batching using PyTorch here in the Snapshots. Actually, It is the continuation of                    previous Snapshots. I hope you will also spend some time reading about the Topics and Papers                    mentioned above. I am excited about the days ahead!!Incase you want to read the Papers                    mentioned above :Neural Machine Translation with Attention : https://lnkd.in/dQmDSqUDynamic                    Memory Network for QA : https://lnkd.in/dws-BsUJourney of 66Days in Natural Language                    Processing:https://lnkd.in/dSSFhnt#66DaysOfData #nlp",False
/feed/update/urn:li:activity:6729390405407793153/,"Day 60 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read Papers about GloVe and Recurrent Neural Network Language Model. I have read a Paper which                    is named as ""GloVe : Global Vectors for Word Representation"". Here, I have read about Matrix                    Factorization Methods, The GloVe Model and Complexity of the Model, Evaluation Methods, Word                    Analogies, Model Analysis and various other related topics.Similarly, I have read another                    Paper which is named as ""A Simple Way to Initialize Recurrent Networks of Rectified Linear Units"".                    Here, I have read about Language Modeling and Initialization, Speech Recognition and various other                    related topics. I have presented the Implementation of Transformer Model, Batches and                    Masking using PyTorch here in the Snapshot. Actually, It is the continuation of previous Snapshots.                    I hope you will also spend some time reading about the Topics and Papers mentioned above. I am                    excited about the days ahead!!Incase you want to read :GloVe :                    https://lnkd.in/d2JH8nPRNN Language Model : https://lnkd.in/dtNnkYsJourney of 66Days in                    Natural Language Processing: https://lnkd.in/dSSFhnt#datascience #machinelearning                    #deeplearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6729017777660665857/,"Day 59 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read Papers about Skip Gram and NLP. I have read a Paper which is named as ""Efficient                    Estimation of Word Representations in Vector Space"". Here, I have read about Model Architectures,                    Feed Forward Neural Net Language Model, Recurrent Neural Net Language Model, Log Linear Models, CBOW                    Models, Continuous Skip Gram Model and various other related topics. I have also read the                    Paper which is named as ""Distributed Representations of Words and Phrases and their                    Compositionality"". Here, I have read about Skip Gram Model, Hierarchical Softmax, Negative Sampling,                    Subsampling and Additive Compositionality. I have presented the Implementation of                    Positionwise Feed Forward Class, Embedding, Positional Encoding and Feed Forward using PyTorch here                    in the Snapshot. Actually, It is the continuation of yesterday's Snapshot. I hope you will also                    spend some time reading the Topics and Papers mentioned above about Skip Gram Model. Excited about                    the days ahead!!Incase you want to read :Skip Gram and Softmax :                    https://lnkd.in/dTB7peDSkip Gram and Sampling : https://lnkd.in/d582nHM#datascience                    #machinelearning #deeplearning #66DaysOfData",False
/feed/update/urn:li:activity:6728656620122865664/,"Day 58 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read some Articles about Deep Learning and Natural Language Processing. I have read an Article                    named ""NLP's ImageNet Moment"" which is written by Sebastian Ruder. Here, I have read about the                    recent advances in NLP such as ULMFiT, ELMO, OpenAI Transformer and the ImageNet for                    Language.Similarly, I have read another Article named as ""The Illustrated BERT, ELMO and                    Co."" which is written by Jay Alammar. Here, I have read about BERT Model Architecture, Word                    Embeddings, Transfer Learning and the topics mentioned above. I have also read another                    Article named ""Generalized Language Models"" which is written by Lilian Weng. Here, I am reading                    about Language Models and I am still reading this Article. I have presented the                    Implementation of Attention and Multi Head Attention Class using PyTorch here in the Snapshot.                    Actually, It is the continuation of yesterday's Snapshot. I hope you will also spend some time                    reading the Articles mentioned above. Excited about the days ahead!!Incase you want to read:                    NLP's ImageNet Moment: https://lnkd.in/dcziSADThe Illustrated BERT:                    https://lnkd.in/dj5N_xWGeneralized LM: https://lnkd.in/dE2xbyJ#nlp #machinelearning                    #deeplearning #66DaysOfData",False
/feed/update/urn:li:activity:6728309704285536256/,"Day 57 of #66DaysOfData!In my journey of Natural Language Processing, Today I have read a                    Paper about Deep Learning and Text Classification which is named as ""Deep Learning Based Text                    Classification: A Comprehensive Review"". I have completed reading this Paper. I have read about                    Unsupervised Learning using Autoencoders, Adversarial Training, Reinforcement Learning and few                    Popular Datasets such as YELP and IMDB. I have also read about Popular Metrics for Text                    Classification such as Accuracy and Error Rate, Precision, Recall and F1 Score from this Paper.                    I have also read one of the most popular Articles named ""The Illustrated Transformer"" by Jay                    Alammar. It teaches about the Transformer from Scratch. I have read the topics such as Encoder,                    Decoder, Self Attention, Feed Forward and Multi Head Attention in this Article.I have                    presented the Implementation of Decoder Class, DecoderLayer with FeedForward and Subsequent Mask                    using PyTorch here in the Snapshots. I hope you will spend some time reading the Paper and Article                    mentioned above. Excited about the days ahead!!Incase you want to read :Paper:                    https://lnkd.in/dNrDXXcArticle: https://lnkd.in/dWM-ipsJourney of 66Days in Natural Language                    Processing: https://lnkd.in/dSSFhnt#deeplearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6727922339650011136/,"Day 56 of #66DaysOfData!In my journey of Natural Language Processing, Today I have read a                    Paper about Deep Learning and Text Classification which is named as ""Deep Learning Based Text                    Classification: A Comprehensive Review"". I have read about various Text Classification Tasks such as                    Sentiment Analysis, News Categorization, Topic Analysis and Question Answering. I have also read                    about various Deep Learning Models for Text Classification such as Feed Forward Neural Networks, RNN                    Based Models, CNN Based Models, Capsule Neural Networks, Models with Attention, Transformers, Graph                    Neural Networks, Siamese Neural Networks and Hybrid Models from this Paper.I have presented                    the simple Implementation of Layer Normalization, Sublayer Connection, Encoder Layer and Feed                    Forward Layer using PyTorch here in the Snapshots. I hope you will also spend time reading about                    Deep Learning and Text Classification from the Paper mentioned above. I am excited about the days                    ahead !!Incase you want to read the Paper mentioned above:                    https://lnkd.in/dNrDXXc#deeplearning #machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6727603695384592387/,"Day 55 of #66DaysOfData!Transfer Learning :Transfer Learning is a process where a Model                    is first pretrained on a Data rich task before being fine tuned on a Downstream Task and Transfer                    Learning has emerged as a powerful technique in Natural Language Processing or NLP. The                    effectiveness of Transfer Learning has given rise to a Diversity of approaches, methodology, and                    practice.In my journey of Natural Language Processing, Today I have started reading the                    Papers of Transformers and Natural Language Processing which is named as ""Exploring the Limits of                    Transfer Learning with a Unified Text to Text Transformer"". I have presented the                    Implementation of Encoder and Decoder Class which are the key components in any Dominant Sequence                    Models. I hope you will also spend some time reading about the Transformer from the Paper mentioned                    above. Excited about the days ahead !!Incase you want to read the Paper mentioned above:                    https://lnkd.in/d_r5Qsh#datascience #machinelearning #deeplearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6727230322901041153/,"Day 54 of #66DaysOfData! with Ken JeeReformer: The Efficient TransformerReformer is a                    Transformer Model which combines two crucial techniques to solve the problems of Attention and                    Memory allocation that limit Transformer Application to Long context windows. Reformer uses LSH to                    reduce the complexity of attending over Long sequences and Reversible residual layers to more                    efficiently use the memory available.In my journey of Natural Language Processing, Today I                    have learned about Long Sequential Data, Transformer Complexity, Locality Sensitive Hashing or LSH                    Attention, Reversible Residual Layers and Reformer from the course ""Natural Language Processing with                    Attention Models"" which is the last course of ""Natural Language Processing Specialization"".                    I have presented the simple Implementation of Reformer Language Model along with the                    Implementation of Training the Reformer Model using Trax here in the Snapshots. I have also                    presented the Implementation of Forward Reversible Layer and Reverse Reversible Layer here. I hope                    you will gain some insights and you will also spend some time learning about the same. Excited about                    the days ahead!!I am happy to share this Natural Language Processing Specialization                    Certificate:https://lnkd.in/d53a88A#machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6726835342856466432/,"Day 53 of #66DaysOfData! with Ken JeeTransformer and NLP :The Transformer is a Deep                    Learning Model which are designed to handle Sequential Data such as Translation and Summarization                    and allows much more parallelization than RNNs such as LSTM.In my journey of Natural                    Language Processing, Today I have learned about Transformers and it's Applications, Dot Product,                    Multi Head and Causal Attentions, The Transformer Decoder and Summarizer, State of Art Transformers                    such as GPT2 or Generative Pretraining for Transformer, BERT or Bidirectional Encoder                    Representations from Transformer, T5 or Text to Text Transfer Transformer and Multitask Training                    Strategy and GLUE Benchmark from the course ""NLP with Attention Models"" which is the last course of                    ""NLP Specialization"".I have presented the simple Implementation of Transformer Language                    Model using Trax here in the Snapshot. I have also presented the Implementation of                    PositionalEncoder, FeedForward and DecoderBlock which returns the list of Layers required for the                    Transformer Model here. I hope you will gain some insights and you will also spend some time                    learning about the Transformer Model. Excited about the days ahead!!Journey of 66Days in                    Natural Language Processing: https://lnkd.in/dSSFhnt#deeplearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6726144030800912384/,"Day 52 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have learned about Sequence to Sequence Models, Alignment, Attention Models, BLEU or Bilingual                    Evaluation Understudy, ROUGE or Recall Oriented Understudy, Greedy Decoding, Random Sampling,                    Temperature Parameter, Beam Search Decoding, Minimum Bayes Risk or MBR and Teacher Forcing Algorithm                    in LSTM from the course ""Natural Language Processing with Attention Models"" which is the last course                    of ""Natural Language Processing Specialization"".Teacher Forcing : Teacher Forcing is the                    technique where the target word is passed as the next input to the Decoder. Training with Teacher                    Forcing converges faster.I have presented the simple Implementation of Neural Machine                    Translation Model which uses Attention along with the Implementation of Preparing Attention Input                    using Encoder and Decoder Activations here in the Snapshots. I have presented this Implementation on                    the basis of this course for self understanding so I have not included all the dependencies here in                    the Snapshot. I hope you will also spend some time going through the topics mentioned above. Excited                    about the days ahead !!Teacher Forcing Algorithm in RNNs:                    https://lnkd.in/dpw2FwC#machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6725788609888182272/,"Day 51 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have learned and Implemented about Siamese Neural Networks and LSTMs, Implementation of Trax in                    Siamese Neural Networks under the hood of ""Natural Language Processing"". I have completed                    working with ""Quora Questions Answers Dataset"" to build a LSTM Model that can identify the Similar                    Questions or the Duplicate Questions which is useful when we have to work with several versions of                    the same Questions. I have build a Model using Trax which can identify the Duplicate Questions.                    Neural Networks and Trax :Trax is good for Implementing new state of the Art Algorithms                    like Transformers, Reformers and BERT. It is actively maintained by Google Brain Team for Advanced                    Deep Learning tasks.I have presented the Implementation of Siamese Neural Network using LSTM                    Model that can identify the Similar or Duplicate Questions here in the Snapshots. I have also                    presented the output of the Model here which fascinates me a lot. I hope you will gain some insights                    from here and you will also spend some time working on the same. Excited about the days ahead                    !!The Notebook with complete Documentation till now is here:                    https://lnkd.in/d2Nsrat#machinelearning #deeplearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6725418687953690624/,"Day 50 of #66DaysOfData! with Ken JeeSiamese Neural Network :Siamese Neural Network is                    an Artificial Neural Network which uses the same weight while working in tandem on two different                    input vectors to compute comparable output vectors. In my journey of Natural Language                    Processing, Today I have learned and Implemented about Data Generators or Iterators, Siamese Neural                    Networks, LSTMs and Triplet Loss under the hood of ""Natural Language Processing"".I have                    continued working with ""Quora Questions Answers Dataset"" to build a Model using LSTM that can                    identify the Similar Questions or the Duplicate Questions which is useful when we have to work with                    several versions of the same Questions. I have presented the Implementation of Siamese                    Neural Network using LSTM Model along with the Implementation of Triplet Loss here in the Snapshots.                    I have also presented the Implementation of Training the Model using Data Generators and other                    dependencies. I have presented all the Implementations using Trax Framework here. I hope you will                    gain some insights from here and you will also continue working on the same. Excited about the days                    ahead !!The Notebook with complete Documentation till now is here:                    https://lnkd.in/d2Nsrat#machinelearning #deeplearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6725064858561216512/,"Day 49 of #66DaysOfData!In my journey of Natural Language Processing, Today I have learned                    about LSTMs and Named Entity Recognition, RNNs and Vanishing Gradients, Siamese Neural Networks,                    Triplets and One Shot Learning from the course ""Natural Language Processing with Sequence Models""                    which is the part of ""Natural Language Processing Specialization"" on Coursera. I have completed                    three Courses in this Specialization.I have started working with ""Quora Questions Answers                    Dataset"" to build a LSTM Model that can Identify the Similar Questions or the Duplicate Questions                    which is useful when we have to work with several versions of the same Questions. I will build this                    Model using Trax Framework which is maintained by Google Brain Team and good for Implementing new                    state of the Art Algorithms like Transformers, Reformers and BERT.I have presented the                    simple Implementation of Data Preparation for training the LSTM Model using Quora Dataset here in                    the Snapshot. I hope you will gain some insights and you will also start working on the same.                    Excited about the days ahead !!Incase you want to see my Notebook, although I have just                    started working on it. The Notebook with complete Documentation till now is here:                    https://lnkd.in/d2Nsrat#datascience #machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6724668297310171136/,"Day 48 of #66DaysOfData! with Ken JeeGated Recurrent Unit :A Gated Recurrent Unit or GRU                    is a gating mechanism in RNN similar to a Long Short Term Memory or LSTM unit but without an output                    gate. GRU solves the vanishing Gradient problem that can come with standard RNN by using an Update                    Gate and a Reset Gate. In my journey of Natural Language Processing, Today I have started                    learning and Implementing NLP from the third course of ""Natural Language Processing Specialization""                    on Coursera i.e ""Natural Language Processing with Sequence Models"". I have covered the topics such                    as Trax and Neural Networks, Dense and ReLU Layers, Serial Layers, RNN and GRU using Trax, Deep and                    Bidirectional RNN under the hood of Natural Language Processing. Neural Networks and Trax                    :Trax is good for Implementing new state of the Art Algorithms like Transformers, Reformers and                    BERT. It is actively maintained by Google Brain Team for Advanced Deep Learning tasks.I have                    presented the simple Implementation of Training GRU Model using Trax here in the Snapshot. I hope                    you will gain some insights from here and you will also spend some time learning about Trax                    Frameworks in Natural Language Processing and Deep Learning. Excited about the days ahead                    !!#deeplearning #machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6724338148551213058/,"Day 47 of #66DaysOfData! with Ken JeeContinuous Bag of Words :In the Continuous Bag of                    Words Model, The distributed representations of context or surrounding words are combined to predict                    the word in the middle. The Model predicts the current word from a window of surrounding context                    words.In my journey of Natural Language Processing, Today I have learned about Ngrams and                    Probabilities, Sequence Probabilities, Smoothing, Word Embeddings, CBOW, Cleaning and Tokenization                    from the course ""Natural Language Processing with Probabilistic Models"" which is the second course                    in ""Natural Language Processing Specialization"" on Coursera. I have completed the two Courses in                    this Specialization. I have presented the simple Implementation of Initializing the Model,                    Softmax Activation Function, Forward Propagation, Cost Function and Back Propagation for training                    the CBOW Model here in the Snapshot. It presents the Mathematics behind simple Neural Network which                    is crucial for understanding the Implementation of Neural Networks and Deep Learning. I hope you                    will gain some insights and you will also spend some time learning about the topics mentioned above.                    Excited about days ahead !!Journey of 66Days in Natural Language Processing:                    https://lnkd.in/dSSFhnt#deeplearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6723934958110441472/,"Day 46 of #66DaysOfData! with Ken JeeMinimum Edit Distance : The Minimum Edit Distance                    between two strings is defined as the minimum number of Editing Operations like Insertion, Deletion                    and Substitution needed to transform one string into another. It's working principle is applicable                    in building the Auto Correction Model.In my journey of Natural Language Processing, Today I                    have started learning and Implementing NLP from the second course of ""Natural Language Processing                    Specialization"" on Coursera i.e ""Natural Language Processing with Probabilistic Models"". I have                    covered the topics such as Autocorrect Models using Minimum Edit Distance Algorithm, POS Tagging,                    Markov's Models and The Viterbi Algorithm under the hood of Natural Language Processing. I will                    spend couple of weeks in this Specialization.I have presented the simple Implementation of                    Minimum Edit Distance Algorithm whose working principle is applicable in Auto Correction Model here                    in the Snapshot. I hope you will also spend some time learning about the topics mentioned above. I                    hope you will also spend some time in this Specialization. I am excited about the days to come                    !!Journey of 66Days in Natural Language Processing:                    https://lnkd.in/dSSFhnt#datascience #machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6723578244630638592/,"Day 45 of #66DaysOfData! with Ken JeeKNearest Neighbors : The KNN Algorithm is a simple,                    supervised machine learning algorithm that can be used to solve both classification and regression                    problems. KNN works by finding the distances between a Query and all the examples in the Data,                    selecting the specified number of examples or K closest to the Query, then votes for the most                    Frequent Label or Averages the Labels.In my journey of Natural Language Processing, Today I                    have completed the first course i.e "" NLP with Classification and Vector Spaces "" from NLP                    Specialization on Coursera. I have covered the topics such as Vector Space Models, Euclidean                    Distance, Cosine Similarity, PCA, KNN, Logistic Regression, Naive Bayes and many more in this                    course. I have presented the simple Implementation of KNN along with techniques for creating                    Hash table here in the Snapshots. Actually, I have presented this Implementation on the basis of                    this Course for self understanding so I have not included all the dependencies here in the Snapshot.                    I hope you will also spend some time going through this Specialization. Excited about the days ahead                    !!I am proud to share my First course certificate in this Specialization:                    https://lnkd.in/dZW-UbD#datascience #machinelearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6723254031910764544/,"Day 44 of #66DaysOfData!Logistic Regression : Logistic Regression is a Statistical Model                    that in its basic form uses a logistic function to model a binary dependent variable, although many                    more complex extensions exist. In Regression Analysis, Logistic Regression is estimating the                    parameters of a Logistic Model in a form of Binary Regression. In my journey of Natural                    Language Processing, Today I have started learning from "" Natural Language Processing Specialization                    "" on Coursera. I have just started learning the first course and I will spend couple of weeks in                    this Specialization. I have covered the topics such as Logistic Regression and Naive Bayes along                    with various Fundamental Preprocessing steps under the hood of "" Natural Language Processing "". This                    Specialization is managed and organized by the team of Andrew Ng i.e deeplearning.ai so that I will                    gain more exposure to Mathematics behind every topics which has very high Importance. I have                    presented the Implementation of Naive Bayes Classifier along with Testing procedure here in the                    Snapshot. I hope you will also spend some time going through this Specialization and I am so much                    excited about the days to come !! #deeplearning #datascience #machinelearning #nlp                    #neuralnetworks #66DaysOfData",False
/feed/update/urn:li:activity:6722859389633490944/,"Day 43 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read and Implemented about MLP Classifier, The Training Routine Class using PyTorch's                    Implementation, The Training Loop, Evaluation, Inference and Inspection of the Model prepared using                    PyTorch under the hood of ""Natural Language Processing with PyTorch"". I have continued                    working in Surname Classification Model Inferring Demographic Information which has applications                    from Product Recommendations to ensuring fair outcomes for users across different Demographics.                    Today, I have worked out the Implementation of MLP Classifier, The Training Routine Class, Inference                    and Inspection of the Model prepared using PyTorch. I have presented the simple                    Implementation of MLP Classifier Model along with the Snapshot of the Inference and Inspection of                    the Model Evaluation using PyTorch. Actually, It is the continuation of yesterday's Snapshot. I hope                    you will gain some insights and you will also spend some time working on the same. Excited about the                    days to come !!I have completed working on the Notebook though I have left to update                    something. The Notebook with complete Documentation up to today is here:                    https://lnkd.in/gVBWsXf#datascience #machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6722486860444127233/,"Day 42 of #66DaysOfData! with Ken JeeThe Vectorizer Class :The Vocabulary converts                    individual tokens into Integers and The Surname Vectorizer is responsible for applying the                    Vocabulary and converting surname into Vectors. Surnames are sequence of characters and each                    character is an individual token in the Vocabulary.In my journey of Natural Language                    Processing, Today I have read and Implemented about PyTorch's Data Representation, The Vocabulary                    Class, The Vectorizer Class and The DataLoader Class under the hood of ""Natural Language Processing                    with PyTorch"".I have continued working in Surname Classification Model Inferring Demographic                    Information which has applications from Product Recommendations to ensuring fair outcomes for users                    across different Demographics. Today, I have worked out the Implementation of Dataset Class,                    Vectorizer Class, Vocabulary Class and Multi Layer Perceptron or MLP Classifier Model.I have                    presented the Implementation of Vectorizer Class using PyTorch here in the Snapshot. I hope you will                    gain some insights from here and you will also spend some time working on the same. Excited about                    the days to come !!The Notebook with complete Documentation up to today is here:                    https://lnkd.in/gVBWsXf#datascience #machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6722156737652232192/,"Day 41 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read and Implemented about Feed Forward Networks for Natural Language Processing using PyTorch.                    I have covered the topics such as The Multi Layer Perceptron (MLP), Simple XOR Functions with MLP                    using PyTorch and Softmax Activation Function under the hood of ""Natural Language Processing with                    PyTorch"". I have started working in Surname Classification Model Inferring Demographic                    Information which has applications from Product Recommendations to ensuring fair outcomes for users                    across different Demographics. I will be using PyTorch for building the Classifier Model.I                    have presented the Implementation of PyTorch in building the simple MLP Class here in the Snapshots.                    I have also presented the techniques for processing the raw Dataset for Surname Classification                    Project using PyTorch. I hope you will gain some insights and you will also spend some time learning                    about MLP and working on the same and get ready for building the Model. Excited about the days ahead                    !!Incase you want to see my Notebook, although I have just started working on it. The                    Notebook with complete Documentation till now is here: https://lnkd.in/gVBWsXf#datascience                    #machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6721829160752291840/,"Day 40 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have continued working on yesterday's Notebook which was of Text Dataset i.e Amazon Electronics                    Reviews Dataset for Analysis using only TensorFlow and TensorBoard. Actually, I spent most of my                    time in Training the Model and I tried to optimize the Model to increase the speed of Training but                    still the Training procedure took most part of my time using GPU as well. Apart from that, Today I                    have spent some time working on Greedy Programming Algorithms. Actually I got a chance to interview                    from one of the Tech Giants as well.I had prepared a Model using LSTM which was trained on                    Amazon Reviews Dataset. This Snapshot is the continuation of yesterday's Snapshots. So, I have                    presented some basic workflow of Model Evaluation and deploying the Trained Model on unseen Text                    Data for Analysis. I have also presented the simple technique of Data Visualization for evaluating                    the Model here in the Snapshot. I hope you will also spend some time working on the same Dataset. I                    am so much excited about the days to come !!The Notebook with complete Documentation till                    now is here: https://lnkd.in/d5jC5Sb#datascience #machinelearning #deeplearning                    #neuralnetworks #tensorflow #66DaysOfData",False
/feed/update/urn:li:activity:6721433303536177152/,"Day 39 of #66DaysOfData! with Ken JeeLong Short Term Memory (LSTM): Long Short Term                    Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of                    Deep Learning. LSTM has Feedback connections. It can not only process single data points, but also                    entire sequences of data such as Speech or Video. In my journey of Natural Language                    Processing, Today I have started working on new Text Dataset i.e Amazon Electronics Reviews Dataset                    for Analysis using only TensorFlow and TensorBoard. I will be working on the same until I finish it                    completely. Apart from that, Today I have watched some videos on YouTube and read some Notebooks in                    Kaggle under the hood of Natural Language Processing.I have presented the overall                    Implementation of TensorFlow and TensorBoard in Processing the Data such as Tokenization and                    Encoding and the techniques for preparing the complete LSTM Model here in the Snapshots. I hope you                    will gain some insights and you will also spend some time working on the same. Excited about the                    days to come!!Incase you want to see my Notebook although I am still working on it. The                    Notebook with complete Documentation till now is here: https://lnkd.in/d5jC5Sb#deeplearning                    #machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6721039622316552192/,"Day 38 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read and Implemented about Perceptron Classifier, The Training Routine Class using PyTorch's                    Implementation, The Training Loop, Evaluation, Inference and Inspection of the Model prepared using                    PyTorch under the hood of ""Natural Language Processing with PyTorch"". I am continuing my journey                    along with the book, ""Natural Language Processing with PyTorch"". Today, I have continued working                    with YELP Review Dataset for Sentiment Analysis using PyTorch.I have presented the simple                    Implementation of PyTorch in Training the Classifier Model along with the process of Instantiating                    the Dataset, Model, Loss, Optimizer and Training State here in the Snapshots. Actually, It is the                    continuation of yesterday's Snapshot. I hope you will gain some insights about the Implementation of                    PyTorch in Natural Lanuage Processing from here. And I hope you will also spend some time working on                    the same. I am so much excited about the days to come !! Incase you want to see my Notebook,                    I have presented the Implementation of PyTorch in YELP Reviews Dataset for Sentiment Analysis. The                    Notebook with complete Documentation till now is here: https://lnkd.in/dJVAeTe#datascience                    #machinelearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6720699754113716224/,"Day 37 of #66DaysOfData! with Ken JeeThe Vocabulary Class :The Vocabulary Class not only                    manages the Bijection i.e Allowing user to add new Tokens and have the Index auto increment but also                    handles the special token called UNK which stands for Unknown. In my journey of Natural                    Language Processing, Today I have read and Implemented about PyTorch's Data Representation, The                    Vocabulary Class, The Vectorizer Class and The DataLoader Class under the hood of "" Natural Language                    Processing with PyTorch "". I am continuing my journey along with the book "" Natural Language                    Processing with PyTorch "".I have presented the Implementation of Dataset Class using PyTorch                    here in the Snapshot. I have been working on the Implementation of Dataset Class, Vectorizer Class                    and DataLoader Class and I feel quite overwhelmed with the complexity of PyTorch because I am not                    familiar with PyTorch Framework. But I am keeping track on every lines of the Code. I hope you will                    also spend some time working on the same and I am so much excited about the coming days                    !!Incase you want to see my Notebook although I haven't completed working on it. The                    Notebook with complete Documentation is here: https://lnkd.in/djXePep#datascience                    #machinelearning #66DaysOfData",False
/feed/update/urn:li:activity:6720346702437482496/,"Day 36 of #66DaysOfData! with Ken JeeRegularization :In Machine Learning, Regularization                    is the process of adding Information in order to solve a well posed problems or to prevent                    Overfitting. In my journey of Natural Language Processing, Today I have read and Implemented                    about Model Selection approaches, Choosing Loss Functions, Choosing Optimizers, Gradient Based                    Supervised Learning, Evaluation Metrics, Regularization and Early Stopping under the hood of ""                    Natural Language Processing with PyTorch "". I have started working on the YELP Reviews Dataset and                    the Neural Networks will be Implemented using PyTorch. I have presented some Data                    Preprocessing Techniques which I have Implemented while working with YELP Reviews Dataset here in                    the Snapshot. I haven't completed working on the same. I hope you will also spend some time working                    on the same and I am so much excited to Implement PyTorch in Natural Language Processing in coming                    days !! Incase you want to work with YELP Reviews Dataset and see my Notebook as well                    although I haven't completed working on it. The Notebook with complete Documentation till now is                    here: https://lnkd.in/d-NYPvxHere, I have added the Dataset as well.#datascience                    #machinelearning #nlp #deeplearning #66DaysOfCode",False
/feed/update/urn:li:activity:6719984231608553472/,"Day 35 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read and Implemented The Supervised Learning Paradigm, Computational Graphs, PyTorch Basics and                    various NLP fundamentals such as Tokenization, Lemmatization, Tagging and Semantics. I have also                    read and Implemented about Activation Functions such as Sigmoid, Tanh, ReLU, Softmax and Loss                    Functions such as Mean Squared Error and Cross Entropy under the hood of ""Natural Language                    Processing with PyTorch"". I have presented the Implementation of PyTorch in various                    Activation Functions and Loss Functions along with few preliminaries for Understanding and working                    with PyTorch here in the Snapshots. I hope you will get some insights about the Implementations of                    PyTorch and I hope that you will also spend some time working on the same and get ready to start the                    Implementation of PyTorch in Natural Language Processing in coming days !!In case you have                    missed something interesting in Natural Language Processing. I have been Documenting about NLP since                    the start of my journey. The complete Documentation is here:                    https://lnkd.in/dAsCwde#datascience #machinelearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6719639464047587328/,"Day 34 of #66DaysOfData!In my Journey of Natural Language Processing, Today I have read                    about Sentence Segmentation, Named Entity Recognition, Understanding Chatbot Approaches and few NLP                    Pipelines under the hood of Natural Language Processing. I have also started reading the book                    ""Natural Language Processing with PyTorch"". Actually, I had never worked with PyTorch and                    never found any particular reason to start with PyTorch. But, Today I got motivated to start Natural                    Language Processing with PyTorch. I will be reading and Implementing Natural Language Processing                    with PyTorch from today. I have presented the simple Implementation of AIML Bot and the                    Vectorization concept here in the Snapshot. I am fond of revisiting the small concepts again and                    again so that I won't get stuck while Implementing in real problems. I am so excited to start                    Natural Language Processing with PyTorch in coming days !!#datascience #machinelearning #nlp                    #pytorch #66DaysOfCode",False
/feed/update/urn:li:activity:6719242704963821568/,"Day 33 of #66DaysOfData! with Ken Jee"" Sequence to Sequence Model ""- Sequence to                    Sequence Neural Networks can be built with a modular and reusable Encoder and Decoder Architecture.                    The Encoder Model generates a Thought Vector which is a Dense and fixed Dimension Vector                    representation of the Data. The Decoder Model use Thought Vectors to generate Output Sequences.                    In my journey of Natural Language Processing, Today I have read and Implemented various                    topics under Sequence to Sequence Networks. I have continued working on the Chatbot using Sequence                    to Sequence Learning. I have used the Keras Functional API and Cornell Dialog Dataset for Training                    the Model.I have presented the Implementation of Thought Encoder and Thought Decoder using                    Keras Functional API here in the Snapshot. I have also presented the techniques for Training the                    Model and Generating the Response Sequences here. I hope you will gain some insights from it and you                    will also spend some time working on the same. I am excited about the days to come !!Incase                    you want to see my Notebook though I haven't completed working on it. The Notebook with complete                    Documentation till now is here: https://lnkd.in/d2i2HYk#datascience #machinelearning #nlp                    #66DaysOfCode",False
/feed/update/urn:li:activity:6718875531523198976/,"Day 32 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read and Implemented the topics such as Encoder and Decoder Architecture in Sequence to                    sequence Models, Thought Vector, Decoding Thought using LSTM, Sequence Encoder and Keras Functional                    API in assembling Sequence to Sequence Pipeline under the hood of Natural Language                    Processing.I have started working on building a Chatbot using Sequence to sequence Neural                    Networks and Keras Functional API. I have presented the simple Implementation of processing the Text                    Corpus and few steps to make the Text Data ready to train the Sequence to sequence Chatbot here in                    the Snapshot. I will be using the Cornell Dialog Dataset for training the Chatbot. I hope you will                    gain some insights from it and you will also spend some time working on the same. I will continue                    working on this Project and I am so much excited about the days to come !!Incase you want to                    see my Notebook though I haven't completed working on it. The Notebook with complete Documentation                    till now is here: https://lnkd.in/dTWGQbu#datascience #machinelearning #deeplearning #nlp                    #66DaysOfCode",False
/feed/update/urn:li:activity:6718493233124536320/,"Day 31 of #66DaysOfData! with Ken JeeLong Short Term Memory or LSTM "" Long Short Term                    Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of                    Deep Learning. Unlike standard Feedforward Neural Networks, LSTM has Feedback connections. It can                    not only process single data points, but also entire sequences of data such as Speech or Video. LSTM                    is applicable for Text Generation as well. "" Today I have read and Implemented about                    Generating the Text using Long Short Term Memory or LSTM. I have prepared a Model using LSTM which                    generates the Text similar to William Shakespeare's writing. I have used the Gutenberg Corpus which                    contains the 3 plays of Shakespeare to train the Neural Network. I have presented the                    Implementation of LSTM Model as well as the Implementation of Keras API in Text Generation here in                    the Snapshot. I have also presented the Snapshot of Generated Text with the help of LSTM Model. I                    hope you will gain some insights and you will also spend some time working on the same. I am excited                    about the days to come !!I have completed working on Generating Text with the help of LSTM                    Model. The Notebook with complete Documentation is here:                    https://lnkd.in/dFB2nv3#66DaysOfCode #datascience #nlp #deeplearning",False
/feed/update/urn:li:activity:6718145981017288704/,"Day 30 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read and Implemented about Long Short Term Memory or LSTM. I have covered the topics such as                    LSTM working principle, Backpropagation through time, Keras API and various other topics similar to                    CNN and RNN as mentioned in the previous Snapshots under the hood of NLP. Actually, I have primarily                    focused on Implementing the LSTM Model in the same Large Movie Review Dataset to compare the                    effectiveness of CNN, RNN and LSTM on Sentiment Analysis of Text Data. Long Short Term                    Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of                    Deep Learning. Unlike standard Feedforward Neural Networks, LSTM has Feedback connections. I                    have presented the Simple Implementation of Long Short Term Memory or LSTM Model in the same Large                    Movie Review Dataset. I have also presented the short info of all the Parameters and codes mentioned                    here in the Model. I hope you will gain some insights about the Implementation of LSTM Model in                    Sentiment Analysis. Excited about the days ahead !!I have completed working on the same with                    LSTM Model. The Notebook with complete Documentation is here:                    https://lnkd.in/dgaUGkB#datascience #machinelearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6717788232160956416/,"Day 29 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read and Implemented about Simple Recurrent Neural Network as well as Bidirectional Recurrent                    Neural Network. Here, I have covered the topics such as Backpropagation through Time,                    Hyperparameters, Statefulness, Bidirectional Networks and various other topics similar to                    Convolutional Neural Network mentioned in the previous Snapshot. I have Implemented the                    Recurrent Neural Network in the same Large Movie Review Dataset to predict the Sentiment of Text                    Data. I have presented the overall Implementation of RNN Model here in the Snapshot. I have also                    presented the short info of all the Parameters and codes mentioned here in the Model. I hope you                    will gain some insights about the Implementation of RNN Model in Sentiment Analysis. Although the                    Implementation of RNN Model is not so appreciable, I hope you will spend some time understanding the                    working principle of RNN and working on the same.I have completed working on the Sentiment                    Analysis of Large Movie Review Dataset using Simple RNN as well as Bidirectional RNN. The Notebook                    with complete Documentation is here: https://lnkd.in/d7-zVHJ#datascience #machinelearning                    #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6717423517610082306/,"Day 28 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read and Implemented about Convolutional Neural Network for Natural Language Processing. I have                    covered the topics such as Convolutional Neural Network Architecture, Pooling, Dropout, CNN                    parameters, Optimization and Training CNN Model under the hood of Convolutional Neural Network for                    Natural Language Processing.I have presented the overall Implementation of Convolutional                    Neural Network here in the Snapshot. I have also presented the short info of all the parameters                    mentioned in the CNN Model and the process of Compiling and Training the Model as well. I hope you                    will gain insights about the Implementation of CNN Model in Sentiment Analysis. Actually, It is the                    continuation of yesterday's Snapshots. I hope you will also spend some time working on it. Excited                    about the days to come !!I have completed working on the Sentiment Analysis of Large Movie                    Review Dataset. I have prepared a Model using Convolutional Neural Network which can classify the                    Sentiment of Text Data. The Notebook with complete Documentation is here:                    https://lnkd.in/dxW6nMvJourney of 66Days in Natural Language Processing:                    https://lnkd.in/dSSFhnt#datascience #machinelearning #deeplearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6717123209503752192/,"Day 27 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read and Implemented about Convolutional Neural Network for Natural Language Processing. I have                    covered the topics such as CNN building blocks, Step size or Stride, Filter Composition, Padding,                    Convolutional Pipeline, Learning, Narrow Windows and Implementing the Keras API under the hood of                    Convolutional Neural Network for NLP.I have started working on the Sentiment Analysis of                    Large Movie Review Dataset which was compiled for the 2011 paper ""Learning Word Vectors for                    Sentiment Analysis"". Since, It is a very large Dataset, I have used just the subset of the Dataset.                    I will be Implementing CNN for this Project.I have presented the basic Implementations of                    approaching the Dataset such as Importing the Dependencies, Processing the Dataset with Tokenization                    and Google News pretrained Model Vectorization and Splitting the Dataset into Training set and Test                    set.I have presented the overall Implementation of above mentioned approaches with proper                    Documentation here: https://lnkd.in/dE_mhq3I have just initialized working on this Notebook and                    I will update it everyday.#datascience #machinelearning #nlp #DL #66DaysOfData",False
/feed/update/urn:li:activity:6716731798254059520/,"Day 26 of #66DaysOfData!In my journey of Natural Language Processing, Today I have read and                    Implemented about Word Vectors, Softmax Function, Negative Sampling, Document Similarity with                    Doc2Vec and Google's Word2vec, GloVe and Facebook's FastText Models which were pretrained on                    Billions of Text Data. I have primarily focused on learning and Implementing the Word2vec pretrained                    Model today. I am continuing my learning journey along with the book, ""Natural Language Processing                    in Action"".Word2vec is a Model for Natural Language Processing. The Word2vec Algorithm uses                    a Neural Network Model to learn Word associations from a large corpus of text. Once Word2vec Model                    is trained, It can detect Synonymous words or suggest additional words for a partial sentence.                    Word2vec is a group of related Models that are used to produce Word Embeddings.I have                    presented the Implementation to access the Google's Word2vec pretrained Model and it's basic                    Functions and the process to create own Domain Specific Word2vec Model. I have also presented the                    Implementation of Doc2vec Model here in the Snapshot. I hope you will also spend so time to learn                    about Word2vec pretrained Model. Excited about the days to come !!#datascience                    #machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6716364064429355008/,"Day 25 of #66DaysOfData!In my journey of Natural Language Processing, Today I have started                    learning and Implementing Neural Networks and Deep Learning for Natural Language Processing. I have                    completed the Implementation of LSA and LDIA in Semantic Analysis along with LDA. I have read the                    topics such as Neural Networks and Perceptrons, Gradients, Local and Global Minimum and                    Backpropagation under the hood of Neural Networks and Deep Learning. Actually, I have primarily                    focused on reading the topics needed to understand the Neural Networks and Deep Learning rather than                    Implementing the concepts.I have presented the Simple workflow of Neural Networks using                    Keras API. I will be Implementing the Keras API in Natural Language Processing from today. I hope                    you will also spend some time to learn the basic topics which I have mentioned above to understand                    the Neural Networks and Deep Learning. Excited to learn and Implement Neural Networks for NLP in                    coming days !!I have completed the Implementation of LSA and LDIA in Semantic Analysis:                    https://lnkd.in/g4eKCm2#deeplearning #machinelearning #nlp #neuralnetworks #66DaysOfData",False
/feed/update/urn:li:activity:6715979958855970816/,"Day 24 of #66DaysOfData! with Ken Jee In my journey of Natural Language Processing, Today I                    have read and Implemented Latent Semantic Analysis ""LSA"", Singular Value Decomposition ""SVD"",                    Principal Component Analysis ""PCA"", Truncated SVD and Latent Dirichlet Allocation ""LDIA"". I have                    primarily focused on reading and Implementing LSA and LDIA for Semantic Analysis. LSA works well                    with Normalized TFIDF Vectors whereas LDIA works well with raw Bag of Words ""BOW"" Count                    Vectors.Semantic Analysis is the process of relating the Syntactic structures from the                    levels of Phrases, Clauses, Sentences and Paragraphs to the level of the writing as a whole and to                    their Language dependent meanings.I have presented the Implementation of Linear Discriminant                    Analysis ""LDA"" while working with TFIDF Vectors and BOW Count Vectors here in the Snapshot. I hope                    you will gain insights about the Implementation of LDA Classifier along with LDIA Topic Vectors and                    BOW count Vectors. Excited about the days to come !!I have presented the overall                    Implementation of Semantic Analysis with LSA, LDA and LDIA with proper Documentation here:                    https://lnkd.in/dd-wi6D#datascience #machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6715636845201375232/,"Day 23 of #66DaysOfData!In my journey of Natural Language Processing, Today I have started                    learning about Semantic Analysis. I have read and implemented about Topic Vectors, Algorithms for                    Scoring Topics and Semantic Analysis such as Latent Semantic Analysis ""LSA"", Linear Discriminant                    Analysis ""LDA"" and Latent Dirichlet Allocation ""LDIA"". Today, I primarily focused on reading and                    Implementing about Linear Discriminant Analysis ""LDA"". LDA is one of the most straight forward and                    fast Dimension Reduction and Classification Models. In Natural Language Processing, Semantic                    Analysis is the process of relating the Syntactic structures from the levels of Phrases, Clauses,                    Sentences and Paragraphs to the level of the writing as a whole and to their Language dependent                    meanings. I have presented the Implementation of LDA's working Principal which states that                    Computing the centroid of TFIDF vectors for each side of the binary Class here in the Snapshot. I                    hope you will gain insights about Implementation of LDA Classifier and creating NLP Pipeline of                    Tokenizer and Vectorizer from here. And I hope you will spend some time in learning about Semantic                    Analysis as well. Excited about the days ahead !!#datascience #machinelearning #nlp                    #66DaysOfCode",False
/feed/update/urn:li:activity:6715246191619125248/,"Day 22 of #66DaysOfData!In my journey of Natural Language Processing, Today I have read and                    Implemented about Bag of Words, Vectorizing concept, Vector Spaces, Cosine Similarity, Zipf's Law                    and Inverse Frequency concept, Text Modeling, TFIDF, Relevance Ranking and Okapi BM25 concept. I                    have completed the first three chapters of the book ""Natural Language Processing in Action"" and this                    chapter primarily focuses on the concept of Vectorizing the Tokens which are obtained after                    Tokenization using TFIDF Vectorizer. Text Vectorization is the process of converting Text into                    Numerical representation. I have also read and Implemented the concept of Cosine Similarity under                    the hood of Natural Language Processing.I have presented the Implementation of TFIDF                    Vectorizer and also the process of Tokenizing the Text Documents and removing the Stopwords. I have                    also Implemented the Cosine Similarity using Numpy and pure Python as well in this Snapshot. I hope                    you will gain insights about Text Vectorization and Tokenization from here. Excited about the days                    to come !!Journey of 66Days in Natural Language Processing:                    https://lnkd.in/dSSFhnt#datascience #machinelearning #nlp #deeplearning #66DaysOfCode",False
/feed/update/urn:li:activity:6714878823738961920/,"Day 21 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read and implemented about Stopwords, Stemming, Lemmatization and Sentiment Analysis using                    VADER Approach which is the Algorithm composed by human and also the Machine Learning Approach with                    the help of Naive Bayes Classifier. I have completed the first 2 chapters of the book, ""Natural                    Language Processing in Action"" and it is helping me a lot along my journey.I have presented                    the Implementation of VADER Approach in Sentiment Analysis along with Naive Bayes Classifier and I                    have also included the Implementation of Casual Tokenizer in the Movies Dataset. I hope you will                    gain insights about the Implementation of VADER Approach and Naive Bayes in Sentiment Analysis.                    Actually, VADER Approach is not as efficient as Machine Learning Approach such as Naive Bayes                    Classifier. I hope you will spend some time in learning about Naive Bayes in Sentiment Analysis                    along with VADER Approach. I am really grateful for your supports. Excited about the days to come                    !!Journey of 66Days in Natural Language Processing:                    https://lnkd.in/dSSFhnt#datascience #machinelearning #deeplearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6714531748539052032/,"Day 20 of #66DaysOfData!In my journey of Natural Language Processing, Today I have read a                    couple of topics under the hood of Natural Language Processing such as Tokenized Phrase, Dot Product                    in NLP, Bag of Words and Overlapping, Token improvement with Regex which is apart from Regular                    Expressions, Contractions, Extending Vocabulary with NGrams. I have also read and implemented the                    Regex Tokenizer, Tree Bank Tokenizer and Casual Tokenizer. Actually I am continuing my                    learning journey with a book ""Natural Language Processing in Action"" and lots of the preprocessing                    concepts which I have already read are coming along my way. I prefer to go through the concepts                    again because I don't want to skip any topics from this book. Although the concepts might match                    along the way I won't repeat the same implementation in any of my Snapshots. I have                    presented the Implementation of Regex Tokenizer, Tree Bank Tokenizer, Casual Tokenizer and NGrams                    here in the Snapshots. These Tokenization steps are more better than Traditional Tokenization steps                    using Regular Expressions. I hope you will spend some time to learn about these Tokenizers. Excited                    about the days ahead !!#datascience #machinelearning #nlp #deeplearning #66DaysOfCode",False
/feed/update/urn:li:activity:6714151396586856448/,"Day 19 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have read and implemented the first chapter of the book, ""Natural Language Processing in Action"". In                    this chapter, I have covered the topics such as Natural Language Processing and the Magic, Regular                    Expressions, Finite State Machine (FSM) concept, Word order and Grammar, simple NLP Chatbot pipeline                    and Natural Language IQ.I have presented the simple Chatbot using Regular Expressions and                    Finite State Machine (FSM) concept. Basically, I will be working on much advanced Chatbots using                    Neural Networks in coming days. I hope you will also google out the FSM concept in NLP and also the                    Implementation of Regular Expressions in FSM from here. Excited about the days ahead                    !!#datascience #machinelearning #deeplearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6713839483793498112/,"Day 18 of #66DaysOfData!In my journey of Natural Language Processing, Today I have read a                    topic of Natural Language Processing in the book ""Dive into Deep Learning"" by Aston Zhang. Here, I                    have covered the topics such as Text Processing, Machine Translation, Natural Language Processing                    pre training and Word Embedding. The information and explanations were great and the code                    implementation is in MXNET. I am not quite familiar with MXNET framework so I have just presented a                    small snapshot here.Apart from that, Today I have read a very small part of the book                    ""Natural Language Processing in Action"" by Hobson Lane. I am so much thankful to Anthony Camarillo                    for sharing this book with me. And I will give continuation with this book ""Natural Language                    Processing in Action"". Excited about the days ahead !!#datascience #machinelearning #nlp                    #66DaysOfCode",False
/feed/update/urn:li:activity:6713425804283387905/,"Day 17 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have learned about the Implementation of Fastai in preparing a Sentiment Classifier Model. I have                    prepared a Model using Fastai API which can classify Sentiment of Internet Movie Database reviews                    i.e. classifying the Positive or Negative Sentiment. Fastai's API is really powerful and                    effective so that the Model can classify the Sentiment of Internet Movie Database reviews with above                    90% accuracy in just few lines of code. I have learned about Word Tokenization, Subword                    Tokenization, Numericalization, TextBlock and DataBlock API and Training the Classifier Model using                    Fastai. I have presented the snapshot of the Implementation of Fastai API in preparing the                    Language Model and Training the Model. I have also presented the Implementation of Fastai API in                    preparing the Classifier Model using the Language Model and also the concept of unfreezing the                    Model. I hope you can gain insights from here as well. This snapshot is the continuation of                    yesterday's snapshot. Excited about the days ahead !!Sentiment Classification of ""Internet                    Movie Database reviews"" : https://lnkd.in/dg6B9c9#datascience #machinelearning #nlp                    #66DaysOfData",False
/feed/update/urn:li:activity:6713104098746490880/,"Day 16 of #66DaysOfData! In my journey of Natural Language Processing, Today I have learned                    the Implementations of NLP from Fastai course which has been published recently. As the Fastai                    course primarily focuses on the Coding part and follows the top down aspect of Learning and                    Teaching. It's bit complicated to learn than other courses. Fastai's API is really amazing and                    powerful as well. I learned the basic steps of NLP with Fastai such as Word Tokenization, Subword                    Tokenization, Numericalization, and Preparing TextBlock and DataBlock. I am currently working on                    Sentiment Analysis of IMDB reviews using Fastai.I have shared the Implementations of the                    Word Tokenization, Subword Tokenization, Numericalization and process to prepare the TextBlock and                    DataBlock with Fastai here. I hope you can gain insights about the Implementation of Fastai from                    here. Excited about the days ahead!!#datascience #machinelearning #deeplearning #nlp                    #66DaysOfCode",False
/feed/update/urn:li:activity:6712694237743341568/,"Day 15 of #66DaysOfData! with Ken JeeSingular Value Decomposition (SVD) The words that                    appear most frequently in one topic would appear less frequently in the other, otherwise that word                    wouldn't make a good choice to separate out the two topics. Therefore, the topics are Orthogonal.                    The SVD algorithm factorizes a matrix into one matrix with orthogonal columns and one with                    orthogonal rows along with diagonal matrix which contains the relative importance of each factor.                    NonNegative Matrix Factorization (NMF) Non Negative Matrix Factorization (NMF) is a                    factorization or constrain of non negative dataset. NMF is non exact factorization that factors into                    one short positive matrix.Topic Frequency Inverse Document Frequency (TFIDF) TFIDF is a                    way to normalize the term counts by taking into account how often they appear in a document and how                    long the document is and how common or rare the document is.In my journey of Natural                    Language Processing, Today I have learned and implemented SVD, NMF and TFIDF in Topic Modeling                    project. I have captured just the overview of the implementations here. I hope you can gain                    insights.Topic Modeling with SVD and NMF : https://lnkd.in/deXFvnd#datascience                    #deeplearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6712342567595343872/,"Day 14 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have learned about Sequence Classification, Sentence Segmentation and various Evaluation methods                    under the hood of Natural Language Processing. I have covered the fundamental topics such as Test                    Data, Accuracy measure, Precision and Recall, Confusion Matrices, Cross validation, Decision Trees                    and Naive Bayes with proper implementations which are so helpful for my understanding.I have                    been in this journey for 2 weeks and I have covered all the fundamentals which are so relevant in                    Natural Language Processing. I had been following the ""Natural Language Processing with Python"" book                    and it really helps me a lot. Now, I will be focusing more on Implementations so that I will be                    following the course of Fastai on Natural Language Processing. I have implemented the Naive Bayes                    Classifier in Text Corpus and I hope you can gain insight about the Implementation of Naive Bayes.                    Excited about the days ahead!!#datascience #machinelearning #deeplearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6712018164508590080/,"Day 13 of #66DaysOfData! with Ken JeeSupervised Classification : Classification is the                    process of choosing the correct class label for a given input. The Classification is said to be                    Supervised Classification if it is built based on training corpora containing the correct label for                    each input. One common example of Classification is deciding whether an Email is spam or                    not.In my journey of Natural Language Processing, Today I have learned about Supervised                    Classification. I have covered the topics such as Choosing Right Features, Document Classification,                    Gender Identification, Part of Speech Tagging using Naive Bayes Classifier and Decision Trees                    Classifier under the hood of Supervised Classification of NLP. I have presented the basic                    Implementation of Naive Bayes Classifier in Document Classification and I hope you can gain insights                    from it as well. Excited about the days ahead !!I have been capturing my progress in Natural                    Language Processing in GitHub. I will appreciate your thoughts and recommendations. :                    https://lnkd.in/dE4vrji#datascience #machinelearning #deeplearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6711613514819112960/,"Day 12 of #66DaysOfData! with Ken JeePart of Speech Tagging : The process of classifying                    words into their ""Parts of Speech"" and Labeling them accordingly is known as Part of Speech Tagging                    which is also known as POS tagging or simply Tagging. The collections of tags used for a particular                    task is known as Tagset.In my journey of Natural Language Processing, Today I have learned                    about Automatic Tagging such as Default Tagger, Regular Expression Tagger and Lookup Tagger along                    with N-Gram Tagging such as Unigram Tagger, Bigram Tagger and so on. I have also learned about                    Combining Taggers using backoff parameter. I hope you can gain insights about the Implementation of                    Tagging. Excited about the days to come !!#datascience #machinelearning #nlp #deeplearning                    #66DaysOfCode",False
/feed/update/urn:li:activity:6711293057079234560/,"Day 11 of #66DaysOfData! with Ken JeeIn my journey of Natural Language Processing, Today I                    have completed all the preliminaries process or techniques required in Natural Language Processing                    included in the book, ""Natural Language Processing with Python"". I have completed the topics such as                    Tokenization, Stemming, Lemmatization and Text Processing with Regular Expressions. Apart from that,                    I have also worked with various Text Corpora such as Brown Corpus, Inaugural Address Corpus, Reuters                    Corpus and so on. And I have completed first 110 pages of the book, ""Natural Language Processing                    with Python"".I have plotted a simple bar plot using various categories of Brown Corpus                    presenting the Frequency Distribution of various words appearing inside the Corpus. I hope you can                    gain insights from the plot as well. Excited about the days ahead !!#datascience                    #machinelearning #deeplearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6710896747821981696/,"Day 10 of #66DaysOfData!In my journey of Natural Language Processing, Today I have learned                    about the various useful applications of Regular Expressions such as Finding Word Stems, Regular                    Expressions for Tokenizing Text, Normalizing Text such as Stemmers and Lemmatization. I have                    also read about the issues of Tokenization : Tokenization turns out to be far more difficult task                    than one might have expected. No single solution works well accross the board. Another issue of                    Tokenization is the presence of contractions as well. Example: didn't .#datascience                    #machinelearning #deeplearning #nlp #66DaysOfCode",False
/feed/update/urn:li:activity:6710521293541801984/,"Day 9 of #66DaysOfData!In my journey of Natural Language Processing, Today I have learned                    about Text Processing with Unicode and Regular Expressions for Detecting Word Patterns with proper                    explanations. Within a Program, we can manipulate Unicode just like normal strings. Unicode are                    encoded as stream of bytes. Examples: ASCII, Latin-2, UTF-8.I have also implemented the                    Regular Expressions for Detecting the Word Patterns using basic meta characters such as:*                    Dollar sign ( $ ) matches the characters of the end of word.* Dot symbol ( . ) matches any                    single character.* Caret symbol ( ^ ) matches the characters of the start of word.* Question                    mark ( ? ) specifies the previous character is optional.* Plus sign ( + ) means one or more                    instances of the preceding item.* Sign ( * ) means zero or more instances of the preceding                    item.* Backslash ( \ ) means following character is deprived.* Pipe symbol ( | ) specifies                    the choice between left and right.#datascience #machinelearning #nlp #66DaysOfData!",False
/feed/update/urn:li:activity:6710097165345136640/,"Day 8 of #66DaysOfData!In my journey of Natural Language Processing, Today I am learning                    about Processing Raw Text in NLP. Basically, I have completed processing the Text from Electronic                    Books and from HTML documents. Apart from that, I have learned about WordNet. The topics I have                    covered in WordNet are: * The WordNet Hierarchy and  * Semantic                    Similarity#datascience #machinelearning #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6709643932906602496/,"Day 7 of #66DaysOfData!In my journey of Natural Language Processing, Today I have learned                    about different Text Corpora and Basic Corpus Functionality defined in NLTK. The topics I have                    covered are summarized below: * Web and Chat Text * Brown Corpus * Reuters Corpus *                    Inaugural Address Corpus * Annotated Text Corpora * Corpora in Other Languages *                    Conditional Frequency DistributionsI have also learned about Loading own Corpus, Plotting                    and Tabulating Distributions and Generating Random Text with Bigrams. Since, I have started working                    and learning from the book ""Natural Language Processing with Python"", I have completed first 50                    pages till now. Excited about the days ahead !!#datascience #machinelearning #deeplearning                    #nlp #66DaysOfData",False
/feed/update/urn:li:activity:6709080935389782017/,"Day 6 of #66DaysOfData!In my journey of Natural Language Processing, Today I have learned                    about various interesting challenges with proper explanation of each topic under the hood of NLP                    such as: * Word Sense Disambiguation * Pronounce Resolution * Generating Language                    Output * Machine Translation * Spoken Dialog System  * Textual Entailment I have                    also explored about Gutenberg Corpus using NLTK and Python. Again, I am so thankful to Ken Jee who                    has started this #66DaysOfData journey. It's really helping me a lot. Each day I have a thought of                    learning something in NLP. Excited about the days ahead!!#datascience #machinelearning #NLP                    #66DaysOfData",False
/feed/update/urn:li:activity:6708415168025522176/,"Day 5 of #66DaysOfData!As a part of journey, I have started the book, ""Natural Language                    Processing with Python"". It's really amazing and I have encountered some very basic functions but                    unknown to me such as concordance function, similar function, common context function and a basic                    dispersion plot as well. I will be using this book in my journey. Excited for the days ahead                    !!Link to NLP with Python: https://www.nltk.org/book/#datascience #machinelearning                    #66DaysOfCode",False
/feed/update/urn:li:activity:6708197133431906304/,"Day 4 of #66DaysOfData!"" Lemmatization in Natural Language Processing """" Lemmatization                    is the process of grouping together the inflected forms of words so that they can analysed as a                    single item, identified by the word's lemma or a dictionary form. It is the process where individual                    tokens from a sentence or words are reduced to their base form.Lemmatization is much more                    informative than simple stemming. Lemmatization looks at the surrounding text to determine a given                    words's part of speech where it doesn't categorize the phrases. ""Today, I learned about                    Lemmatization and it's simple implementation using Spacy as well NLTK. I have covered the                    fundamentals of NLP such as Tokenization, Stemming and Lemmatization. Excited for the days ahead                    !!#datascience #machinelearning #deeplearning #66DaysOfData",False
/feed/update/urn:li:activity:6707681243489689600/,"Day 3 of #66DaysOfData!""Stemming in Natural Language Processing""""Stemming is a process                    of converting a word to its most general form or stem. It's basically the process of removing the                    suffix from a word and reduce it to its root word. It helps in reducing the size of                    vocabulary""""Porter Stemmer is one of the most common and gentle stemmer which is very fast                    but not very precise.""""Snowball Stemmer, whose actual name is English Stemmer is more                    precise over large data-sets.""""Lancaster Stemmer is very aggressive algorithm. It will                    hugely trim down the working data which itself has pros and cons.""Today, I learned about                    Stemming in NLP which is one of the most important step while working with text. I have presented                    the implementation of Porter Stemmer, Snowball Stemmer and Lancaster Stemmer. Excited for coming                    days !!#datascience #machinelearning #NLP #66DaysOfData",False
/feed/update/urn:li:activity:6707316144220450817/,"Day 2 of #66DaysOfData!""In Natural Language Processing, String Tokenization is a process                    where the string is split into individual words or individual parts without blanks and tabs. In the                    same step, the words in the string is converted into lower case. The Tokenize module from                    NLTK(Naural Language Toolkit) makes very easy to carry out this process.""Today, I learned                    about String Tokenization, Stop word and Punctuation in NLP. I have implemented TweetTokenizer and                    performed the process to remove stopwords and punctuation from the tokenized tweets. Excited about                    the days ahead !!#datascience #machinelearning #66DaysOfData",False
/feed/update/urn:li:activity:6706955307710476288/,"Day 1 of #66DaysOfData!""Natural language processing is a field of linguistics, computer                    science, and artificial intelligence concerned with the interactions between computers and human                    language, in particular how to program computers to process and analyze large amounts of natural                    language data.""Today, I am learning NLP from very first beginning. It's going to be a long                    run, and so excited to share the journey with Ken Jee. I am so thankful to Ken Jee for starting this                    journey !!#datascience #machinelearning #66DaysOfData",False
/feed/update/urn:li:activity:6706049593727041537/,"""Language Translation Model""Language Translation is a key service needed by the people who                    are travelling as well as for the people who are settling in a new country. In this Project, I will                    build the Sequential Model that translates the English sentences to French Sentences. Implementation                    of Natural Language Processing for converting words or texts into numbers which is then trained                    using Machine Learning Model to make the predictions. Neural Machine Translation has been used by                    Google Translate and it is implemented by billion of users around the world.I have prepared a                    simple Model using Recurrent Neural Network. You can gain insights about the workflow of preparing                    the Language Translation Model using RNN.I have recently created a site primarily focusing                    on it. You can get the overview of the Project at a glance visiting this site:                    https://lnkd.in/dapUtPD You can access to the Project with code from here:                    https://lnkd.in/dSq2m4R#machinelearning #datascience #deeplearning #python                    #learningneverstops",False
/feed/update/urn:li:activity:6703897486366412800/,"""Twitter Sentiment Analysis""I have prepared a Model using Naive Bayes Classifier which can                    classify the Positive and Negative sentiments of the tweets in twitter. My primary concern in this                    Project are summarized below:"" Understanding the Natural Language Processing (NLP) works by                    converting text or words into numbers and are trained to Machine Learning Model to make predictions.                    """" Predictions could be Sentiment inferred from social media posts and product reviews.                    """" Machine Learning based Sentiment analysis is crucial for companies to automatically                    predict whether the customers are satisfied or not. """" Understanding that the process could                    be done automatically without manually reviewing thousands of tweets and customers reviews by                    humans. """" Prepared a Model that will analyze thousands of Twitter tweets to predict                    people's sentiment. ""I have recently published an article about it :                    https://lnkd.in/dzv9PWEYou can access to the Project with code from here :                    https://lnkd.in/dXCAvHG#datascience #machinelearning                    #learningneverstops#naivebayesalgorithm #github",False
/feed/update/urn:li:activity:6701888138169597953/,"""Code for Cause"" Code for Cause is an initiative started by a group of like-minded people                    working for a similar cause. Our primary focus is to provide guidance and mentorship to students.                    Not only for those who lack on-campus opportunities but also for those who lack awareness about the                    possibilities in the field. We provide a hands-on learning experience and keep students informed                    about the latest trends in technology, opportunities so that they can keep up with the fast-paced                    digital world via following a pi-shape learning pattern â˜€ï¸I am so much thankful to Kunal                    Kushwaha for being part of this community.Check it out: codecau.se/ytIf you are                    interested, you can join us: https://lnkd.in/duYWVEb#codeforcause #machinelearning                    #opensource #learningneverstops",False
/feed/update/urn:li:activity:6697082991040045056/,"#Fastai It's been a while learning #Fastai and always been curious in implementing it's #API on                    my own work or project and finally I implemented it in one of my project. I have not implemented                    complex Deep Learning API in this project , it's just a simple pre-processing API of Machine                    Learning. I have worked on one of the #Kaggle competition and the score stand in top 15 of                    public leaderboard. I have used the default hyperparameters without much tuning and still the score                    amazes me. I just want to encourage you to try Fastai if you have never tried it before. I will                    always try to implement #FastaiAPI.You can look my implemetation:                    https://lnkd.in/dpMem-U#fastai #datascience #machinelearning",False
/feed/update/urn:li:activity:6694652781967503360/,"Hello everyone, I am so excited to share that, Ken Jee who is one of the great content creator of                    Data Science has reviewed my Resume, GitHub and LinkedIn profile on one of his Reviewing Your Data                    Science Projects series. Personally, It was so informative and inspiring for me in my Data Science                    Journey. He has always been active on his channel and has always been creating awesome contents for                    Data Science aspirants like me and many more. I want to thank Ken Jee for his dedication in creating                    such amazing stuffs and Personally I want to thank him for giving his time and effort in reviewing                    my work and profiles and give such informative suggestions.If you are interested in Data                    Science and if you want to get inspired by well known Data Scientist, then checkout his youtube                    channel: https://lnkd.in/dTcfXEu#datascience #kenjee #learningneverstops",False
/feed/update/urn:li:activity:6694590822760910849/,"""Jeremy Howard""#Fastai #DeepLearningToday, I have completed Practical Deep Learning for                    coders Part 1. It was much more challenging and practical than any other course out there. If you                    have already taken his courses or knowing it right now, I am quite excited to share some of his                    quotes here because it's a lot inspiring and encouraging for me. - ""I never have many plans,                    I just try to finish what I start.""- ""If you are not having a fun with it, it's really                    really hard to continue because there's a lot of frustation in Deep Learning. It's like working and                    working, also didn't work and also didn't work and at last it works and it just a picture of cat.                    So, you should have fun with what you are doing.""- ""Finish something properly. Just do                    something and get it done what you have started.""- ""Just code, code all the                    time.""And I personally wants to thank Jeremy Howard. He is really a role model of                    #DeepLearning.Get started: https://course.fast.ai/##deeplearning #fastai #learningneverstops                    #datascience",False
/feed/update/urn:li:activity:6691733410311892992/,"I have performed Image Segmentation with Fastai Library which was built on the top of PyTorch. I                    have achieved the accuracy of 92%. Guys, Can you have a look in my GitHub and share your                    thoughts. I am so humble to learn more.Github: https://lnkd.in/dPTavK6#deeplearning                    #computervision #datascience #learndatascience #learningneverstops",False
/feed/update/urn:li:activity:6690561498957287424/,"Finally, I have completed this, Deep Learning Specialization.It was a wonderful journey                    throughout the course as I have learned a lot about pros and cons of Deep Learning from scratch. I                    am so much thankful for Coursera with this insightful and comprehensive specialization. And I have                    always been a great fan of Andrew Ng and I am so much thankful for him and his deeplearning.ai                    team.I would like to recommend this specialization to anyone who wants to learn Deep Learning                    from scratch.#deeplearning #datascience #coursera #machinelearning #learningneverstops",False
/feed/update/urn:li:activity:6689850927207202816/,"The GitHub Arctic Code VaultThe GitHub Arctic Code Vault is a data repository preserved                    in the Arctic World Archive (AWA), a very-long-term archival facility 250 meters deep in the                    permafrost of an Arctic mountain. The archive is located in a decommissioned coal mine in the                    Svalbard archipelago, closer to the North Pole than the Arctic Circle. GitHub will capture a                    snapshot of every active public repository on 02/02/2020 and preserve that data in the Arctic Code                    Vault.Millions of developers around the world contributed to the open source software now stored                    in the Arctic Code Vault. To recognize and celebrate these contributions, we designed the Arctic                    Code Vault Badge, which is shown in the highlights section of a developerâ€™s profile on GitHub. Hover                    and you can discover some of the repositories an individual contributed                    to.Source:https://lnkd.in/d3sWZeN""I am so much excited and humble to learn and                    contribute more and more on coming days""Github: https://lnkd.in/fCCYyTK#github                    #ArticCodeVault",False
/feed/update/urn:li:activity:6688446560977596416/,"Do you how effective is Partial Dependence Plot (PDP) ?I was learning Deep Learning and                    Machine Learning from the fast.ai which was created by great Machine Learning practitioner Jeremy                    Howard. I am still learning a lot of useful techniques and tricks about Machine Learning & Deep                    Learning from his course but still I am curious to know if you guys have ever end up into this                    plot.As I learned, this plot shows the dependence of one variable with whole other variables                    of the DataFrame. You guys can search it and learn more about it and implement it as well.I                    have shared it because I thought it is still new to most of us as I didn't know about it few days                    ago.I have implemented this plot to work out one of the past competitions of Kaggle , ""Blue                    book for Bulldozers"" and the result surprise me. The rmse score obtained by my model lies somewhere                    in between top 5 of public leaderboard. You can check it out.Github:                    https://lnkd.in/dqBP-RYKaggle: https://lnkd.in/dQmwzTV",False
/feed/update/urn:li:activity:6685411738042552320/,"I am working on Deep Learning Specialization of Coursera platform provided by deeplearning.ai over                    few weeks.Thanks to team of Andrew Ng and deeplearning.ai for such a comprehensive                    specialization and I recommend anyone who is highly interested in Deep Learning.Now, I have only                    one course left in this specialization and honestly I have learned a lot. #deeplearningai                    #coursera",False
/feed/update/urn:li:activity:6682150371672035328/,"Hey, Deep Learning with RNN and LSTM is composing Folk songs music. I am so much thankful to                    #MITDeepLearning team. Thank you Steve Nouri, it's also a part of your useful contents. Helps and                    motivates a lot.You can access the code to my Github:https://lnkd.in/e4MsbXv                    #deeplearning #RNN #LSTM",False
/feed/update/urn:li:activity:6650972517194399744/,The Battle Of Neighborhoods#datascientist #machinelearning #dataanalysts,False
/feed/update/urn:li:activity:6676827281295503360/,"Once again, I am so thankful to Data Camp with this wonderful career track in Machine Learning. I                    have been reading and learning Machine Learning from Data Camp from past two months and finally I                    have completed it. I have learned a lot and those who are wondering if this track is good or not, I                    will say that it is one of the best platform to learn by doing, you will get your hands dirty by                    working on real world problems and this track is so comprehensive as well.Thanks again                    !!DataCamp #machinelearning #datascience #python",False
/feed/update/urn:li:activity:6666988846695223296/,I am so thankful to DataCamp that I learned a lot and I am confident enough to dive into Machine                    Learning now. And I wish you guys would add more projects on this career track so that learners can                    get more hands_on experience.#datascientists #machinelearning #python3 #datavisualization                    #statistics #pandas #dataanalysis,False
/feed/update/urn:li:activity:6656111471396773888/,"Hey guys!!I have completed the ""Machine Learning"" course of ""Stanford"" offered by Coursera. It                    was taught by Professor Andrew Ng. It is one of the best online course for anyone getting started                    with Machine Learning. Anyone who is getting started in this course and want to know anything                    about this course can feel free to ask anything below in the comment. And if you need help in                    submitting the assignment you can visit my Github                    account.https://lnkd.in/fCCYyTK#machinelearning #datascientist",False
